@book{hurwitz2013big,
  title     = {Big data for dummies},
  author    = {Hurwitz, Judith S and Nugent, Alan and Halper, Fern and Kaufman, Marcia},
  year      = {2013},
  publisher = {John Wiley \& Sons}
}

@book{hurwitz2020cloud,
  title     = {Cloud computing for dummies},
  author    = {Hurwitz, Judith S and Kirsch, Daniel},
  year      = {2020},
  publisher = {John Wiley \& Sons}
}

@book{anderson2015statistics,
  title     = {Statistics for big data for dummies},
  author    = {Anderson, Alan},
  year      = {2015},
  publisher = {John Wiley \& Sons}
}

@book{kohavi2020trustworthy,
  title     = {{Trustworthy online controlled experiments: A practical guide to a/b testing}},
  author    = {Kohavi, Ron and Tang, Diane and Xu, Ya},
  year      = {2020},
  publisher = {Cambridge University Press}
}

@book{debarros2022practical,
  title     = {Practical SQL: A Beginner's Guide to Storytelling with Data},
  author    = {DeBarros, Anthony},
  year      = {2022},
  publisher = {no starch Press}
}
@book{loeliger2012version,
  title     = {Version Control with Git: Powerful tools and techniques for collaborative software development},
  author    = {Loeliger, Jon and McCullough, Matthew},
  year      = {2012},
  publisher = {" O'Reilly Media, Inc."}
}

@article{blischak2016quick,
  title     = {A quick introduction to version control with Git and GitHub},
  author    = {Blischak, John D and Davenport, Emily R and Wilson, Greg},
  journal   = {PLoS computational biology},
  volume    = {12},
  number    = {1},
  pages     = {e1004668},
  year      = {2016},
  publisher = {Public Library of Science San Francisco, CA USA}
}

@book{mcquaid2014git,
  title     = {Git in practice},
  author    = {McQuaid, Mike},
  year      = {2014},
  publisher = {Simon and Schuster}
}

@book{ankam2016big,
  title     = {Big data analytics},
  author    = {Ankam, Venkat},
  year      = {2016},
  publisher = {Packt Publishing Ltd}
}
@article{hainesmodern,
  title     = {Modern Data Engineering with Apache Spark},
  author    = {Haines, Scott},
  publisher = {Springer}
}
@book{erl2013cloud,
  title     = {Cloud computing: concepts, technology \& architecture},
  author    = {Erl, Thomas and Puttini, Ricardo and Mahmood, Zaigham},
  year      = {2013},
  publisher = {Pearson Education}
}

@book{erl2016big,
  title     = {Big data fundamentals: concepts, drivers \& techniques},
  author    = {Erl, Thomas and Khattak, Wajid and Buhler, Paul},
  year      = {2016},
  publisher = {Prentice Hall Press}
}

@book{shroff2010enterprise,
  title     = {Enterprise cloud computing: technology, architecture, applications},
  author    = {Shroff, Gautam},
  year      = {2010},
  publisher = {Cambridge university press}
}

@book{bhowmik2017cloud,
  title     = {Cloud computing},
  author    = {Bhowmik, Sandeep},
  year      = {2017},
  publisher = {Cambridge University Press}
}
@book{alla2018big,
  title     = {Big Data Analytics with Hadoop 3: Build highly effective analytics solutions to gain valuable insight into your big data},
  author    = {Alla, Sridhar},
  year      = {2018},
  publisher = {Packt Publishing Ltd}
}

@book{card1999readings,
  title     = {Readings in information visualization: using vision to think},
  author    = {Card, Stuart K and Mackinlay, Jock and Shneiderman, Ben},
  year      = {1999},
  publisher = {Morgan Kaufmann}
}

@article{anscombe1973graphs,
  title     = {Graphs in statistical analysis},
  author    = {Anscombe, Francis J},
  journal   = {The american statistician},
  volume    = {27},
  number    = {1},
  pages     = {17--21},
  year      = {1973},
  publisher = {Taylor \& Francis}
}

@inproceedings{matejka2017same,
  title     = {Same stats, different graphs: generating datasets with varied appearance and identical statistics through simulated annealing},
  author    = {Matejka, Justin and Fitzmaurice, George},
  booktitle = {Proceedings of the 2017 CHI conference on human factors in computing systems},
  pages     = {1290--1294},
  year      = {2017}
}

@article{Tidwell2020,
  abstract = {3rd edition. It's not easy to design good application interfaces in a world where companies must create compelling, seamless user experiences across an exploding number of channels, screens, and contexts. Design patterns, design systems, and component-based UI frameworks have emerged and now rapidly evolve to meet the challenge. This bestselling book is one of the few reliable sources to help you navigate through the maze of design options. By capturing UI best practices and reusable ideas as design patterns, Designing Interfaces provides solutions to common design problems that you can tailor to the situation at hand. This updated edition includes patterns for mobile apps and social media, as well as web applications and desktop software. Each pattern contains full-color examples and practical design advice that you can use immediately. Experienced designers can use this guide as a sourcebook of ideas; novices will find a roadmap to the world of interface and interaction design.},
  author   = {Jennifer Tidwell and Charles Brewer and Aynne Valencia},
  isbn     = {9781492051961},
  issn     = {19381425},
  issue    = {4},
  journal  = {MRS Bulletin},
  pages    = {27-30},
  title    = {Designing Interfaces 3rd edition},
  volume   = {16},
  url      = {https://www.oreilly.com/library/view/designing-interfaces-3rd/9781492051954/},
  year     = {2020}
}
@article{Ambrose2019,
  author    = {Gavin. Ambrose and Paul. Harris},
  isbn      = {9781350035270},
  pages     = {288},
  publisher = {Bloomsbury Visual Arts},
  title     = {The Visual Dictionary of Graphic Design},
  url       = {https://www.bloomsbury.com/us/visual-dictionary-of-graphic-design-9781350035270/},
  year      = {2019}
}
@article{Resnick2003,
  abstract  = {From the Publisher: Complete coverage of basic design principles illustrated by student examples. Design for Communication offers a unique approach to mastering the basic design principles, conceptual problem-solving methods, and critical-thinking skills that distinguish graphic designers from desktop technicians. This book presents forty-two basic to advanced graphic design and typography assignments collaboratively written by college educators to teach the fundamental processes, concepts, and techniques through hands-on applications. Each assignment is illustrated with actual student solutions, and each includes a process narrative and an educator's critical analysis revealing the reasoning behind the creative strategies employed by each individual student solution. Assignments are organized from basic to advanced within six sections: The elements and principles of design; Typography as image; Creative word play; Word and image; Grid and visual hierarchy; Visual advocacy. Design for Communication is a highly visual resource of instruction, information, ideas, and inspiration for students and professionals. Introduction -- What is graphic design -- What do graphic designers do -- I want to be a graphic designer-where do I begin -- Design process -- Why bother with such a long process when I just like to make things -- Why should I do these assignments -- Section 1: Elements And Principles Of Design -- Star symbol / Susan Merritt -- Object semantics / Kermit Bailey -- Symbol design / Lisa Fontaine -- Lettermark / Susan Merritt -- Vinyletteror / Kenneth Fitzgerald -- Letterform as shape / Jan Conradi -- Concert poster / Arnold Holland -- Design history chair / Hank Richardson -- Section 2: Typography As Image -- Shaping words / Richard Ybarra -- Newspaper stories-a typographic workshop / Jurgen Hefele -- Typographic self-portrait / Esen Karol -- Typographic self-portrait / Elizabeth Resnick -- Typeface poster / Hyun Mee Kim -- Directions poster / Frank Baseman -- Poetry in motion / Elizabeth Resnick -- Section 3: Creative Wordplay -- Descriptive pairs / Elizabeth Resnick -- Letters as image / Hyun Mee Kim -- Concrete poetry / Kenneth Fitzgerald -- Arthur Murray Dance advertisement / Frank Baseman -- CD cover: typographical music / Heather Corcoran -- Section 4: Word and image -- Word and image / Elizabeth Resnick -- Book cover design: the Interpretation of Dreams by Sigmund Freud / Elizabeth Resnick -- Book cover design: Einstein's Dreams by Alan Lightman / Elizabeth Resnick -- Postage stamp design: a celebration of cultural diversity in America / Elizabeth Resnick and Glenn Berger -- Postage stamp design: a celebration of American primary education: reading, writing, mathematics and science / Elizabeth Resnick -- Stamp design / Michael Burke -- Sculpture poster / Tom Briggs -- Gardening poster / Karen Bernstein -- Kitchen of meaning exhibition poster / Kermit Bailey -- Section 5: Grid and visual hierarchy -- Typographic history spread / Elizabeth Resnick -- Two-sided typography poster / Judith Aronson -- Three type specimens / Carol Sogard -- Visual poetry calendar / Peggy Re -- Science lecture series posters / Elizabeth Resnick -- Book: Lewis & Clark / Heather Corcoran -- Journey journal / Gulizar Cepoglu -- Section 6: Visual Advocacy -- Literacy poster: learn to read / Elizabeth Resnick -- Human rights poster / Elizabeth Resnick -- Designing dissent: advocacy poster series / Elizabeth Resnick -- Happy Deutschland / Jurgen Hefele -- Three-dimensional direct response (mail) solicitation / Elizabeth Resnick and Glenn Berger -- Tolerance Bus Shelter Poster / Frank Baseman -- Bibliography -- Instructor contact information -- Index.},
  author    = {Elizabeth. Resnick},
  isbn      = {978-0-471-41829-0},
  pages     = {255},
  publisher = {Wiley & Sons},
  title     = {Design for communication : conceptual graphic design basics},
  url       = {https://www.wiley.com/en-us/Design+for+Communication%3A+Conceptual+Graphic+Design+Basics-p-9780471418290},
  year      = {2003}
}


@book{tufte1983visual,
  title     = {The visual display of quantitative information},
  author    = {Tufte, Edward R and Graves-Morris, Peter R},
  volume    = {2},
  number    = {9},
  year      = {1983},
  publisher = {Graphics press Cheshire, CT}
}

@article{tufte1991envisioning,
  title     = {Envisioning information},
  author    = {Tufte, Edward R},
  journal   = {Optometry and Vision Science},
  volume    = {68},
  number    = {4},
  pages     = {322--324},
  year      = {1991},
  publisher = {LWW}
}

@book{tufte2006beautiful,
  title     = {Beautiful evidence},
  author    = {Tufte, Edward R and Tufte, Edward Rolf},
  volume    = {1},
  year      = {2006},
  publisher = {Graphics Press Cheshire, CT}
}

@book{tufte1974data,
  title     = {Data analysis for politics and policy},
  author    = {Tufte, Edward R},
  year      = {1974},
  publisher = {Prentice-Hall Englewood Cliffs, NJ}
}
@book{tufte1997visual,
  title     = {Visual explanations: Images and quantities, evidence and narrative},
  author    = {Tufte, Edward R},
  year      = {1997},
  publisher = {Graphics Press}
}

@book{brase2016understanding,
  title     = {Understanding basic statistics},
  author    = {Brase, Charles Henry and Brase, Corrinne Pellillo},
  year      = {2011},
  edition   = {10th},
  publisher = {Cengage Learning Hampshire, UK}
}


@article{chaplin2004geographic,
  title     = {Geographic distribution of environmental factors influencing human skin coloration},
  author    = {Chaplin, George},
  journal   = {American Journal of Physical Anthropology: The Official Publication of the American Association of Physical Anthropologists},
  volume    = {125},
  number    = {3},
  pages     = {292--302},
  year      = {2004},
  publisher = {Wiley Online Library}
}

@book{tufte1983visual,
  author    = {Edward R. Tufte},
  title     = {The Visual Display of Quantitative Information},
  year      = {1983},
  publisher = {Graphics Press},
  address   = {Cheshire, Connecticut}
}

@book{cleveland1994elements,
  author    = {William S. Cleveland},
  title     = {The Elements of Graphing Data},
  year      = {1994},
  publisher = {Hobart Press},
  address   = {Summit, New Jersey}
}

@book{wilkinson1999grammar,
  author    = {Leland Wilkinson},
  title     = {The Grammar of Graphics},
  year      = {1999},
  publisher = {Springer},
  address   = {New York}
}

@book{wickham2009ggplot2,
  author    = {Hadley Wickham},
  title     = {ggplot2: Elegant Graphics for Data Analysis},
  year      = {2009},
  publisher = {Springer},
  address   = {New York}
}

@article{anscombe1973graphs,
  author    = {F.J. Anscombe},
  title     = {Graphs in Statistical Analysis},
  journal   = {The American Statistician},
  volume    = {27},
  number    = {1},
  pages     = {17--21},
  year      = {1973},
  publisher = {American Statistical Association}
}

@article{matejka2017same,
  author  = {Justin Matejka and George Fitzmaurice},
  title   = {Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing},
  journal = {ACM CHI Conference on Human Factors in Computing Systems},
  year    = {2017},
  pages   = {1290--1294}
}

@book{playfair1786atlas,
  author    = {William Playfair},
  title     = {The Commercial and Political Atlas},
  year      = {1786},
  publisher = {J. Debrett},
  address   = {London}
}

@article{card1999readings,
  author    = {Stuart K. Card, Jock D. Mackinlay, and Ben Shneiderman},
  title     = {Readings in Information Visualization: Using Vision to Think},
  year      = {1999},
  publisher = {Morgan Kaufmann},
  address   = {San Francisco, CA}
}

@book{yau2011data,
  author    = {Nathan Yau},
  title     = {Visualize This: The FlowingData Guide to Design, Visualization, and Statistics},
  year      = {2011},
  publisher = {Wiley},
  address   = {Indianapolis, IN}
}

@book{minard1869napoleon,
  author    = {Charles Joseph Minard},
  title     = {Carte figurative des pertes successives en hommes de l'Armée Française dans la campagne de Russie 1812-1813},
  year      = {1869},
  publisher = {Self-published},
  address   = {Paris}
}

@article{franconeri2021data,
  author    = {Steven Franconeri},
  title     = {ExperCeption Dot Net: Data Visualization Quick Reference},
  year      = {2021},
  publisher = {Northwestern University},
  url       = {https://static1.squarespace.com/static/54c9ed77e4b03009ff855b12/t/60b9af61c8f89f0d0d4e5d15/1622781794396/Franconeri_ExperCeptionDotNet_DataVisQuickRef+%283%29.pdf}
}

@article{nightingale1858mortality,
  author  = {Florence Nightingale},
  title   = {Diagram of the Causes of Mortality in the Army in the East},
  year    = {1858},
  journal = {Journal of the Statistical Society of London},
  volume  = {21},
  number  = {1},
  pages   = {1--26}
}

@book{abela2015slide,
  author    = {Andrew Abela},
  title     = {Announcing the Slide Chooser},
  year      = {2015},
  publisher = {Extreme Presentation},
  url       = {https://extremepresentation.typepad.com/blog/2015/01/announcing-the-slide-chooser.html}
}

@book{schwabish2014continuum,
  author    = {Jon Schwabish and Severino Ribecca},
  title     = {The Graphic Continuum},
  year      = {2014},
  publisher = {PolicyViz},
  url       = {https://policyviz.com/2014/09/09/graphic-continuum/}
}

@misc{holtz2022data,
  author = {Yan Holtz and Conor Healy},
  title  = {From Data to Viz},
  year   = {2022},
  url    = {https://www.data-to-viz.com/}
}

@misc{ferdio2022dataviz,
  author = {Ferdio},
  title  = {The Data Viz Project},
  year   = {2022},
  url    = {https://datavizproject.com/}
}

@inproceedings{rendle2010factorization,
  title        = {Factorization machines},
  author       = {Rendle, Steffen},
  booktitle    = {2010 IEEE International conference on data mining},
  pages        = {995--1000},
  year         = {2010},
  organization = {IEEE}
}


@article{Zewe2023,
  author  = {Adam Zewe},
  journal = {MIT News},
  month   = {11},
  title   = {Explained: Generative AI},
  url     = {https://news.mit.edu/2023/explained-generative-ai-1109},
  year    = {2023}
}


@article{Wang2023,
  title         = {A survey of the evolution of language model-based dialogue systems},
  author        = {Wang, Hongru and Wang, Lingzhi and Du, Yiming and Chen, Liang and Zhou, Jingyan and Wang, Yufei and Wong, Kam-Fai},
  journal       = {arXiv preprint arXiv:2311.16789},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2311.16789}
}


@article{talib2016text,
  title     = {Text mining: techniques, applications and issues},
  author    = {Talib, Ramzan and Hanif, Muhammad Kashif and Ayesha, Shaeela and Fatima, Fakeeha},
  journal   = {International journal of advanced computer science and applications},
  volume    = {7},
  number    = {11},
  pages     = {414--418},
  year      = {2016},
  publisher = {The Science and Information Organization}
}



@article{Bengio2003NeuralLM,
  title   = {A Neural Probabilistic Language Model},
  author  = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  journal = {Journal of Machine Learning Research},
  volume  = {3},
  pages   = {1137--1155},
  year    = {2003},
  url     = {https://www.jmlr.org/papers/v3/bengio03a.html}
}

@article{TurneyPantel2010VSM,
  title   = {From Frequency to Meaning: Vector Space Models of Semantics},
  author  = {Turney, Peter D. and Pantel, Patrick},
  journal = {Journal of Artificial Intelligence Research},
  volume  = {37},
  pages   = {141--188},
  year    = {2010},
  doi     = {10.1613/jair.2934}
}

@article{Mikolov2013Word2Vec,
  title         = {Distributed Representations of Words and Phrases and their Compositionality},
  author        = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeff},
  booktitle     = {Advances in Neural Information Processing Systems},
  volume        = {26},
  year          = {2013},
  url           = {https://arxiv.org/abs/1310.4546},
  archiveprefix = {arXiv},
  eprint        = {1310.4546}
}

@article{Vaswani2017Attention,
  title         = {Attention Is All You Need},
  author        = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and others},
  booktitle     = {Advances in Neural Information Processing Systems},
  volume        = {30},
  year          = {2017},
  url           = {https://arxiv.org/abs/1706.03762},
  archiveprefix = {arXiv},
  eprint        = {1706.03762}
}

@article{Kaplan2020Scaling,
  title         = {Scaling Laws for Neural Language Models},
  author        = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and others},
  journal       = {arXiv preprint arXiv:2001.08361},
  year          = {2020},
  url           = {https://arxiv.org/abs/2001.08361},
  archiveprefix = {arXiv},
  eprint        = {2001.08361}
}

@article{Zhao2024LLMSurvey,
  title         = {A Survey of Large Language Models},
  author        = {Zhao, Wayne Xin and Zhou, Kun and Li, Jun and others},
  journal       = {ACM Computing Surveys},
  year          = {2024},
  url           = {https://arxiv.org/abs/2303.18223},
  archiveprefix = {arXiv},
  eprint        = {2303.18223}
}

@article{Collobert2011NLPFromScratch,
  title   = {Natural Language Processing (Almost) from Scratch},
  author  = {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and others},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  pages   = {2493--2537},
  year    = {2011},
  url     = {https://www.jmlr.org/papers/v12/collobert11a.html}
}

@article{Kogan2019BusinessIE,
  title   = {Text Mining and Information Extraction in Business Intelligence},
  author  = {Kogan, Shimon and Levin, Dimitry and Routledge, Bryan R. and Sagi, Jacob and Smith, Noah A.},
  journal = {Foundations and Trends in Accounting},
  volume  = {13},
  number  = {1},
  pages   = {1--141},
  year    = {2019},
  doi     = {10.1561/1400000058}
}

@book{Gentzkow2019TextAsData,
  title     = {Text as Data},
  author    = {Gentzkow, Matthew and Kelly, Bryan and Taddy, Matt},
  publisher = {Journal of Economic Perspectives},
  volume    = {33},
  number    = {3},
  pages     = {3--28},
  year      = {2019},
  doi       = {10.1257/jep.33.3.3}
}

@article{Schulhoff2025PromptReport,
  title         = {The Prompt Report: A Systematic Survey of Prompting Techniques},
  author        = {Schulhoff, Sander and others},
  journal       = {arXiv preprint arXiv:2406.06608},
  year          = {2025},
  url           = {https://arxiv.org/abs/2406.06608},
  archiveprefix = {arXiv},
  eprint        = {2406.06608}
}

@article{Wei2022CoT,
  title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author        = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and others},
  journal       = {arXiv preprint arXiv:2201.11903},
  year          = {2022},
  url           = {https://arxiv.org/abs/2201.11903},
  archiveprefix = {arXiv},
  eprint        = {2201.11903}
}

@article{Khattab2023DSPy,
  title         = {DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author        = {Khattab, Omar and others},
  journal       = {arXiv preprint arXiv:2310.03714},
  year          = {2023},
  url           = {https://arxiv.org/abs/2310.03714},
  archiveprefix = {arXiv},
  eprint        = {2310.03714}
}

@article{Ouyang2022RLHF,
  title         = {Training Language Models with Human Feedback},
  author        = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and others},
  journal       = {arXiv preprint arXiv:2203.02155},
  year          = {2022},
  url           = {https://arxiv.org/abs/2203.02155},
  archiveprefix = {arXiv},
  eprint        = {2203.02155}
}

@article{Lewis2020RAG,
  title         = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  author        = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and others},
  journal       = {Advances in Neural Information Processing Systems},
  volume        = {33},
  year          = {2020},
  url           = {https://arxiv.org/abs/2005.11401},
  archiveprefix = {arXiv},
  eprint        = {2005.11401}
}

@article{Barnett2024RAGFailures,
  title         = {Seven Failure Points When Engineering a Retrieval-Augmented Generation System},
  author        = {Barnett, Joshua and others},
  journal       = {arXiv preprint arXiv:2401.05856},
  year          = {2024},
  url           = {https://arxiv.org/abs/2401.05856},
  archiveprefix = {arXiv},
  eprint        = {2401.05856}
}

@article{Asai2023SelfRAG,
  title         = {Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection},
  author        = {Asai, Akari and others},
  journal       = {arXiv preprint arXiv:2310.11511},
  year          = {2023},
  url           = {https://arxiv.org/abs/2310.11511},
  archiveprefix = {arXiv},
  eprint        = {2310.11511}
}

@article{Hu2021LoRA,
  title         = {LoRA: Low-Rank Adaptation of Large Language Models},
  author        = {Hu, Edward and Shen, Yelong and Wallis, Phillip and others},
  journal       = {arXiv preprint arXiv:2106.09685},
  year          = {2021},
  url           = {https://arxiv.org/abs/2106.09685},
  archiveprefix = {arXiv},
  eprint        = {2106.09685}
}

@article{Dettmers2023QLoRA,
  title         = {QLoRA: Efficient Finetuning of Quantized LLMs},
  author        = {Dettmers, Tim and others},
  journal       = {arXiv preprint arXiv:2305.14314},
  year          = {2023},
  url           = {https://arxiv.org/abs/2305.14314},
  archiveprefix = {arXiv},
  eprint        = {2305.14314}
}

@article{Huang2023Hallucinations,
  title         = {A Survey on Hallucination in Large Language Models},
  author        = {Huang, Lei and others},
  journal       = {arXiv preprint arXiv:2309.05276},
  year          = {2023},
  url           = {https://arxiv.org/abs/2309.05276},
  archiveprefix = {arXiv},
  eprint        = {2309.05276}
}

@article{Bender2021StochasticParrots,
  title   = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author  = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  journal = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency},
  year    = {2021},
  doi     = {10.1145/3442188.3445922}
}


@article{Raymond1999CathedralBazaar,
  title   = {The Cathedral and the Bazaar},
  author  = {Raymond, Eric S.},
  journal = {First Monday},
  volume  = {3},
  number  = {3},
  year    = {1999},
  url     = {https://firstmonday.org/ojs/index.php/fm/article/view/578}
}

@article{Wilson2014BestPractices,
  title   = {Best Practices for Scientific Computing},
  author  = {Wilson, Greg and Aruliah, D. A. and Brown, C. T. and others},
  journal = {PLoS Biology},
  volume  = {12},
  number  = {1},
  year    = {2014},
  doi     = {10.1371/journal.pbio.1001745}
}

@article{Peng2011ReproducibleResearch,
  title   = {Reproducible Research in Computational Science},
  author  = {Peng, Roger D.},
  journal = {Science},
  volume  = {334},
  number  = {6060},
  pages   = {1226--1227},
  year    = {2011},
  doi     = {10.1126/science.1213847}
}

@article{Stodden2014PracticeReproducible,
  title     = {The Practice of Reproducible Research: Case Studies and Lessons from the Data-Intensive Sciences},
  author    = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger},
  journal   = {Computing in Science \& Engineering},
  year      = {2014},
  publisher = {IEEE}
}

@article{Stodden2016ComputationalRepro,
  title   = {Enhancing Reproducibility for Computational Methods},
  author  = {Stodden, Victoria and McNutt, Marcia and Bailey, David H. and others},
  journal = {Science},
  volume  = {354},
  number  = {6317},
  year    = {2016},
  doi     = {10.1126/science.aah6168}
}

@book{Knuth1984LiterateProgramming,
  title     = {Literate Programming},
  author    = {Knuth, Donald E.},
  publisher = {CSLI},
  year      = {1984}
}

@article{Firth1957Distributional,
  title   = {A Synopsis of Linguistic Theory 1930–1955},
  author  = {Firth, J. R.},
  journal = {Studies in Linguistic Analysis},
  year    = {1957}
}

@article{Bubeck2023SparksAGI,
  title         = {Sparks of Artificial General Intelligence: Early Experiments with GPT-4},
  author        = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and others},
  journal       = {arXiv preprint arXiv:2303.12712},
  year          = {2023},
  url           = {https://arxiv.org/abs/2303.12712},
  archiveprefix = {arXiv},
  eprint        = {2303.12712}
}

@article{OpenAI2023GPT4V,
  title   = {GPT-4V(ision) System Card},
  author  = {{OpenAI}},
  journal = {OpenAI Technical Report},
  year    = {2023},
  url     = {https://openai.com/research/gpt-4v-system-card}
}

@article{Kirillov2023SAM,
  title         = {Segment Anything},
  author        = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and others},
  journal       = {arXiv preprint arXiv:2304.02643},
  year          = {2023},
  url           = {https://arxiv.org/abs/2304.02643},
  archiveprefix = {arXiv},
  eprint        = {2304.02643}
}

@article{Zhang2023MultimodalCoT,
  title         = {Multimodal Chain-of-Thought Reasoning in Large Language Models},
  author        = {Zhang, Han and others},
  journal       = {arXiv preprint arXiv:2302.00923},
  year          = {2023},
  url           = {https://arxiv.org/abs/2302.00923},
  archiveprefix = {arXiv},
  eprint        = {2302.00923}
}

@article{Liu2023ContextWindow,
  title         = {Lost in the Middle: How Language Models Use Long Contexts},
  author        = {Liu, Nelson F. and others},
  journal       = {arXiv preprint arXiv:2307.03172},
  year          = {2023},
  url           = {https://arxiv.org/abs/2307.03172},
  archiveprefix = {arXiv},
  eprint        = {2307.03172}
}

@article{Ding2023LongNet,
  title         = {LongNet: Scaling Transformers to 1,000,000,000 Tokens},
  author        = {Ding, Shuai and others},
  journal       = {arXiv preprint arXiv:2307.02486},
  year          = {2023},
  url           = {https://arxiv.org/abs/2307.02486},
  archiveprefix = {arXiv},
  eprint        = {2307.02486}
}

@article{Xu2024LazyLLM,
  title         = {LazyLLM: Dynamic Token Budgeting for Efficient LLM Inference},
  author        = {Xu, Rui and others},
  journal       = {arXiv preprint arXiv:2402.07864},
  year          = {2024},
  url           = {https://arxiv.org/abs/2402.07864},
  archiveprefix = {arXiv},
  eprint        = {2402.07864}
}

@article{Yan2024CorrectiveRAG,
  title         = {Corrective Retrieval Augmented Generation},
  author        = {Yan, Yukun and others},
  journal       = {arXiv preprint arXiv:2401.15884},
  year          = {2024},
  url           = {https://arxiv.org/abs/2401.15884},
  archiveprefix = {arXiv},
  eprint        = {2401.15884}
}

@article{Sarmah2024HybridRAG,
  title         = {HybridRAG: Integrating Retrieval and Reasoning for Knowledge-Intensive Tasks},
  author        = {Sarmah, Ankit and others},
  journal       = {arXiv preprint arXiv:2402.15012},
  year          = {2024},
  url           = {https://arxiv.org/abs/2402.15012},
  archiveprefix = {arXiv},
  eprint        = {2402.15012}
}

@article{Blei2006DynamicTopics,
  title   = {Dynamic Topic Models},
  author  = {Blei, David M. and Lafferty, John D.},
  journal = {Proceedings of the 23rd International Conference on Machine Learning},
  year    = {2006}
}

@article{Hamilton2016SemanticChange,
  title   = {Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change},
  author  = {Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
  journal = {ACL},
  year    = {2016}
}

@article{Gu2024LLMJudge,
  title         = {LLM-as-a-Judge: Evaluating Language Models with Language Models},
  author        = {Gu, Shisheng and others},
  journal       = {arXiv preprint arXiv:2402.04788},
  year          = {2024},
  url           = {https://arxiv.org/abs/2402.04788},
  archiveprefix = {arXiv},
  eprint        = {2402.04788}
}

@article{Dhuliawala2023CoVe,
  title         = {Chain-of-Verification Reduces Hallucination in Large Language Models},
  author        = {Dhuliawala, Shehzaad and others},
  journal       = {arXiv preprint arXiv:2309.11495},
  year          = {2023},
  url           = {https://arxiv.org/abs/2309.11495},
  archiveprefix = {arXiv},
  eprint        = {2309.11495}
}



@article{Raymond1999CathedralBazaar,
  title   = {The Cathedral and the Bazaar},
  author  = {Raymond, Eric S.},
  journal = {First Monday},
  volume  = {3},
  number  = {3},
  year    = {1999},
  url     = {https://firstmonday.org/ojs/index.php/fm/article/view/578},
  doi     = {10.5210/fm.v3i2.578}
}

@article{Wilson2014BestPractices,
  title   = {Best Practices for Scientific Computing},
  author  = {Wilson, Greg and Aruliah, D. A. and Brown, C. T. and others},
  journal = {PLoS Biology},
  volume  = {12},
  number  = {1},
  year    = {2014},
  doi     = {10.1371/journal.pbio.1001745}
}

@article{Peng2011ReproducibleResearch,
  title   = {Reproducible Research in Computational Science},
  author  = {Peng, Roger D.},
  journal = {Science},
  volume  = {334},
  number  = {6060},
  pages   = {1226--1227},
  year    = {2011},
  doi     = {10.1126/science.1213847}
}

@article{Stodden2014PracticeReproducible,
  title     = {The Practice of Reproducible Research: Case Studies and Lessons from the Data-Intensive Sciences},
  author    = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger},
  journal   = {Computing in Science \& Engineering},
  year      = {2014},
  publisher = {IEEE}
}

@article{Stodden2016ComputationalRepro,
  title   = {Enhancing Reproducibility for Computational Methods},
  author  = {Stodden, Victoria and McNutt, Marcia and Bailey, David H. and others},
  journal = {Science},
  volume  = {354},
  number  = {6317},
  year    = {2016},
  doi     = {10.1126/science.aah6168}
}

@book{Knuth1984LiterateProgramming,
  title     = {Literate Programming},
  author    = {Knuth, Donald E.},
  publisher = {CSLI},
  year      = {1984}
}

@article{Firth1957Distributional,
  title   = {A Synopsis of Linguistic Theory 1930–1955},
  author  = {Firth, J. R.},
  journal = {Studies in Linguistic Analysis},
  year    = {1957}
}

@article{Bubeck2023SparksAGI,
  title         = {Sparks of Artificial General Intelligence: Early Experiments with GPT-4},
  author        = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and others},
  journal       = {arXiv preprint arXiv:2303.12712},
  year          = {2023},
  url           = {https://arxiv.org/abs/2303.12712},
  archiveprefix = {arXiv},
  eprint        = {2303.12712}
}

@article{OpenAI2023GPT4V,
  title   = {GPT-4V(ision) System Card},
  author  = {{OpenAI}},
  journal = {OpenAI Technical Report},
  year    = {2023},
  url     = {https://openai.com/research/gpt-4v-system-card}
}

@article{Kirillov2023SAM,
  title         = {Segment Anything},
  author        = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and others},
  journal       = {arXiv preprint arXiv:2304.02643},
  year          = {2023},
  url           = {https://arxiv.org/abs/2304.02643},
  archiveprefix = {arXiv},
  eprint        = {2304.02643}
}

@article{Zhang2023MultimodalCoT,
  title         = {Multimodal Chain-of-Thought Reasoning in Large Language Models},
  author        = {Zhang, Han and others},
  journal       = {arXiv preprint arXiv:2302.00923},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2302.00923}
}

@article{Liu2023ContextWindow,
  title         = {Lost in the Middle: How Language Models Use Long Contexts},
  author        = {Liu, Nelson F. and others},
  journal       = {arXiv preprint arXiv:2307.03172},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2307.03172}
}

@article{Ding2023LongNet,
  title         = {LongNet: Scaling Transformers to 1,000,000,000 Tokens},
  author        = {Ding, Shuai and others},
  journal       = {arXiv preprint arXiv:2307.02486},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2307.02486}
}

@article{Xu2024LazyLLM,
  title         = {LazyLLM: Dynamic Token Budgeting for Efficient LLM Inference},
  author        = {Xu, Rui and others},
  journal       = {arXiv preprint arXiv:2402.07864},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.07864}
}

@article{Yan2024CorrectiveRAG,
  title         = {Corrective Retrieval Augmented Generation},
  author        = {Yan, Yukun and others},
  journal       = {arXiv preprint arXiv:2401.15884},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2401.15884}
}

@article{Sarmah2024HybridRAG,
  title         = {HybridRAG: Integrating Retrieval and Reasoning for Knowledge-Intensive Tasks},
  author        = {Sarmah, Ankit and others},
  journal       = {arXiv preprint arXiv:2402.15012},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.15012}
}

@article{Blei2006DynamicTopics,
  title   = {Dynamic Topic Models},
  author  = {Blei, David M. and Lafferty, John D.},
  journal = {Proceedings of the 23rd International Conference on Machine Learning},
  year    = {2006}
}

@article{Hamilton2016SemanticChange,
  title   = {Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change},
  author  = {Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
  journal = {ACL},
  year    = {2016}
}

@article{Gu2024LLMJudge,
  title         = {LLM-as-a-Judge: Evaluating Language Models with Language Models},
  author        = {Gu, Shisheng and others},
  journal       = {arXiv preprint arXiv:2402.04788},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.04788}
}

@article{Dhuliawala2023CoVe,
  title         = {Chain-of-Verification Reduces Hallucination in Large Language Models},
  author        = {Dhuliawala, Shehzaad and others},
  journal       = {arXiv preprint arXiv:2309.11495},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2309.11495}
}

@article{Bengio2003NeuralLM,
  title   = {A Neural Probabilistic Language Model},
  author  = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  journal = {Journal of Machine Learning Research},
  volume  = {3},
  pages   = {1137--1155},
  year    = {2003},
  url     = {https://www.jmlr.org/papers/v3/bengio03a.html}
}

@article{TurneyPantel2010VSM,
  title   = {From Frequency to Meaning: Vector Space Models of Semantics},
  author  = {Turney, Peter D. and Pantel, Patrick},
  journal = {Journal of Artificial Intelligence Research},
  volume  = {37},
  pages   = {141--188},
  year    = {2010},
  doi     = {10.1613/jair.2934}
}

@article{Mikolov2013Word2Vec,
  title         = {Distributed Representations of Words and Phrases and their Compositionality},
  author        = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeff},
  booktitle     = {Advances in Neural Information Processing Systems},
  volume        = {26},
  year          = {2013},
  url           = {https://arxiv.org/abs/1310.4546},
  archiveprefix = {arXiv},
  eprint        = {1310.4546}
}

@article{Vaswani2017Attention,
  title         = {Attention Is All You Need},
  author        = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and others},
  booktitle     = {Advances in Neural Information Processing Systems},
  volume        = {30},
  year          = {2017},
  url           = {https://arxiv.org/abs/1706.03762},
  archiveprefix = {arXiv},
  eprint        = {1706.03762}
}

@article{Kaplan2020Scaling,
  title         = {Scaling Laws for Neural Language Models},
  author        = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and others},
  journal       = {arXiv preprint arXiv:2001.08361},
  year          = {2020},
  url           = {https://arxiv.org/abs/2001.08361},
  archiveprefix = {arXiv},
  eprint        = {2001.08361}
}

@article{Zhao2024LLMSurvey,
  title         = {A Survey of Large Language Models},
  author        = {Zhao, Wayne Xin and Zhou, Kun and Li, Jun and others},
  journal       = {ACM Computing Surveys},
  year          = {2024},
  url           = {https://arxiv.org/abs/2303.18223},
  archiveprefix = {arXiv},
  eprint        = {2303.18223}
}

@article{Collobert2011NLPFromScratch,
  title   = {Natural Language Processing (Almost) from Scratch},
  author  = {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and others},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  pages   = {2493--2537},
  year    = {2011},
  url     = {https://www.jmlr.org/papers/v12/collobert11a.html}
}

@article{Kogan2019BusinessIE,
  title   = {Text Mining and Information Extraction in Business Intelligence},
  author  = {Kogan, Shimon and Levin, Dimitry and Routledge, Bryan R. and Sagi, Jacob and Smith, Noah A.},
  journal = {Foundations and Trends in Accounting},
  volume  = {13},
  number  = {1},
  pages   = {1--141},
  year    = {2019},
  doi     = {10.1561/1400000058}
}

@book{Gentzkow2019TextAsData,
  title     = {Text as Data},
  author    = {Gentzkow, Matthew and Kelly, Bryan and Taddy, Matt},
  publisher = {Journal of Economic Perspectives},
  volume    = {33},
  number    = {3},
  pages     = {3--28},
  year      = {2019},
  doi       = {10.1257/jep.33.3.3}
}

@article{Schulhoff2025PromptReport,
  title         = {The Prompt Report: A Systematic Survey of Prompting Techniques},
  author        = {Schulhoff, Sander and others},
  journal       = {arXiv preprint arXiv:2406.06608},
  year          = {2025},
  url           = {https://arxiv.org/abs/2406.06608},
  archiveprefix = {arXiv},
  eprint        = {2406.06608}
}

@article{Wei2022CoT,
  title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author        = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and others},
  journal       = {arXiv preprint arXiv:2201.11903},
  year          = {2022},
  url           = {https://arxiv.org/abs/2201.11903},
  archiveprefix = {arXiv},
  eprint        = {2201.11903}
}

@article{Khattab2023DSPy,
  title         = {DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author        = {Khattab, Omar and others},
  journal       = {arXiv preprint arXiv:2310.03714},
  year          = {2023},
  url           = {https://arxiv.org/abs/2310.03714},
  archiveprefix = {arXiv},
  eprint        = {2310.03714}
}

@article{Ouyang2022RLHF,
  title         = {Training Language Models with Human Feedback},
  author        = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and others},
  journal       = {arXiv preprint arXiv:2203.02155},
  year          = {2022},
  url           = {https://arxiv.org/abs/2203.02155},
  archiveprefix = {arXiv},
  eprint        = {2203.02155}
}

@article{Lewis2020RAG,
  title         = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  author        = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and others},
  journal       = {Advances in Neural Information Processing Systems},
  volume        = {33},
  year          = {2020},
  url           = {https://arxiv.org/abs/2005.11401},
  archiveprefix = {arXiv},
  eprint        = {2005.11401}
}

@article{Barnett2024RAGFailures,
  title         = {Seven Failure Points When Engineering a Retrieval-Augmented Generation System},
  author        = {Barnett, Joshua and others},
  journal       = {arXiv preprint arXiv:2401.05856},
  year          = {2024},
  url           = {https://arxiv.org/abs/2401.05856},
  archiveprefix = {arXiv},
  eprint        = {2401.05856}
}

@article{Asai2023SelfRAG,
  title         = {Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection},
  author        = {Asai, Akari and others},
  journal       = {arXiv preprint arXiv:2310.11511},
  year          = {2023},
  url           = {https://arxiv.org/abs/2310.11511},
  archiveprefix = {arXiv},
  eprint        = {2310.11511}
}

@article{Hu2021LoRA,
  title         = {LoRA: Low-Rank Adaptation of Large Language Models},
  author        = {Hu, Edward and Shen, Yelong and Wallis, Phillip and others},
  journal       = {arXiv preprint arXiv:2106.09685},
  year          = {2021},
  url           = {https://arxiv.org/abs/2106.09685},
  archiveprefix = {arXiv},
  eprint        = {2106.09685}
}

@article{Dettmers2023QLoRA,
  title         = {QLoRA: Efficient Finetuning of Quantized LLMs},
  author        = {Dettmers, Tim and others},
  journal       = {arXiv preprint arXiv:2305.14314},
  year          = {2023},
  url           = {https://arxiv.org/abs/2305.14314},
  archiveprefix = {arXiv},
  eprint        = {2305.14314}
}

@article{Huang2023Hallucinations,
  title         = {A Survey on Hallucination in Large Language Models},
  author        = {Huang, Lei and others},
  journal       = {arXiv preprint arXiv:2309.05276},
  year          = {2023},
  url           = {https://arxiv.org/abs/2309.05276},
  archiveprefix = {arXiv},
  eprint        = {2309.05276}
}

@article{Bender2021StochasticParrots,
  title   = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author  = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  journal = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency},
  year    = {2021},
  doi     = {10.1145/3442188.3445922}
}


@article{Mikolov2013Efficient,
  abstract  = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  author    = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  journal   = {1st International Conference on Learning Representations, ICLR 2013 - Workshop Track Proceedings},
  month     = {9},
  publisher = {International Conference on Learning Representations, ICLR},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  url       = {http://arxiv.org/abs/1301.3781},
  year      = {2013}
}

@inproceedings{Sutskever2014Sequence,
  author    = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Sequence to Sequence Learning with Neural Networks},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5a18e133cbf9f257297f410bb7eca942-Paper.pdf},
  volume    = {27},
  year      = {2014}
}
@article{Enyan2024Distributional,
  abstract = {Distributional semantics is the linguistic theory that a word's meaning can be derived from its distribution in natural language (i.e., its use). Language models are commonly viewed as an implementation of distributional semantics, as they are optimized to capture the statistical features of natural language. It is often argued that distributional semantics models should excel at capturing graded/vague meaning based on linguistic conventions, but struggle with truth-conditional reasoning and symbolic processing. We evaluate this claim with a case study on vague (e.g. "many") and exact (e.g. "more than half") quantifiers. Contrary to expectations, we find that, across a broad range of models of various types, LLMs align more closely with human judgements on exact quantifiers versus vague ones. These findings call for a re-evaluation of the assumptions underpinning what distributional semantics models are, as well as what they can capture.},
  author   = {Zhang Enyan and Zewei Wang and Michael A. Lepori and Ellie Pavlick and Helena Aparicio},
  isbn     = {2410.13984v1},
  month    = {10},
  title    = {Are LLMs Models of Distributional Semantics? A Case Study on Quantifiers},
  url      = {http://arxiv.org/abs/2410.13984},
  year     = {2024}
}



@article{Gupta2024,
  author    = {Sonakshi Gupta and Akhlak Mahmood and Pranav Shetty and Aishat Adeboye and Rampi Ramprasad},
  doi       = {10.1038/s43246-024-00708-9},
  issn      = {2662-4443},
  issue     = {1},
  journal   = {Communications Materials},
  keywords  = {Databases,Polymers},
  month     = {12},
  pages     = {269},
  publisher = {Nature Publishing Group},
  title     = {Data extraction from polymer literature using large language models},
  volume    = {5},
  url       = {https://www.nature.com/articles/s43246-024-00708-9},
  year      = {2024}
}


@article{Wei2022Emergent,
  abstract  = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  author    = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
  journal   = {Transactions on Machine Learning Research},
  month     = {10},
  publisher = {Transactions on Machine Learning Research},
  title     = {Emergent Abilities of Large Language Models},
  volume    = {2022-August},
  url       = {http://arxiv.org/abs/2206.07682},
  year      = {2022}
}


@inproceedings{Shaw2018Selfattention,
  abstract  = {Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.},
  author    = {Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
  city      = {Stroudsburg, PA, USA},
  doi       = {10.18653/v1/N18-2074},
  isbn      = {9781948087292},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language
               Technologies, Volume 2 (Short Papers)},
  month     = {3},
  pages     = {464-468},
  publisher = {Association for Computational Linguistics},
  title     = {Self-Attention with Relative Position Representations},
  volume    = {2},
  url       = {http://aclweb.org/anthology/N18-2074},
  year      = {2018}
}


@article{Clark2019Bert,
  title   = {What Does BERT Look At? An Analysis of BERT's Attention},
  author  = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal = {arXiv preprint arXiv:1906.04341},
  year    = {2019}
}

@article{Perez2019Turing,
  title   = {On the Turing Completeness of Modern Neural Network Architectures},
  author  = {P{\'e}rez, Jorge and Marinkovi{\'c}, Javier and Barcel{\'o}, Pablo},
  journal = {arXiv preprint arXiv:1901.03429},
  year    = {2019}
}

@article{Abnar2020Quantifying,
  title   = {Quantifying attention flow in transformers},
  author  = {Abnar, Samira and Zuidema, Willem},
  journal = {arXiv preprint arXiv:2005.00928},
  year    = {2020}
}

@article{Yun2019Transformers,
  title   = {Are transformers universal approximators of sequence-to-sequence functions?},
  author  = {Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  journal = {arXiv preprint arXiv:1912.10077},
  year    = {2019}
}

@article{Jain2019Attention,
  title   = {Attention is not explanation},
  author  = {Jain, Sarthak and Wallace, Byron C},
  journal = {arXiv preprint arXiv:1902.10186},
  year    = {2019}
}

@article{Xiao2023Efficient,
  title   = {Efficient streaming language models with attention sinks},
  author  = {Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal = {arXiv preprint arXiv:2309.17453},
  year    = {2023}
}

@article{Hoffmann2022training,
  title   = {Training compute-optimal large language models},
  author  = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal = {arXiv preprint arXiv:2203.15556},
  year    = {2022}
}


@article{mcculloch1943logical,
  author  = {McCulloch, Warren S. and Pitts, Walter},
  title   = {A logical calculus of the ideas immanent in nervous activity},
  journal = {The Bulletin of Mathematical Biophysics},
  volume  = {5},
  number  = {4},
  pages   = {115--133},
  year    = {1943}
}

@article{rosenblatt1958perceptron,
  author  = {Rosenblatt, Frank},
  title   = {The perceptron: A probabilistic model for information storage and organization in the brain},
  journal = {Psychological Review},
  volume  = {65},
  number  = {6},
  pages   = {386},
  year    = {1958}
}

@book{minsky1969perceptrons,
  author    = {Minsky, Marvin and Papert, Seymour},
  title     = {Perceptrons: An Introduction to Computational Geometry},
  publisher = {MIT Press},
  year      = {1969}
}

@article{rumelhart1986learning,
  author  = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  title   = {Learning representations by back-propagating errors},
  journal = {Nature},
  volume  = {323},
  number  = {6088},
  pages   = {533--536},
  year    = {1986}
}

@inproceedings{krizhevsky2012imagenet,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  title     = {ImageNet classification with deep convolutional neural networks},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {25},
  year      = {2012}
}

@inproceedings{vaswani2017attention,
  author    = {Vaswani, Ashish and others},
  title     = {Attention is all you need},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {30},
  year      = {2017}
}

@misc{liang2022helm,
  author = {Liang, Percy and others},
  title  = {Holistic evaluation of language models},
  note   = {arXiv:2211.09110},
  year   = {2022}
}

@inproceedings{lin2021truthfulqa,
  author    = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  title     = {TruthfulQA: Measuring how models mimic human falsehoods},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  pages     = {3214--3252},
  year      = {2021}
}

@inproceedings{ouyang2022training,
  author    = {Ouyang, Long and others},
  title     = {Training language models to follow instructions with human feedback},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {35},
  pages     = {27730--27744},
  year      = {2022}
}

@inproceedings{heusel2017gans,
  author    = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  title     = {GANs trained by a two time-scale update rule converge to a local Nash equilibrium},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  volume    = {30},
  year      = {2017}
}

@inproceedings{hessel2021clipscore,
  author    = {Hessel, Jack and Holtzman, Ari and Forbes, Maxwell and Bras, Ronan Le and Choi, Yejin},
  title     = {CLIPScore: A reference-free evaluation metric for image captioning},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages     = {7514--7528},
  year      = {2021}
}

@article{gomez2018automatic,
  author  = {G{\'o}mez-Bombarelli, Rafael and others},
  title   = {Automatic chemical design using a data-driven continuous representation of molecules},
  journal = {ACS Central Science},
  volume  = {4},
  number  = {2},
  pages   = {268--276},
  year    = {2018}
}

@article{brown2019guacamol,
  author  = {Brown, Nathan and Fiscato, Massimo and Segler, Marwin H. S. and Vaucher, Alain C.},
  title   = {GuacaMol: Benchmarking models for de novo molecular design},
  journal = {Journal of Chemical Information and Modeling},
  volume  = {59},
  number  = {3},
  pages   = {1096--1108},
  year    = {2019}
}

@article{fedus2022switch,
  author  = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  title   = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  journal = {Journal of Machine Learning Research},
  volume  = {23},
  number  = {120},
  pages   = {1--39},
  year    = {2022}
}

@misc{jiang2024mixtral,
  author = {Jiang, Albert Q. and others},
  title  = {Mixtral of Experts},
  note   = {arXiv:2401.04088},
  year   = {2024}
}

@article{su2024roformer,
  author  = {Su, Jianlin and Lu, Yixuan and Pan, Sheng and Murtadha, Ahmed and Wen, Bo and Liu, Yang},
  title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  journal = {Neurocomputing},
  volume  = {568},
  pages   = {127063},
  year    = {2024}
}

@misc{peng2023yarn,
  author = {Peng, Baolin and Quesnelle, Justin and Fan, Hao and Shi, Elaine},
  title  = {YaRN: Efficient Context Window Extension of Large Language Models},
  note   = {arXiv:2309.00071},
  year   = {2023}
}

@misc{ainslie2023gqa,
  author = {Ainslie, Joshua and others},
  title  = {GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  note   = {arXiv:2305.13245},
  year   = {2023}
}

@inproceedings{dao2022flashattention,
  author    = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  title     = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {35},
  pages     = {16344--16359},
  year      = {2022}
}

@inproceedings{dao2023flashattention2,
  author    = {Dao, Tri},
  title     = {FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2024}
}

@inproceedings{rajbhandari2020zero,
  author    = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  title     = {ZeRO: Memory optimizations toward training trillion parameter models},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '20)},
  year      = {2020}
}

@article{zhao2023pytorch,
  author  = {Zhao, Yuchen and others},
  title   = {PyTorch FSDP: Experiences on scaling fully sharded data parallel},
  journal = {Proceedings of the VLDB Endowment},
  volume  = {16},
  number  = {12},
  pages   = {3848--3860},
  year    = {2023}
}

@misc{kaplan2020scaling,
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and others},
  title  = {Scaling Laws for Neural Language Models},
  note   = {arXiv:2001.08361},
  year   = {2020}
}

@misc{hoffmann2022training,
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and others},
  title  = {Training Compute-Optimal Large Language Models},
  note   = {arXiv:2203.15556},
  year   = {2022}
}

@article{chowdhery2023palm,
  author  = {Chowdhery, Aakanksha and others},
  title   = {PaLM: Scaling Language Modeling with Pathways},
  journal = {Journal of Machine Learning Research},
  volume  = {24},
  number  = {240},
  pages   = {1--113},
  year    = {2023}
}

@inproceedings{hu2021lora,
  author    = {Hu, Edward J. and others},
  title     = {LoRA: Low-Rank Adaptation of Large Language Models},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2022}
}

@inproceedings{leviathan2023fast,
  author    = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  title     = {Fast Inference from Transformers via Speculative Decoding},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
  volume    = {202},
  pages     = {19274--19286},
  year      = {2023}
}

@article{bommasani2022opportunities,
   abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
   author = {Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
   month = {7},
   title = {On the Opportunities and Risks of Foundation Models},
   url = {http://arxiv.org/abs/2108.07258},
   year = {2022}
}


@article{Schaeffer2023emergent,
   abstract = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.},
   author = {Rylan Schaeffer and Brando Miranda and Sanmi Koyejo},
   journal = {Advances in Neural Information Processing Systems},
   month = {5},
   publisher = {Neural information processing systems foundation},
   title = {Are Emergent Abilities of Large Language Models a Mirage?},
   volume = {36},
   url = {http://arxiv.org/abs/2304.15004},
   year = {2023}
}

@article{Chung2022scaling,
   abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
   author = {Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tai and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
   issn = {15337928},
   journal = {Journal of Machine Learning Research},
   keywords = {Bias & Toxicity,Chain-of-Thought Reasoning,Instruction Finetuning,Language Models,Natural Language Processing},
   month = {10},
   publisher = {Microtome Publishing},
   title = {Scaling Instruction-Finetuned Language Models},
   volume = {25},
   url = {https://arxiv.org/pdf/2210.11416},
   year = {2022}
}
