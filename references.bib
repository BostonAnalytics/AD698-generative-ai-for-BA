@book{hurwitz2013big,
  title     = {Big data for dummies},
  author    = {Hurwitz, Judith S and Nugent, Alan and Halper, Fern and Kaufman, Marcia},
  year      = {2013},
  publisher = {John Wiley \& Sons}
}

@book{hurwitz2020cloud,
  title     = {Cloud computing for dummies},
  author    = {Hurwitz, Judith S and Kirsch, Daniel},
  year      = {2020},
  publisher = {John Wiley \& Sons}
}

@book{anderson2015statistics,
  title     = {Statistics for big data for dummies},
  author    = {Anderson, Alan},
  year      = {2015},
  publisher = {John Wiley \& Sons}
}

@book{kohavi2020trustworthy,
  title     = {{Trustworthy online controlled experiments: A practical guide to a/b testing}},
  author    = {Kohavi, Ron and Tang, Diane and Xu, Ya},
  year      = {2020},
  publisher = {Cambridge University Press}
}

@book{debarros2022practical,
  title     = {Practical SQL: A Beginner's Guide to Storytelling with Data},
  author    = {DeBarros, Anthony},
  year      = {2022},
  publisher = {no starch Press}
}
@book{loeliger2012version,
  title     = {Version Control with Git: Powerful tools and techniques for collaborative software development},
  author    = {Loeliger, Jon and McCullough, Matthew},
  year      = {2012},
  publisher = {" O'Reilly Media, Inc."}
}

@article{blischak2016quick,
  title     = {A quick introduction to version control with Git and GitHub},
  author    = {Blischak, John D and Davenport, Emily R and Wilson, Greg},
  journal   = {PLoS computational biology},
  volume    = {12},
  number    = {1},
  pages     = {e1004668},
  year      = {2016},
  publisher = {Public Library of Science San Francisco, CA USA}
}

@book{mcquaid2014git,
  title     = {Git in practice},
  author    = {McQuaid, Mike},
  year      = {2014},
  publisher = {Simon and Schuster}
}

@book{ankam2016big,
  title     = {Big data analytics},
  author    = {Ankam, Venkat},
  year      = {2016},
  publisher = {Packt Publishing Ltd}
}
@article{hainesmodern,
  title     = {Modern Data Engineering with Apache Spark},
  author    = {Haines, Scott},
  publisher = {Springer}
}
@book{erl2013cloud,
  title     = {Cloud computing: concepts, technology \& architecture},
  author    = {Erl, Thomas and Puttini, Ricardo and Mahmood, Zaigham},
  year      = {2013},
  publisher = {Pearson Education}
}

@book{erl2016big,
  title     = {Big data fundamentals: concepts, drivers \& techniques},
  author    = {Erl, Thomas and Khattak, Wajid and Buhler, Paul},
  year      = {2016},
  publisher = {Prentice Hall Press}
}

@book{shroff2010enterprise,
  title     = {Enterprise cloud computing: technology, architecture, applications},
  author    = {Shroff, Gautam},
  year      = {2010},
  publisher = {Cambridge university press}
}

@book{bhowmik2017cloud,
  title     = {Cloud computing},
  author    = {Bhowmik, Sandeep},
  year      = {2017},
  publisher = {Cambridge University Press}
}
@book{alla2018big,
  title     = {Big Data Analytics with Hadoop 3: Build highly effective analytics solutions to gain valuable insight into your big data},
  author    = {Alla, Sridhar},
  year      = {2018},
  publisher = {Packt Publishing Ltd}
}

@book{card1999readings,
  title     = {Readings in information visualization: using vision to think},
  author    = {Card, Stuart K and Mackinlay, Jock and Shneiderman, Ben},
  year      = {1999},
  publisher = {Morgan Kaufmann}
}

@article{anscombe1973graphs,
  title     = {Graphs in statistical analysis},
  author    = {Anscombe, Francis J},
  journal   = {The american statistician},
  volume    = {27},
  number    = {1},
  pages     = {17--21},
  year      = {1973},
  publisher = {Taylor \& Francis}
}

@inproceedings{matejka2017same,
  title     = {Same stats, different graphs: generating datasets with varied appearance and identical statistics through simulated annealing},
  author    = {Matejka, Justin and Fitzmaurice, George},
  booktitle = {Proceedings of the 2017 CHI conference on human factors in computing systems},
  pages     = {1290--1294},
  year      = {2017}
}

@article{Tidwell2020,
  abstract = {3rd edition. It's not easy to design good application interfaces in a world where companies must create compelling, seamless user experiences across an exploding number of channels, screens, and contexts. Design patterns, design systems, and component-based UI frameworks have emerged and now rapidly evolve to meet the challenge. This bestselling book is one of the few reliable sources to help you navigate through the maze of design options. By capturing UI best practices and reusable ideas as design patterns, Designing Interfaces provides solutions to common design problems that you can tailor to the situation at hand. This updated edition includes patterns for mobile apps and social media, as well as web applications and desktop software. Each pattern contains full-color examples and practical design advice that you can use immediately. Experienced designers can use this guide as a sourcebook of ideas; novices will find a roadmap to the world of interface and interaction design.},
  author   = {Jennifer Tidwell and Charles Brewer and Aynne Valencia},
  isbn     = {9781492051961},
  issn     = {19381425},
  issue    = {4},
  journal  = {MRS Bulletin},
  pages    = {27-30},
  title    = {Designing Interfaces 3rd edition},
  volume   = {16},
  url      = {https://www.oreilly.com/library/view/designing-interfaces-3rd/9781492051954/},
  year     = {2020}
}
@article{Ambrose2019,
  author    = {Gavin. Ambrose and Paul. Harris},
  isbn      = {9781350035270},
  pages     = {288},
  publisher = {Bloomsbury Visual Arts},
  title     = {The Visual Dictionary of Graphic Design},
  url       = {https://www.bloomsbury.com/us/visual-dictionary-of-graphic-design-9781350035270/},
  year      = {2019}
}
@article{Resnick2003,
  abstract  = {From the Publisher: Complete coverage of basic design principles illustrated by student examples. Design for Communication offers a unique approach to mastering the basic design principles, conceptual problem-solving methods, and critical-thinking skills that distinguish graphic designers from desktop technicians. This book presents forty-two basic to advanced graphic design and typography assignments collaboratively written by college educators to teach the fundamental processes, concepts, and techniques through hands-on applications. Each assignment is illustrated with actual student solutions, and each includes a process narrative and an educator's critical analysis revealing the reasoning behind the creative strategies employed by each individual student solution. Assignments are organized from basic to advanced within six sections: The elements and principles of design; Typography as image; Creative word play; Word and image; Grid and visual hierarchy; Visual advocacy. Design for Communication is a highly visual resource of instruction, information, ideas, and inspiration for students and professionals. Introduction -- What is graphic design -- What do graphic designers do -- I want to be a graphic designer-where do I begin -- Design process -- Why bother with such a long process when I just like to make things -- Why should I do these assignments -- Section 1: Elements And Principles Of Design -- Star symbol / Susan Merritt -- Object semantics / Kermit Bailey -- Symbol design / Lisa Fontaine -- Lettermark / Susan Merritt -- Vinyletteror / Kenneth Fitzgerald -- Letterform as shape / Jan Conradi -- Concert poster / Arnold Holland -- Design history chair / Hank Richardson -- Section 2: Typography As Image -- Shaping words / Richard Ybarra -- Newspaper stories-a typographic workshop / Jurgen Hefele -- Typographic self-portrait / Esen Karol -- Typographic self-portrait / Elizabeth Resnick -- Typeface poster / Hyun Mee Kim -- Directions poster / Frank Baseman -- Poetry in motion / Elizabeth Resnick -- Section 3: Creative Wordplay -- Descriptive pairs / Elizabeth Resnick -- Letters as image / Hyun Mee Kim -- Concrete poetry / Kenneth Fitzgerald -- Arthur Murray Dance advertisement / Frank Baseman -- CD cover: typographical music / Heather Corcoran -- Section 4: Word and image -- Word and image / Elizabeth Resnick -- Book cover design: the Interpretation of Dreams by Sigmund Freud / Elizabeth Resnick -- Book cover design: Einstein's Dreams by Alan Lightman / Elizabeth Resnick -- Postage stamp design: a celebration of cultural diversity in America / Elizabeth Resnick and Glenn Berger -- Postage stamp design: a celebration of American primary education: reading, writing, mathematics and science / Elizabeth Resnick -- Stamp design / Michael Burke -- Sculpture poster / Tom Briggs -- Gardening poster / Karen Bernstein -- Kitchen of meaning exhibition poster / Kermit Bailey -- Section 5: Grid and visual hierarchy -- Typographic history spread / Elizabeth Resnick -- Two-sided typography poster / Judith Aronson -- Three type specimens / Carol Sogard -- Visual poetry calendar / Peggy Re -- Science lecture series posters / Elizabeth Resnick -- Book: Lewis & Clark / Heather Corcoran -- Journey journal / Gulizar Cepoglu -- Section 6: Visual Advocacy -- Literacy poster: learn to read / Elizabeth Resnick -- Human rights poster / Elizabeth Resnick -- Designing dissent: advocacy poster series / Elizabeth Resnick -- Happy Deutschland / Jurgen Hefele -- Three-dimensional direct response (mail) solicitation / Elizabeth Resnick and Glenn Berger -- Tolerance Bus Shelter Poster / Frank Baseman -- Bibliography -- Instructor contact information -- Index.},
  author    = {Elizabeth. Resnick},
  isbn      = {978-0-471-41829-0},
  pages     = {255},
  publisher = {Wiley & Sons},
  title     = {Design for communication : conceptual graphic design basics},
  url       = {https://www.wiley.com/en-us/Design+for+Communication%3A+Conceptual+Graphic+Design+Basics-p-9780471418290},
  year      = {2003}
}


@book{tufte1983visual,
  title     = {The visual display of quantitative information},
  author    = {Tufte, Edward R and Graves-Morris, Peter R},
  volume    = {2},
  number    = {9},
  year      = {1983},
  publisher = {Graphics press Cheshire, CT}
}

@article{tufte1991envisioning,
  title     = {Envisioning information},
  author    = {Tufte, Edward R},
  journal   = {Optometry and Vision Science},
  volume    = {68},
  number    = {4},
  pages     = {322--324},
  year      = {1991},
  publisher = {LWW}
}

@book{tufte2006beautiful,
  title     = {Beautiful evidence},
  author    = {Tufte, Edward R and Tufte, Edward Rolf},
  volume    = {1},
  year      = {2006},
  publisher = {Graphics Press Cheshire, CT}
}

@book{tufte1974data,
  title     = {Data analysis for politics and policy},
  author    = {Tufte, Edward R},
  year      = {1974},
  publisher = {Prentice-Hall Englewood Cliffs, NJ}
}
@book{tufte1997visual,
  title     = {Visual explanations: Images and quantities, evidence and narrative},
  author    = {Tufte, Edward R},
  year      = {1997},
  publisher = {Graphics Press}
}

@book{brase2016understanding,
  title     = {Understanding basic statistics},
  author    = {Brase, Charles Henry and Brase, Corrinne Pellillo},
  year      = {2011},
  edition   = {10th},
  publisher = {Cengage Learning Hampshire, UK}
}


@article{chaplin2004geographic,
  title     = {Geographic distribution of environmental factors influencing human skin coloration},
  author    = {Chaplin, George},
  journal   = {American Journal of Physical Anthropology: The Official Publication of the American Association of Physical Anthropologists},
  volume    = {125},
  number    = {3},
  pages     = {292--302},
  year      = {2004},
  publisher = {Wiley Online Library}
}

@book{tufte1983visual,
  author    = {Edward R. Tufte},
  title     = {The Visual Display of Quantitative Information},
  year      = {1983},
  publisher = {Graphics Press},
  address   = {Cheshire, Connecticut}
}

@book{cleveland1994elements,
  author    = {William S. Cleveland},
  title     = {The Elements of Graphing Data},
  year      = {1994},
  publisher = {Hobart Press},
  address   = {Summit, New Jersey}
}

@book{wilkinson1999grammar,
  author    = {Leland Wilkinson},
  title     = {The Grammar of Graphics},
  year      = {1999},
  publisher = {Springer},
  address   = {New York}
}

@book{wickham2009ggplot2,
  author    = {Hadley Wickham},
  title     = {ggplot2: Elegant Graphics for Data Analysis},
  year      = {2009},
  publisher = {Springer},
  address   = {New York}
}

@article{anscombe1973graphs,
  author    = {F.J. Anscombe},
  title     = {Graphs in Statistical Analysis},
  journal   = {The American Statistician},
  volume    = {27},
  number    = {1},
  pages     = {17--21},
  year      = {1973},
  publisher = {American Statistical Association}
}

@article{matejka2017same,
  author  = {Justin Matejka and George Fitzmaurice},
  title   = {Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing},
  journal = {ACM CHI Conference on Human Factors in Computing Systems},
  year    = {2017},
  pages   = {1290--1294}
}

@book{playfair1786atlas,
  author    = {William Playfair},
  title     = {The Commercial and Political Atlas},
  year      = {1786},
  publisher = {J. Debrett},
  address   = {London}
}

@article{card1999readings,
  author    = {Stuart K. Card, Jock D. Mackinlay, and Ben Shneiderman},
  title     = {Readings in Information Visualization: Using Vision to Think},
  year      = {1999},
  publisher = {Morgan Kaufmann},
  address   = {San Francisco, CA}
}

@book{yau2011data,
  author    = {Nathan Yau},
  title     = {Visualize This: The FlowingData Guide to Design, Visualization, and Statistics},
  year      = {2011},
  publisher = {Wiley},
  address   = {Indianapolis, IN}
}

@book{minard1869napoleon,
  author    = {Charles Joseph Minard},
  title     = {Carte figurative des pertes successives en hommes de l'Armée Française dans la campagne de Russie 1812-1813},
  year      = {1869},
  publisher = {Self-published},
  address   = {Paris}
}

@article{franconeri2021data,
  author    = {Steven Franconeri},
  title     = {ExperCeption Dot Net: Data Visualization Quick Reference},
  year      = {2021},
  publisher = {Northwestern University},
  url       = {https://static1.squarespace.com/static/54c9ed77e4b03009ff855b12/t/60b9af61c8f89f0d0d4e5d15/1622781794396/Franconeri_ExperCeptionDotNet_DataVisQuickRef+%283%29.pdf}
}

@article{nightingale1858mortality,
  author  = {Florence Nightingale},
  title   = {Diagram of the Causes of Mortality in the Army in the East},
  year    = {1858},
  journal = {Journal of the Statistical Society of London},
  volume  = {21},
  number  = {1},
  pages   = {1--26}
}

@book{abela2015slide,
  author    = {Andrew Abela},
  title     = {Announcing the Slide Chooser},
  year      = {2015},
  publisher = {Extreme Presentation},
  url       = {https://extremepresentation.typepad.com/blog/2015/01/announcing-the-slide-chooser.html}
}

@book{schwabish2014continuum,
  author    = {Jon Schwabish and Severino Ribecca},
  title     = {The Graphic Continuum},
  year      = {2014},
  publisher = {PolicyViz},
  url       = {https://policyviz.com/2014/09/09/graphic-continuum/}
}

@misc{holtz2022data,
  author = {Yan Holtz and Conor Healy},
  title  = {From Data to Viz},
  year   = {2022},
  url    = {https://www.data-to-viz.com/}
}

@misc{ferdio2022dataviz,
  author = {Ferdio},
  title  = {The Data Viz Project},
  year   = {2022},
  url    = {https://datavizproject.com/}
}

@inproceedings{rendle2010factorization,
  title        = {Factorization machines},
  author       = {Rendle, Steffen},
  booktitle    = {2010 IEEE International conference on data mining},
  pages        = {995--1000},
  year         = {2010},
  organization = {IEEE}
}


@article{Zewe2023,
  author  = {Adam Zewe},
  journal = {MIT News},
  month   = {11},
  title   = {Explained: Generative AI},
  url     = {https://news.mit.edu/2023/explained-generative-ai-1109},
  year    = {2023}
}


@article{Wang2023,
  title         = {A survey of the evolution of language model-based dialogue systems},
  author        = {Wang, Hongru and Wang, Lingzhi and Du, Yiming and Chen, Liang and Zhou, Jingyan and Wang, Yufei and Wong, Kam-Fai},
  journal       = {arXiv preprint arXiv:2311.16789},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2311.16789}
}


@article{talib2016text,
  title     = {Text mining: techniques, applications and issues},
  author    = {Talib, Ramzan and Hanif, Muhammad Kashif and Ayesha, Shaeela and Fatima, Fakeeha},
  journal   = {International journal of advanced computer science and applications},
  volume    = {7},
  number    = {11},
  pages     = {414--418},
  year      = {2016},
  publisher = {The Science and Information Organization}
}



@article{Bengio2003NeuralLM,
  title   = {A Neural Probabilistic Language Model},
  author  = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  journal = {Journal of Machine Learning Research},
  volume  = {3},
  pages   = {1137--1155},
  year    = {2003},
  url     = {https://www.jmlr.org/papers/v3/bengio03a.html}
}

@article{TurneyPantel2010VSM,
  title   = {From Frequency to Meaning: Vector Space Models of Semantics},
  author  = {Turney, Peter D. and Pantel, Patrick},
  journal = {Journal of Artificial Intelligence Research},
  volume  = {37},
  pages   = {141--188},
  year    = {2010},
  doi     = {10.1613/jair.2934}
}

@article{Mikolov2013Word2Vec,
  title         = {Distributed Representations of Words and Phrases and their Compositionality},
  author        = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeff},
  booktitle     = {Advances in Neural Information Processing Systems},
  volume        = {26},
  year          = {2013},
  url           = {https://arxiv.org/abs/1310.4546},
  archiveprefix = {arXiv},
  eprint        = {1310.4546}
}

@article{Vaswani2017Attention,
  title         = {Attention Is All You Need},
  author        = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and others},
  booktitle     = {Advances in Neural Information Processing Systems},
  volume        = {30},
  year          = {2017},
  url           = {https://arxiv.org/abs/1706.03762},
  archiveprefix = {arXiv},
  eprint        = {1706.03762}
}

@article{Kaplan2020Scaling,
  title         = {Scaling Laws for Neural Language Models},
  author        = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and others},
  journal       = {arXiv preprint arXiv:2001.08361},
  year          = {2020},
  url           = {https://arxiv.org/abs/2001.08361},
  archiveprefix = {arXiv},
  eprint        = {2001.08361}
}

@article{Zhao2024LLMSurvey,
  title         = {A Survey of Large Language Models},
  author        = {Zhao, Wayne Xin and Zhou, Kun and Li, Jun and others},
  journal       = {ACM Computing Surveys},
  year          = {2024},
  url           = {https://arxiv.org/abs/2303.18223},
  archiveprefix = {arXiv},
  eprint        = {2303.18223}
}

@article{Collobert2011NLPFromScratch,
  title   = {Natural Language Processing (Almost) from Scratch},
  author  = {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and others},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  pages   = {2493--2537},
  year    = {2011},
  url     = {https://www.jmlr.org/papers/v12/collobert11a.html}
}

@article{Kogan2019BusinessIE,
  title   = {Text Mining and Information Extraction in Business Intelligence},
  author  = {Kogan, Shimon and Levin, Dimitry and Routledge, Bryan R. and Sagi, Jacob and Smith, Noah A.},
  journal = {Foundations and Trends in Accounting},
  volume  = {13},
  number  = {1},
  pages   = {1--141},
  year    = {2019},
  doi     = {10.1561/1400000058}
}

@book{Gentzkow2019TextAsData,
  title     = {Text as Data},
  author    = {Gentzkow, Matthew and Kelly, Bryan and Taddy, Matt},
  publisher = {Journal of Economic Perspectives},
  volume    = {33},
  number    = {3},
  pages     = {3--28},
  year      = {2019},
  doi       = {10.1257/jep.33.3.3}
}

@article{Schulhoff2025PromptReport,
  title         = {The Prompt Report: A Systematic Survey of Prompting Techniques},
  author        = {Schulhoff, Sander and others},
  journal       = {arXiv preprint arXiv:2406.06608},
  year          = {2025},
  url           = {https://arxiv.org/abs/2406.06608},
  archiveprefix = {arXiv},
  eprint        = {2406.06608}
}

@article{Wei2022CoT,
  title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author        = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and others},
  journal       = {arXiv preprint arXiv:2201.11903},
  year          = {2022},
  url           = {https://arxiv.org/abs/2201.11903},
  archiveprefix = {arXiv},
  eprint        = {2201.11903}
}

@article{Khattab2023DSPy,
  title         = {DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author        = {Khattab, Omar and others},
  journal       = {arXiv preprint arXiv:2310.03714},
  year          = {2023},
  url           = {https://arxiv.org/abs/2310.03714},
  archiveprefix = {arXiv},
  eprint        = {2310.03714}
}

@article{Ouyang2022RLHF,
  title         = {Training Language Models with Human Feedback},
  author        = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and others},
  journal       = {arXiv preprint arXiv:2203.02155},
  year          = {2022},
  url           = {https://arxiv.org/abs/2203.02155},
  archiveprefix = {arXiv},
  eprint        = {2203.02155}
}

@article{Lewis2020RAG,
  title         = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  author        = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and others},
  journal       = {Advances in Neural Information Processing Systems},
  volume        = {33},
  year          = {2020},
  url           = {https://arxiv.org/abs/2005.11401},
  archiveprefix = {arXiv},
  eprint        = {2005.11401}
}

@article{Barnett2024RAGFailures,
  title         = {Seven Failure Points When Engineering a Retrieval-Augmented Generation System},
  author        = {Barnett, Joshua and others},
  journal       = {arXiv preprint arXiv:2401.05856},
  year          = {2024},
  url           = {https://arxiv.org/abs/2401.05856},
  archiveprefix = {arXiv},
  eprint        = {2401.05856}
}

@article{Asai2023SelfRAG,
  title         = {Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection},
  author        = {Asai, Akari and others},
  journal       = {arXiv preprint arXiv:2310.11511},
  year          = {2023},
  url           = {https://arxiv.org/abs/2310.11511},
  archiveprefix = {arXiv},
  eprint        = {2310.11511}
}

@article{Hu2021LoRA,
  title         = {LoRA: Low-Rank Adaptation of Large Language Models},
  author        = {Hu, Edward and Shen, Yelong and Wallis, Phillip and others},
  journal       = {arXiv preprint arXiv:2106.09685},
  year          = {2021},
  url           = {https://arxiv.org/abs/2106.09685},
  archiveprefix = {arXiv},
  eprint        = {2106.09685}
}

@article{Dettmers2023QLoRA,
  title         = {QLoRA: Efficient Finetuning of Quantized LLMs},
  author        = {Dettmers, Tim and others},
  journal       = {arXiv preprint arXiv:2305.14314},
  year          = {2023},
  url           = {https://arxiv.org/abs/2305.14314},
  archiveprefix = {arXiv},
  eprint        = {2305.14314}
}

@article{Huang2023Hallucinations,
  title         = {A Survey on Hallucination in Large Language Models},
  author        = {Huang, Lei and others},
  journal       = {arXiv preprint arXiv:2309.05276},
  year          = {2023},
  url           = {https://arxiv.org/abs/2309.05276},
  archiveprefix = {arXiv},
  eprint        = {2309.05276}
}

@article{Bender2021StochasticParrots,
  title   = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author  = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  journal = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency},
  year    = {2021},
  doi     = {10.1145/3442188.3445922}
}


@article{Raymond1999CathedralBazaar,
  title   = {The Cathedral and the Bazaar},
  author  = {Raymond, Eric S.},
  journal = {First Monday},
  volume  = {3},
  number  = {3},
  year    = {1999},
  url     = {https://firstmonday.org/ojs/index.php/fm/article/view/578}
}

@article{Wilson2014BestPractices,
  title   = {Best Practices for Scientific Computing},
  author  = {Wilson, Greg and Aruliah, D. A. and Brown, C. T. and others},
  journal = {PLoS Biology},
  volume  = {12},
  number  = {1},
  year    = {2014},
  doi     = {10.1371/journal.pbio.1001745}
}

@article{Peng2011ReproducibleResearch,
  title   = {Reproducible Research in Computational Science},
  author  = {Peng, Roger D.},
  journal = {Science},
  volume  = {334},
  number  = {6060},
  pages   = {1226--1227},
  year    = {2011},
  doi     = {10.1126/science.1213847}
}

@article{Stodden2014PracticeReproducible,
  title     = {The Practice of Reproducible Research: Case Studies and Lessons from the Data-Intensive Sciences},
  author    = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger},
  journal   = {Computing in Science \& Engineering},
  year      = {2014},
  publisher = {IEEE}
}

@article{Stodden2016ComputationalRepro,
  title   = {Enhancing Reproducibility for Computational Methods},
  author  = {Stodden, Victoria and McNutt, Marcia and Bailey, David H. and others},
  journal = {Science},
  volume  = {354},
  number  = {6317},
  year    = {2016},
  doi     = {10.1126/science.aah6168}
}

@book{Knuth1984LiterateProgramming,
  title     = {Literate Programming},
  author    = {Knuth, Donald E.},
  publisher = {CSLI},
  year      = {1984}
}

@article{Firth1957Distributional,
  title   = {A Synopsis of Linguistic Theory 1930–1955},
  author  = {Firth, J. R.},
  journal = {Studies in Linguistic Analysis},
  year    = {1957}
}

@article{Bubeck2023SparksAGI,
  title         = {Sparks of Artificial General Intelligence: Early Experiments with GPT-4},
  author        = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and others},
  journal       = {arXiv preprint arXiv:2303.12712},
  year          = {2023},
  url           = {https://arxiv.org/abs/2303.12712},
  archiveprefix = {arXiv},
  eprint        = {2303.12712}
}

@article{OpenAI2023GPT4V,
  title   = {GPT-4V(ision) System Card},
  author  = {{OpenAI}},
  journal = {OpenAI Technical Report},
  year    = {2023},
  url     = {https://openai.com/research/gpt-4v-system-card}
}

@article{Kirillov2023SAM,
  title         = {Segment Anything},
  author        = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and others},
  journal       = {arXiv preprint arXiv:2304.02643},
  year          = {2023},
  url           = {https://arxiv.org/abs/2304.02643},
  archiveprefix = {arXiv},
  eprint        = {2304.02643}
}

@article{Zhang2023MultimodalCoT,
  title         = {Multimodal Chain-of-Thought Reasoning in Large Language Models},
  author        = {Zhang, Han and others},
  journal       = {arXiv preprint arXiv:2302.00923},
  year          = {2023},
  url           = {https://arxiv.org/abs/2302.00923},
  archiveprefix = {arXiv},
  eprint        = {2302.00923}
}

@article{Liu2023ContextWindow,
  title         = {Lost in the Middle: How Language Models Use Long Contexts},
  author        = {Liu, Nelson F. and others},
  journal       = {arXiv preprint arXiv:2307.03172},
  year          = {2023},
  url           = {https://arxiv.org/abs/2307.03172},
  archiveprefix = {arXiv},
  eprint        = {2307.03172}
}

@article{Ding2023LongNet,
  title         = {LongNet: Scaling Transformers to 1,000,000,000 Tokens},
  author        = {Ding, Shuai and others},
  journal       = {arXiv preprint arXiv:2307.02486},
  year          = {2023},
  url           = {https://arxiv.org/abs/2307.02486},
  archiveprefix = {arXiv},
  eprint        = {2307.02486}
}

@article{Xu2024LazyLLM,
  title         = {LazyLLM: Dynamic Token Budgeting for Efficient LLM Inference},
  author        = {Xu, Rui and others},
  journal       = {arXiv preprint arXiv:2402.07864},
  year          = {2024},
  url           = {https://arxiv.org/abs/2402.07864},
  archiveprefix = {arXiv},
  eprint        = {2402.07864}
}

@article{Yan2024CorrectiveRAG,
  title         = {Corrective Retrieval Augmented Generation},
  author        = {Yan, Yukun and others},
  journal       = {arXiv preprint arXiv:2401.15884},
  year          = {2024},
  url           = {https://arxiv.org/abs/2401.15884},
  archiveprefix = {arXiv},
  eprint        = {2401.15884}
}

@article{Sarmah2024HybridRAG,
  title         = {HybridRAG: Integrating Retrieval and Reasoning for Knowledge-Intensive Tasks},
  author        = {Sarmah, Ankit and others},
  journal       = {arXiv preprint arXiv:2402.15012},
  year          = {2024},
  url           = {https://arxiv.org/abs/2402.15012},
  archiveprefix = {arXiv},
  eprint        = {2402.15012}
}

@article{Blei2006DynamicTopics,
  title   = {Dynamic Topic Models},
  author  = {Blei, David M. and Lafferty, John D.},
  journal = {Proceedings of the 23rd International Conference on Machine Learning},
  year    = {2006}
}

@article{Hamilton2016SemanticChange,
  title   = {Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change},
  author  = {Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
  journal = {ACL},
  year    = {2016}
}

@article{Gu2024LLMJudge,
  title         = {LLM-as-a-Judge: Evaluating Language Models with Language Models},
  author        = {Gu, Shisheng and others},
  journal       = {arXiv preprint arXiv:2402.04788},
  year          = {2024},
  url           = {https://arxiv.org/abs/2402.04788},
  archiveprefix = {arXiv},
  eprint        = {2402.04788}
}

@article{Dhuliawala2023CoVe,
  title         = {Chain-of-Verification Reduces Hallucination in Large Language Models},
  author        = {Dhuliawala, Shehzaad and others},
  journal       = {arXiv preprint arXiv:2309.11495},
  year          = {2023},
  url           = {https://arxiv.org/abs/2309.11495},
  archiveprefix = {arXiv},
  eprint        = {2309.11495}
}



@article{Raymond1999CathedralBazaar,
  title   = {The Cathedral and the Bazaar},
  author  = {Raymond, Eric S.},
  journal = {First Monday},
  volume  = {3},
  number  = {3},
  year    = {1999},
  url     = {https://firstmonday.org/ojs/index.php/fm/article/view/578},
  doi     = {10.5210/fm.v3i2.578}
}

@article{Wilson2014BestPractices,
  title   = {Best Practices for Scientific Computing},
  author  = {Wilson, Greg and Aruliah, D. A. and Brown, C. T. and others},
  journal = {PLoS Biology},
  volume  = {12},
  number  = {1},
  year    = {2014},
  doi     = {10.1371/journal.pbio.1001745}
}

@article{Peng2011ReproducibleResearch,
  title   = {Reproducible Research in Computational Science},
  author  = {Peng, Roger D.},
  journal = {Science},
  volume  = {334},
  number  = {6060},
  pages   = {1226--1227},
  year    = {2011},
  doi     = {10.1126/science.1213847}
}

@article{Stodden2014PracticeReproducible,
  title     = {The Practice of Reproducible Research: Case Studies and Lessons from the Data-Intensive Sciences},
  author    = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger},
  journal   = {Computing in Science \& Engineering},
  year      = {2014},
  publisher = {IEEE}
}

@article{Stodden2016ComputationalRepro,
  title   = {Enhancing Reproducibility for Computational Methods},
  author  = {Stodden, Victoria and McNutt, Marcia and Bailey, David H. and others},
  journal = {Science},
  volume  = {354},
  number  = {6317},
  year    = {2016},
  doi     = {10.1126/science.aah6168}
}

@book{Knuth1984LiterateProgramming,
  title     = {Literate Programming},
  author    = {Knuth, Donald E.},
  publisher = {CSLI},
  year      = {1984}
}

@article{Firth1957Distributional,
  title   = {A Synopsis of Linguistic Theory 1930–1955},
  author  = {Firth, J. R.},
  journal = {Studies in Linguistic Analysis},
  year    = {1957}
}

@article{Bubeck2023SparksAGI,
  title         = {Sparks of Artificial General Intelligence: Early Experiments with GPT-4},
  author        = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and others},
  journal       = {arXiv preprint arXiv:2303.12712},
  year          = {2023},
  url           = {https://arxiv.org/abs/2303.12712},
  archiveprefix = {arXiv},
  eprint        = {2303.12712}
}

@article{OpenAI2023GPT4V,
  title   = {GPT-4V(ision) System Card},
  author  = {{OpenAI}},
  journal = {OpenAI Technical Report},
  year    = {2023},
  url     = {https://openai.com/research/gpt-4v-system-card}
}

@article{Kirillov2023SAM,
  title         = {Segment Anything},
  author        = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and others},
  journal       = {arXiv preprint arXiv:2304.02643},
  year          = {2023},
  url           = {https://arxiv.org/abs/2304.02643},
  archiveprefix = {arXiv},
  eprint        = {2304.02643}
}

@article{Zhang2023MultimodalCoT,
  title         = {Multimodal Chain-of-Thought Reasoning in Large Language Models},
  author        = {Zhang, Han and others},
  journal       = {arXiv preprint arXiv:2302.00923},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2302.00923}
}

@article{Liu2023ContextWindow,
  title         = {Lost in the Middle: How Language Models Use Long Contexts},
  author        = {Liu, Nelson F. and others},
  journal       = {arXiv preprint arXiv:2307.03172},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2307.03172}
}

@article{Ding2023LongNet,
  title         = {LongNet: Scaling Transformers to 1,000,000,000 Tokens},
  author        = {Ding, Shuai and others},
  journal       = {arXiv preprint arXiv:2307.02486},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2307.02486}
}

@article{Xu2024LazyLLM,
  title         = {LazyLLM: Dynamic Token Budgeting for Efficient LLM Inference},
  author        = {Xu, Rui and others},
  journal       = {arXiv preprint arXiv:2402.07864},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.07864}
}

@article{Yan2024CorrectiveRAG,
  title         = {Corrective Retrieval Augmented Generation},
  author        = {Yan, Yukun and others},
  journal       = {arXiv preprint arXiv:2401.15884},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2401.15884}
}

@article{Sarmah2024HybridRAG,
  title         = {HybridRAG: Integrating Retrieval and Reasoning for Knowledge-Intensive Tasks},
  author        = {Sarmah, Ankit and others},
  journal       = {arXiv preprint arXiv:2402.15012},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.15012}
}

@article{Blei2006DynamicTopics,
  title   = {Dynamic Topic Models},
  author  = {Blei, David M. and Lafferty, John D.},
  journal = {Proceedings of the 23rd International Conference on Machine Learning},
  year    = {2006}
}

@article{Hamilton2016SemanticChange,
  title   = {Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change},
  author  = {Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
  journal = {ACL},
  year    = {2016}
}

@article{Gu2024LLMJudge,
  title         = {LLM-as-a-Judge: Evaluating Language Models with Language Models},
  author        = {Gu, Shisheng and others},
  journal       = {arXiv preprint arXiv:2402.04788},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.04788}
}

@article{Dhuliawala2023CoVe,
  title         = {Chain-of-Verification Reduces Hallucination in Large Language Models},
  author        = {Dhuliawala, Shehzaad and others},
  journal       = {arXiv preprint arXiv:2309.11495},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2309.11495}
}

@article{Bengio2003NeuralLM,
  title   = {A Neural Probabilistic Language Model},
  author  = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  journal = {Journal of Machine Learning Research},
  volume  = {3},
  pages   = {1137--1155},
  year    = {2003},
  url     = {https://www.jmlr.org/papers/v3/bengio03a.html}
}

@article{TurneyPantel2010VSM,
  title   = {From Frequency to Meaning: Vector Space Models of Semantics},
  author  = {Turney, Peter D. and Pantel, Patrick},
  journal = {Journal of Artificial Intelligence Research},
  volume  = {37},
  pages   = {141--188},
  year    = {2010},
  doi     = {10.1613/jair.2934}
}

@article{Mikolov2013Word2Vec,
  title         = {Distributed Representations of Words and Phrases and their Compositionality},
  author        = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeff},
  booktitle     = {Advances in Neural Information Processing Systems},
  volume        = {26},
  year          = {2013},
  url           = {https://arxiv.org/abs/1310.4546},
  archiveprefix = {arXiv},
  eprint        = {1310.4546}
}

@article{Vaswani2017Attention,
  title         = {Attention Is All You Need},
  author        = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and others},
  booktitle     = {Advances in Neural Information Processing Systems},
  volume        = {30},
  year          = {2017},
  url           = {https://arxiv.org/abs/1706.03762},
  archiveprefix = {arXiv},
  eprint        = {1706.03762}
}

@article{Kaplan2020Scaling,
  title         = {Scaling Laws for Neural Language Models},
  author        = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and others},
  journal       = {arXiv preprint arXiv:2001.08361},
  year          = {2020},
  url           = {https://arxiv.org/abs/2001.08361},
  archiveprefix = {arXiv},
  eprint        = {2001.08361}
}

@article{Zhao2024LLMSurvey,
  title         = {A Survey of Large Language Models},
  author        = {Zhao, Wayne Xin and Zhou, Kun and Li, Jun and others},
  journal       = {ACM Computing Surveys},
  year          = {2024},
  url           = {https://arxiv.org/abs/2303.18223},
  archiveprefix = {arXiv},
  eprint        = {2303.18223}
}

@article{Collobert2011NLPFromScratch,
  title   = {Natural Language Processing (Almost) from Scratch},
  author  = {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and others},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  pages   = {2493--2537},
  year    = {2011},
  url     = {https://www.jmlr.org/papers/v12/collobert11a.html}
}

@article{Kogan2019BusinessIE,
  title   = {Text Mining and Information Extraction in Business Intelligence},
  author  = {Kogan, Shimon and Levin, Dimitry and Routledge, Bryan R. and Sagi, Jacob and Smith, Noah A.},
  journal = {Foundations and Trends in Accounting},
  volume  = {13},
  number  = {1},
  pages   = {1--141},
  year    = {2019},
  doi     = {10.1561/1400000058}
}

@book{Gentzkow2019TextAsData,
  title     = {Text as Data},
  author    = {Gentzkow, Matthew and Kelly, Bryan and Taddy, Matt},
  publisher = {Journal of Economic Perspectives},
  volume    = {33},
  number    = {3},
  pages     = {3--28},
  year      = {2019},
  doi       = {10.1257/jep.33.3.3}
}

@article{Schulhoff2025PromptReport,
  title         = {The Prompt Report: A Systematic Survey of Prompting Techniques},
  author        = {Schulhoff, Sander and others},
  journal       = {arXiv preprint arXiv:2406.06608},
  year          = {2025},
  url           = {https://arxiv.org/abs/2406.06608},
  archiveprefix = {arXiv},
  eprint        = {2406.06608}
}

@article{Wei2022CoT,
  title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author        = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and others},
  journal       = {arXiv preprint arXiv:2201.11903},
  year          = {2022},
  url           = {https://arxiv.org/abs/2201.11903},
  archiveprefix = {arXiv},
  eprint        = {2201.11903}
}

@article{Khattab2023DSPy,
  title         = {DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author        = {Khattab, Omar and others},
  journal       = {arXiv preprint arXiv:2310.03714},
  year          = {2023},
  url           = {https://arxiv.org/abs/2310.03714},
  archiveprefix = {arXiv},
  eprint        = {2310.03714}
}

@article{Ouyang2022RLHF,
  title         = {Training Language Models with Human Feedback},
  author        = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and others},
  journal       = {arXiv preprint arXiv:2203.02155},
  year          = {2022},
  url           = {https://arxiv.org/abs/2203.02155},
  archiveprefix = {arXiv},
  eprint        = {2203.02155}
}

@article{Lewis2020RAG,
  title         = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  author        = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and others},
  journal       = {Advances in Neural Information Processing Systems},
  volume        = {33},
  year          = {2020},
  url           = {https://arxiv.org/abs/2005.11401},
  archiveprefix = {arXiv},
  eprint        = {2005.11401}
}

@article{Barnett2024RAGFailures,
  title         = {Seven Failure Points When Engineering a Retrieval-Augmented Generation System},
  author        = {Barnett, Joshua and others},
  journal       = {arXiv preprint arXiv:2401.05856},
  year          = {2024},
  url           = {https://arxiv.org/abs/2401.05856},
  archiveprefix = {arXiv},
  eprint        = {2401.05856}
}

@article{Asai2023SelfRAG,
  title         = {Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection},
  author        = {Asai, Akari and others},
  journal       = {arXiv preprint arXiv:2310.11511},
  year          = {2023},
  url           = {https://arxiv.org/abs/2310.11511},
  archiveprefix = {arXiv},
  eprint        = {2310.11511}
}

@article{Hu2021LoRA,
  title         = {LoRA: Low-Rank Adaptation of Large Language Models},
  author        = {Hu, Edward and Shen, Yelong and Wallis, Phillip and others},
  journal       = {arXiv preprint arXiv:2106.09685},
  year          = {2021},
  url           = {https://arxiv.org/abs/2106.09685},
  archiveprefix = {arXiv},
  eprint        = {2106.09685}
}

@article{Dettmers2023QLoRA,
  title         = {QLoRA: Efficient Finetuning of Quantized LLMs},
  author        = {Dettmers, Tim and others},
  journal       = {arXiv preprint arXiv:2305.14314},
  year          = {2023},
  url           = {https://arxiv.org/abs/2305.14314},
  archiveprefix = {arXiv},
  eprint        = {2305.14314}
}

@article{Huang2023Hallucinations,
  title         = {A Survey on Hallucination in Large Language Models},
  author        = {Huang, Lei and others},
  journal       = {arXiv preprint arXiv:2309.05276},
  year          = {2023},
  url           = {https://arxiv.org/abs/2309.05276},
  archiveprefix = {arXiv},
  eprint        = {2309.05276}
}

@article{Bender2021StochasticParrots,
  title   = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author  = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  journal = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency},
  year    = {2021},
  doi     = {10.1145/3442188.3445922}
}


@article{Mikolov2013Efficient,
  abstract  = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  author    = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  journal   = {1st International Conference on Learning Representations, ICLR 2013 - Workshop Track Proceedings},
  month     = {9},
  publisher = {International Conference on Learning Representations, ICLR},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  url       = {http://arxiv.org/abs/1301.3781},
  year      = {2013}
}

@inproceedings{Sutskever2014Sequence,
  author    = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Sequence to Sequence Learning with Neural Networks},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2014/file/5a18e133cbf9f257297f410bb7eca942-Paper.pdf},
  volume    = {27},
  year      = {2014}
}
@article{Enyan2024Distributional,
  abstract = {Distributional semantics is the linguistic theory that a word's meaning can be derived from its distribution in natural language (i.e., its use). Language models are commonly viewed as an implementation of distributional semantics, as they are optimized to capture the statistical features of natural language. It is often argued that distributional semantics models should excel at capturing graded/vague meaning based on linguistic conventions, but struggle with truth-conditional reasoning and symbolic processing. We evaluate this claim with a case study on vague (e.g. "many") and exact (e.g. "more than half") quantifiers. Contrary to expectations, we find that, across a broad range of models of various types, LLMs align more closely with human judgements on exact quantifiers versus vague ones. These findings call for a re-evaluation of the assumptions underpinning what distributional semantics models are, as well as what they can capture.},
  author   = {Zhang Enyan and Zewei Wang and Michael A. Lepori and Ellie Pavlick and Helena Aparicio},
  isbn     = {2410.13984v1},
  month    = {10},
  title    = {Are LLMs Models of Distributional Semantics? A Case Study on Quantifiers},
  url      = {http://arxiv.org/abs/2410.13984},
  year     = {2024}
}



@article{Gupta2024,
  author    = {Sonakshi Gupta and Akhlak Mahmood and Pranav Shetty and Aishat Adeboye and Rampi Ramprasad},
  doi       = {10.1038/s43246-024-00708-9},
  issn      = {2662-4443},
  issue     = {1},
  journal   = {Communications Materials},
  keywords  = {Databases,Polymers},
  month     = {12},
  pages     = {269},
  publisher = {Nature Publishing Group},
  title     = {Data extraction from polymer literature using large language models},
  volume    = {5},
  url       = {https://www.nature.com/articles/s43246-024-00708-9},
  year      = {2024}
}


@article{Wei2022Emergent,
  abstract  = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  author    = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
  journal   = {Transactions on Machine Learning Research},
  month     = {10},
  publisher = {Transactions on Machine Learning Research},
  title     = {Emergent Abilities of Large Language Models},
  volume    = {2022-August},
  url       = {http://arxiv.org/abs/2206.07682},
  year      = {2022}
}


@inproceedings{Shaw2018Selfattention,
  abstract  = {Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.},
  author    = {Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
  city      = {Stroudsburg, PA, USA},
  doi       = {10.18653/v1/N18-2074},
  isbn      = {9781948087292},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language
               Technologies, Volume 2 (Short Papers)},
  month     = {3},
  pages     = {464-468},
  publisher = {Association for Computational Linguistics},
  title     = {Self-Attention with Relative Position Representations},
  volume    = {2},
  url       = {http://aclweb.org/anthology/N18-2074},
  year      = {2018}
}


@article{Clark2019Bert,
  title   = {What Does BERT Look At? An Analysis of BERT's Attention},
  author  = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal = {arXiv preprint arXiv:1906.04341},
  year    = {2019}
}

@article{Perez2019Turing,
  title   = {On the Turing Completeness of Modern Neural Network Architectures},
  author  = {P{\'e}rez, Jorge and Marinkovi{\'c}, Javier and Barcel{\'o}, Pablo},
  journal = {arXiv preprint arXiv:1901.03429},
  year    = {2019}
}

@article{Abnar2020Quantifying,
  title   = {Quantifying attention flow in transformers},
  author  = {Abnar, Samira and Zuidema, Willem},
  journal = {arXiv preprint arXiv:2005.00928},
  year    = {2020}
}

@article{Yun2019Transformers,
  title   = {Are transformers universal approximators of sequence-to-sequence functions?},
  author  = {Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  journal = {arXiv preprint arXiv:1912.10077},
  year    = {2019}
}

@article{Jain2019Attention,
  title   = {Attention is not explanation},
  author  = {Jain, Sarthak and Wallace, Byron C},
  journal = {arXiv preprint arXiv:1902.10186},
  year    = {2019}
}

@article{Xiao2023Efficient,
  title   = {Efficient streaming language models with attention sinks},
  author  = {Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal = {arXiv preprint arXiv:2309.17453},
  year    = {2023}
}

@article{Hoffmann2022training,
  title   = {Training compute-optimal large language models},
  author  = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal = {arXiv preprint arXiv:2203.15556},
  year    = {2022}
}


@article{mcculloch1943logical,
  author  = {McCulloch, Warren S. and Pitts, Walter},
  title   = {A logical calculus of the ideas immanent in nervous activity},
  journal = {The Bulletin of Mathematical Biophysics},
  volume  = {5},
  number  = {4},
  pages   = {115--133},
  year    = {1943}
}

@article{rosenblatt1958perceptron,
  author  = {Rosenblatt, Frank},
  title   = {The perceptron: A probabilistic model for information storage and organization in the brain},
  journal = {Psychological Review},
  volume  = {65},
  number  = {6},
  pages   = {386},
  year    = {1958}
}

@book{minsky1969perceptrons,
  author    = {Minsky, Marvin and Papert, Seymour},
  title     = {Perceptrons: An Introduction to Computational Geometry},
  publisher = {MIT Press},
  year      = {1969}
}

@article{rumelhart1986learning,
  author  = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  title   = {Learning representations by back-propagating errors},
  journal = {Nature},
  volume  = {323},
  number  = {6088},
  pages   = {533--536},
  year    = {1986}
}

@inproceedings{krizhevsky2012imagenet,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  title     = {ImageNet classification with deep convolutional neural networks},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {25},
  year      = {2012}
}

@inproceedings{vaswani2017attention,
  author    = {Vaswani, Ashish and others},
  title     = {Attention is all you need},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {30},
  year      = {2017}
}

@misc{liang2022helm,
  author = {Liang, Percy and others},
  title  = {Holistic evaluation of language models},
  note   = {arXiv:2211.09110},
  year   = {2022}
}

@inproceedings{lin2021truthfulqa,
  author    = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  title     = {TruthfulQA: Measuring how models mimic human falsehoods},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  pages     = {3214--3252},
  year      = {2021}
}

@inproceedings{ouyang2022training,
  author    = {Ouyang, Long and others},
  title     = {Training language models to follow instructions with human feedback},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {35},
  pages     = {27730--27744},
  year      = {2022}
}

@inproceedings{heusel2017gans,
  author    = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  title     = {GANs trained by a two time-scale update rule converge to a local Nash equilibrium},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  volume    = {30},
  year      = {2017}
}

@inproceedings{hessel2021clipscore,
  author    = {Hessel, Jack and Holtzman, Ari and Forbes, Maxwell and Bras, Ronan Le and Choi, Yejin},
  title     = {CLIPScore: A reference-free evaluation metric for image captioning},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages     = {7514--7528},
  year      = {2021}
}

@article{gomez2018automatic,
  author  = {G{\'o}mez-Bombarelli, Rafael and others},
  title   = {Automatic chemical design using a data-driven continuous representation of molecules},
  journal = {ACS Central Science},
  volume  = {4},
  number  = {2},
  pages   = {268--276},
  year    = {2018}
}

@article{brown2019guacamol,
  author  = {Brown, Nathan and Fiscato, Massimo and Segler, Marwin H. S. and Vaucher, Alain C.},
  title   = {GuacaMol: Benchmarking models for de novo molecular design},
  journal = {Journal of Chemical Information and Modeling},
  volume  = {59},
  number  = {3},
  pages   = {1096--1108},
  year    = {2019}
}

@article{fedus2022switch,
  author  = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  title   = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  journal = {Journal of Machine Learning Research},
  volume  = {23},
  number  = {120},
  pages   = {1--39},
  year    = {2022}
}

@misc{jiang2024mixtral,
  author = {Jiang, Albert Q. and others},
  title  = {Mixtral of Experts},
  note   = {arXiv:2401.04088},
  year   = {2024}
}

@article{su2024roformer,
  author  = {Su, Jianlin and Lu, Yixuan and Pan, Sheng and Murtadha, Ahmed and Wen, Bo and Liu, Yang},
  title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  journal = {Neurocomputing},
  volume  = {568},
  pages   = {127063},
  year    = {2024}
}

@misc{peng2023yarn,
  author = {Peng, Baolin and Quesnelle, Justin and Fan, Hao and Shi, Elaine},
  title  = {YaRN: Efficient Context Window Extension of Large Language Models},
  note   = {arXiv:2309.00071},
  year   = {2023}
}

@misc{ainslie2023gqa,
  author = {Ainslie, Joshua and others},
  title  = {GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  note   = {arXiv:2305.13245},
  year   = {2023}
}

@inproceedings{dao2022flashattention,
  author    = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  title     = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {35},
  pages     = {16344--16359},
  year      = {2022}
}

@inproceedings{dao2023flashattention2,
  author    = {Dao, Tri},
  title     = {FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2024}
}

@inproceedings{rajbhandari2020zero,
  author    = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  title     = {ZeRO: Memory optimizations toward training trillion parameter models},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '20)},
  year      = {2020}
}

@article{zhao2023pytorch,
  author  = {Zhao, Yuchen and others},
  title   = {PyTorch FSDP: Experiences on scaling fully sharded data parallel},
  journal = {Proceedings of the VLDB Endowment},
  volume  = {16},
  number  = {12},
  pages   = {3848--3860},
  year    = {2023}
}

@misc{kaplan2020scaling,
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and others},
  title  = {Scaling Laws for Neural Language Models},
  note   = {arXiv:2001.08361},
  year   = {2020}
}

@misc{hoffmann2022training,
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and others},
  title  = {Training Compute-Optimal Large Language Models},
  note   = {arXiv:2203.15556},
  year   = {2022}
}

@article{chowdhery2023palm,
  author  = {Chowdhery, Aakanksha and others},
  title   = {PaLM: Scaling Language Modeling with Pathways},
  journal = {Journal of Machine Learning Research},
  volume  = {24},
  number  = {240},
  pages   = {1--113},
  year    = {2023}
}

@inproceedings{hu2021lora,
  author    = {Hu, Edward J. and others},
  title     = {LoRA: Low-Rank Adaptation of Large Language Models},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2022}
}

@inproceedings{leviathan2023fast,
  author    = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  title     = {Fast Inference from Transformers via Speculative Decoding},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
  volume    = {202},
  pages     = {19274--19286},
  year      = {2023}
}

@article{bommasani2022opportunities,
   abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
   author = {Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
   month = {7},
   title = {On the Opportunities and Risks of Foundation Models},
   url = {http://arxiv.org/abs/2108.07258},
   year = {2022}
}


@article{Schaeffer2023emergent,
   abstract = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.},
   author = {Rylan Schaeffer and Brando Miranda and Sanmi Koyejo},
   journal = {Advances in Neural Information Processing Systems},
   month = {5},
   publisher = {Neural information processing systems foundation},
   title = {Are Emergent Abilities of Large Language Models a Mirage?},
   volume = {36},
   url = {http://arxiv.org/abs/2304.15004},
   year = {2023}
}

@article{Chung2022scaling,
   abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
   author = {Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tai and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
   issn = {15337928},
   journal = {Journal of Machine Learning Research},
   keywords = {Bias & Toxicity,Chain-of-Thought Reasoning,Instruction Finetuning,Language Models,Natural Language Processing},
   month = {10},
   publisher = {Microtome Publishing},
   title = {Scaling Instruction-Finetuned Language Models},
   volume = {25},
   url = {https://arxiv.org/pdf/2210.11416},
   year = {2022}
}



@article{Chung2022,
   abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
   author = {Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
   issn = {15337928},
   journal = {Journal of Machine Learning Research},
   keywords = {Bias & Toxicity,Chain-of-Thought Reasoning,Instruction Finetuning,Language Models,Natural Language Processing},
   month = {12},
   publisher = {Microtome Publishing},
   title = {Scaling Instruction-Finetuned Language Models},
   volume = {25},
   url = {http://arxiv.org/abs/2210.11416},
   year = {2022}
}
@article{Bai2022Constitutional,
   abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
   author = {Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
   month = {12},
   title = {Constitutional AI: Harmlessness from AI Feedback},
   url = {http://arxiv.org/abs/2212.08073},
   year = {2022}
}
@article{Chang2023EvaluatingLLMs,
   abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.},
   author = {Yupeng Chang and Xu Wang and Jindong Wang and Yuan Wu and Linyi Yang and Kaijie Zhu and Hao Chen and Xiaoyuan Yi and Cunxiang Wang and Yidong Wang and Wei Ye and Yue Zhang and Yi Chang and Philip S. Yu and Qiang Yang and Xing Xie},
   doi = {10.1145/3641289},
   isbn = {9798891763357},
   issn = {21576912},
   issue = {3},
   journal = {ACM Transactions on Intelligent Systems and Technology},
   keywords = {Additional Key Words and PhrasesLarge language models,benchmark,evaluation,model assessment},
   month = {12},
   publisher = {Association for Computing Machinery},
   title = {A Survey on Evaluation of Large Language Models},
   volume = {15},
   url = {http://arxiv.org/abs/2307.03109},
   year = {2023}
}
@article{Dai2019TransformerXL,
   abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
   author = {Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
   journal = {ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
   month = {6},
   pages = {2978-2988},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
   url = {http://arxiv.org/abs/1901.02860},
   year = {2019}
}
@article{Gentzkow2019TextAsDataPaper,
   abstract = {An ever-increasing share of human interaction, communication, and culture is recorded as digital text. We provide an introduction to the use of text as an input to economic research. We discuss the features that make text different from other forms of data, offer a practical overview of relevant statistical methods, and survey a variety of applications. (JEL C38, C55, L82, Z13)},
   author = {Matthew Gentzkow and Bryan Kelly and Matt Taddy},
   doi = {10.1257/jel.20181020},
   issn = {0022-0515},
   issue = {3},
   journal = {Journal of Economic Literature},
   month = {9},
   pages = {535-574},
   title = {Text as Data},
   volume = {57},
   url = {https://pubs.aeaweb.org/doi/10.1257/jel.20181020},
   year = {2019}
}
@article{Liang2022HELM,
   abstract = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
   author = {Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Cosgrove and Christopher D. Manning and Christopher Ré and Diana Acosta-Navas and Drew A. Hudson and Eric Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue Wang and Keshav Santhanam and Laurel Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan Kim and Neel Guha and Niladri Chatterji and Omar Khattab and Peter Henderson and Qian Huang and Ryan Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda},
   issue = {1},
   journal = {Annals of the New York Academy of Sciences},
   keywords = {artificial intelligence,evaluation,foundation models,language models,natural language processing,transparency},
   month = {10},
   pages = {140-146},
   publisher = {John Wiley and Sons Inc},
   title = {Holistic Evaluation of Language Models},
   volume = {1525},
   url = {http://arxiv.org/abs/2211.09110},
   year = {2023}
}
@article{Ganguli2022RedTeaming,
   abstract = {We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.},
   author = {Deep Ganguli and Liane Lovitt and Jackson Kernion and Amanda Askell and Yuntao Bai and Saurav Kadavath and Ben Mann and Ethan Perez and Nicholas Schiefer and Kamal Ndousse and Andy Jones and Sam Bowman and Anna Chen and Tom Conerly and Nova DasSarma and Dawn Drain and Nelson Elhage and Sheer El-Showk and Stanislav Fort and Zac Hatfield-Dodds and Tom Henighan and Danny Hernandez and Tristan Hume and Josh Jacobson and Scott Johnston and Shauna Kravec and Catherine Olsson and Sam Ringer and Eli Tran-Johnson and Dario Amodei and Tom Brown and Nicholas Joseph and Sam McCandlish and Chris Olah and Jared Kaplan and Jack Clark},
   month = {11},
   title = {Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned},
   url = {http://arxiv.org/abs/2209.07858},
   year = {2022}
}
@article{Yao2023ReAct,
   abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
   author = {Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
   journal = {11th International Conference on Learning Representations, ICLR 2023},
   month = {3},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {ReAct: Synergizing Reasoning and Acting in Language Models},
   url = {http://arxiv.org/abs/2210.03629},
   year = {2023}
}
