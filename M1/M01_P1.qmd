---
title: "AD698 - Applied Generative AI"
subtitle: "Introduction to Generative AI & Business Applications"
logo: "../theme/figures/met_logotype_black.png"
date: 03/12/2024
date-modified: today
date-format: long
author:
  - name: Nakul R. Padalkar
    affiliations:
      - name: Boston University
        city: Boston
        state: MA
format: 
    revealjs:
        theme: [../theme/presentation.scss]
        html-math-method: katex
        slide-number: c/t
        toc: true
        toc-depth: 1
        auto-stretch: false
        from: markdown+emoji

    pptx:
        reference-doc: ../theme/presentation_template.pptx
self-contained-math: true
fig-align: center
monofont: Roboto
title-slide-attributes:
    data-background-image: "../theme/blank_red.png"
    data-background-size: 103% auto
    data-background-opacity: "0.95"
execute: 
  echo: false
  warning: false
  message: false
bibliography: ../references.bib
csl: ../mis-quarterly.csl
---

## Course Welcome

- Understand the breadth of generative AI technologies
- Explore core generative model architectures
- Gain insights into practical applications

##  Course Schedule (tentative)

```{python}
# | echo: false
# | output: html

import pandas as pd
from IPython.display import display, HTML
import sys

# -------------------------------------------------
# Load course table
# -------------------------------------------------
course_schedule = (
    pd.read_excel(
        "../data/AD698-Schedule.xlsx",
        sheet_name="Course Details"
    )
    .drop("Modules", axis=1)
)
# remove last two rows
course_schedule.drop(course_schedule.index[-2:], inplace=True)


def restrict_width(_):
    return [
        ("max-width", "275px"),
        ("overflow", "hidden"),
        ("text-overflow", "ellipsis")
    ]


# drop online and On-campus columns
course_schedule.drop(["Online", "On-Campus"], axis=1, inplace=True)
course_schedule.drop(course_schedule.index[-8:], inplace=True)


styled_table = (
    course_schedule
    .style
    .hide(axis="index")
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "left")]},
        {"selector": "td", "props": [("text-align", "left")]}
    ])
)

display(HTML(styled_table.to_html()))

```

---

##  Course Schedule (tentative)

```{python}
# | echo: false
# | output: html

import pandas as pd
from IPython.display import display, HTML
import sys

# -------------------------------------------------
# Load course table
# -------------------------------------------------
course_schedule = (
    pd.read_excel(
        "../data/AD698-Schedule.xlsx",
        sheet_name="Course Details"
    )
    .drop("Modules", axis=1)
)
# remove last two rows
course_schedule.drop(course_schedule.index[-2:], inplace=True)


def restrict_width(_):
    return [
        ("max-width", "275px"),
        ("overflow", "hidden"),
        ("text-overflow", "ellipsis")
    ]


# drop online and On-campus columns
course_schedule.drop(["Online", "On-Campus"], axis=1, inplace=True)
course_schedule.drop(course_schedule.index[:8], inplace=True)


styled_table = (
    course_schedule
    .style
    .hide(axis="index")
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "left")]},
        {"selector": "td", "props": [("text-align", "left")]}
    ])
)

display(HTML(styled_table.to_html()))

```

## Course Grading

::: {.columns}
::: {.column width="60%"}

```{python}
# | echo: false
# | eval: true
# | auto-stretch: false

from pywaffle import Waffle
import pandas as pd
import matplotlib.pyplot as plt

deliverables = pd.read_excel("../data/AD698-Schedule.xlsx", sheet_name="Grade")

deliverables = deliverables[[
    "Class Activity", "Count", "Points", "Max Points"]]
# Drop NaN points

points_data = deliverables.dropna(subset=["Points"])

# plt.figure(figsize=(8, 4.0), dpi=100)
plt.pie(
    points_data["Points"],
    labels=points_data["Class Activity"],
    autopct='%1.1f%%',
    startangle=140
)
# plt.title("Distribution of Points by Class Activity")
plt.show()

```

:::
::: {.column width="40%"}
```{python}
# | echo: false
# | output: html
# |

import pandas as pd
from IPython.display import display, Markdown, HTML

deliverables = pd.read_excel(
    "../data/AD698-Schedule.xlsx", sheet_name="Grade")

deliverables = deliverables[[
    "Class Activity", "Count", "Points", "Max Points"]]

numeric_cols = ["Count", "Points", "Max Points"]

# 1) Coerce strings/objects to numbers, invalids → NaN
deliverables[numeric_cols] = deliverables[numeric_cols].apply(
    pd.to_numeric, errors="coerce"
)

# 2) Use pandas' nullable integer dtype (allows NaN)
deliverables[numeric_cols] = deliverables[numeric_cols].astype("Int64")

# 3) Style: show "-" for NaN without changing the data
styled = (
    deliverables.style
    .hide(axis="index")
    .format(na_rep="-")  # <- this prints "-" wherever value is NA
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "left")]},
        {"selector": "td", "props": [("text-align", "left")]},
        {"selector": "tbody td:nth-child(2)", "props": [(
            "max-width", "200px"), ("overflow", "hidden"), ("text-overflow", "ellipsis")]},
        {"selector": "tbody td:nth-child(3)", "props": [(
            "max-width", "200px"), ("overflow", "hidden"), ("text-overflow", "ellipsis")]}
    ])
)

display(HTML(styled.to_html()))
```

:::
:::

---

## Participation Points

- You are added to an AWS Academy Generative AI Foundations course in AWS Academy.
- There are self-paced 10 modules in the course.
- If you complete all modules, you will receive participation points for AWS academy.
- You are also added to AWS Learner Lab for hands-on practice.

![](./M01_lecture01_figures/AWS-Certification-Current-Roadmap.jpg){width="50%" fig-align="center"}

---

## GitHub Repository and Portfolio

- [Lecture 0.1]{.uured-bold}, [Lecture 0.2]{.uured-bold}, [Lecture 0.3]{.uured-bold} videos cover basic portfolio creation using GitHub.
- Follow the video and submit the repository link in the assignment.
- This will be also used for submitting
  - Assignments
  - Projects
  - Weekly Quizzes, Presentations, and Problems solving exercises AKA Labs

![](./M01_lecture01_figures/brian-marion-aaron-project.png){width="55%" fig-align="center"}

---

## In class Presentations

- Look for the Labs page in each lecture folder (M0X/Lecture) for the list of papers and suggestions for the presentation.
- Each student group will deliver **one research paper presentation** during the semester.
  - Presentation length: approximately **15 minutes**
  - Each presentation will be followed by **guided class discussion**
  - All students are expected to read the assigned paper(s), regardless of whether they are presenting
- Presentations focus on:
  - The problem the paper addresses
  - The technical or conceptual contribution
  - Limitations and assumptions
  - Implications for building generative AI systems
- This component develops skills in [critical reading]{.uublue-bold}, [synthesis]{.uublue-bold}, and [technical communication]{.uublue-bold}.


---

## Individual Assignments

- Four individual assignments.
  - Report (word or pdf) and code (jupyter notebook). 
- These are more detailed and require more time to complete.

---

## Group Project

- [Lecture 0.1]{.uured-bold} and [Lecture 0.2]{.uured-bold} videos cover basic portfolio creation using GitHub.
- Focuses on equipping students with the knowledge to explore a dataset and extract meaningful insights. Throughout this assignment, you’ll complete the following:

| **Component**                     | **Points** | **Description**                                                                                                                                    |
|-----------------------------------|------------|----------------------------------------------------------------------------------------------------------------------------------------------------|
| Project Milestones through GitHub | 70         | This includes Corpus Familiarization & Chatbot Scope, Text Structuring & Retrieval Units, Embeddings & Retrieval Baseline, Grounded Generation & Guardrails |   
| Presentation                      | 40         | Effectively communicates the project findings and technical work.                                                                                  |
| Group Feedback                    | 40         | Includes peer feedback and team collaboration evaluation.                                                                                          |
| **Total**                         | **150**    |                                                                                                                                                    |


---

##  Course Site

- [Design and content is subjected to change]{.uured-bold}.
- All the assignments, quizzes, and projects will be posted on the course site.
- The course site is the main source of information for the course.

![](./M01_lecture01_figures/blackboard-ultra-ad698.png){width="80%" fig-align="center"}

---

## Office Hours & Consultation Sessions
- Look for [office hours]{.uugreen-bold} page on blackboard.
- Look for [Consultation Sessions]{.uugreen-bold} page on blackboard.
- [Consultation Sessions]{.uugreen-bold}: For detailed help on assignments and projects.
  - Held separately on Saturday, time needs to be discussed.
  - These will be review of labs and assignments.
  - These will be held on Zoom.
  - [Recorded only if there is an audience]{.uured-bold}.

## Tools used in the course

- [Python]{.uured-bold}: dominant AI/ML language with simple, fast prototyping.  
- [PyTorch / TensorFlow]{.uured-bold}: core deep‑learning frameworks for GenAI.  
- [Hugging Face Transformers]{.uured-bold}: key library for LLMs, text generation, and RAG.  
- [OpenCV & PIL]{.uured-bold}: essential for image/video processing in GenAI.


## What is Generative AI?

![](./M01_lecture01_figures/generative-ai.png){width="80%" fig-align="center"}

## Enterprise AI Landscape

![](./M01_lecture01_figures/Enterprise-generative-AI-application-landscape-vw.png){width="90%" fig-align="center"}

## GenAI Core Characteristics

- Learning from existing data
- Generating novel, contextually relevant outputs
- Spanning multiple modalities (text, image, code, audio)

## Evolution of Model Architecture

![Evolution of Language Model based Dialogue Systems [@Wang2023]](./M01_lecture01_figures/LLM-dialogue-system.jpg){width="80%" fig-align="center" #fig-LLM-dialogue-system}

## Key Generative Model Architectures

::: {.columns}
::: {.column}
- Foundation Models and Algorithms
  -  Convolutional Neural Networks (CNNs)
  -  Recurrent Neural Networks (RNNs)
  -  Transformer Architecture
  -  Generative Adversarial Networks (GANs)
- Language Models
  - GPT (Generative Pre-trained Transformer)
  - BERT (Bidirectional Encoder Representations)
:::
::: {.column}
- Image Generation Models
  - DALL-E
  - Stable Diffusion
  - Midjourney
- Multimodal Models
  - Amazon Nova
  - Meta Llama
  - Gemini
  - GPT

:::
:::



## Simulation, Emulation, and AI

- Given a function/process that exists with complex logical pathways, can it be mimicked by computers?
  - Economical transactions of a small city?
  - Social interactions between a household?
  - Weather patterns over a region?
  - Train of thought of researcher doing their job?
- [Simulation]{.uured-bold}: The imitation of a process, function, or environment that could exist or at least makes sense.
- [Artificial Intelligence]{.uured-bold}: The implementation of systems that perform tasks that seemingly require human skills.
- [Machine Learning]{.uured-bold}: The use of algorithms and statistics to identify, act on, and/or mimic patterns.
- [Deep Learning]{.uured-bold}: The use of neural networks to create function approximators with machine learning patterns.

## Deep Learning - History

![](./M01_lecture01_figures/nn_timeline.webp){width="90%" fig-align="center"}

## Artificial Intelligence - History

![](./M01_lecture01_figures/Intelligent-Machines-AI-timeline.jpg){width="70%" fig-align="center"}

## 1956 - Dartmouth Conference: The Birth of Artificial Intelligence
### Significance
- The term "[Artificial Intelligence]{.uublue-bold}" was first coined at the Dartmouth Conference, with credit to John McCarthy, one of the event organizers.
- [Foundational Moment for AI]{.uublue-bold}: This event is often considered the founding moment of AI as a field of study. It was the first time researchers from various disciplines came together to explore the concept of machine intelligence.

![](./M01_lecture01_figures/dartmouth_Fathers_AI.png){width="75%" fig-align="center"}

## 1958 - Perceptron by Rosenblatt at Cornell

::: {.columns}
::: {.column}

- [The perceptron]{.uured-bold}, introduced by Frank Rosenblatt in 1958, was an early artificial neuron model inspired by the brain and helped launch modern neural networks.  
- Despite its impact, it couldn't handle non–linearly separable problems like **XOR**, a limitation that shaped future AI research.
:::
::: {.column}
![](./M01_lecture01_figures/0925_rosenblatt4.jpg){width="75%" fig-align="center"}
:::
:::



## 1969 - Analysis of Perceptrons Minsky \& Papert

::: {.columns}
::: {.column}

- [Minsky and Papert’s book *Perceptrons*]{.uured-bold} analyzed the strengths and limits of Rosenblatt’s perceptron, emphasizing its inability to solve non‑linearly separable problems like [XOR]{.uured-bold}.  
- Their critique slowed neural‑network research and contributed to the [AI Winter]{.uured-bold}.  
- The work also paved the way for later breakthroughs, including [backpropagation]{.uured-bold} in the 1980s, which enabled training multilayer networks and revived interest in neural nets.
:::
::: {.column}
```{python}
import matplotlib.pyplot as plt
import numpy as np

# All possible binary inputs
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

# Logic gate outputs
def AND(x):   return np.array([a & b for a, b in x])
def NAND(x):  return np.array([1 - (a & b) for a, b in x])
def OR(x):    return np.array([a | b for a, b in x])
def XOR(x):   return np.array([a ^ b for a, b in x])

gates = {
    "AND": AND(X),
    "NAND": NAND(X),
    "OR": OR(X),
    "XOR": XOR(X),
}

fig, axes = plt.subplots(2, 2, figsize=(6, 6))
axes = axes.ravel()

for ax, (name, y) in zip(axes, gates.items()):
    # Plot points: 0 = blue circle, 1 = red diamond
    for (x1, x2), label in zip(X, y):
        if label == 0:
            ax.scatter(x1, x2, c="blue", marker="o", s=80)
        else:
            ax.scatter(x1, x2, c="red", marker="D", s=80)

    # Add (optional) linear boundary for linearly separable ones
    if name in ["AND", "NAND", "OR"]:
        xs = np.linspace(-0.2, 1.2, 100)
        if name == "AND":
            # Example separating line: x1 + x2 = 1.5
            ax.plot(xs, 1.5 - xs, color="blue")
        elif name == "NAND":
            # Example: x1 + x2 = 0.5
            ax.plot(xs, 0.5 - xs, color="blue")
        elif name == "OR":
            # Example: x1 + x2 = 0.5
            ax.plot(xs, 0.5 - xs, color="blue")

    ax.set_title(name)
    ax.set_xlim(-0.2, 1.2)
    ax.set_ylim(-0.2, 1.2)
    ax.set_xticks([0, 1])
    ax.set_yticks([0, 1])
    ax.set_xlabel("x1")
    ax.set_ylabel("x2")

plt.tight_layout()
plt.show()

```

:::
:::


## 1974-1980 - $1^{\text {st }}$ AI Winter

### 1974–1980 — First AI Winter

* Period of reduced funding, interest, and institutional support for AI research
* Triggered by unmet promises of early symbolic AI systems
* Government and industry funding declined sharply
* Many AI labs were closed, downsized, or repurposed

### Key Impacts

* Researchers shifted to adjacent fields (statistics, control systems, cognitive science)
* Machine learning and statistical methods progressed quietly
* Emphasized the gap between theoretical breakthroughs and practical deployment
* Established lasting lessons on expectation management in AI research

---

## 1980s — Backpropagation Popularized

[Backpropagation]{.uured-bold} is a general algorithm for training neural networks by computing gradients of a loss function with respect to model parameters efficiently.

:::{.columns}
::: {.column width="50%"}
- Core ideas
  - Uses the **chain rule** to propagate error signals backward through layers
  - Computes partial derivatives for each weight in the network
  - Enables optimization via **gradient-based learning** (e.g., gradient descent)
- Why it was transformative
  - Solved the *credit assignment problem* in multi-layer networks
  - Made it computationally feasible to train deep (for the time) architectures
  - Shifted neural networks from theoretical constructs to trainable systems
  - Backpropagation unified learning across arbitrary network depth

:::
::: {.column width="50%"}

![](./M01_lecture01_figures/backprop.png){width="75%" fig-align="center"}

:::
:::

---

## 1986 - Multi-Layer Perceptron (MLP)

The 1986 backpropagation work demonstrated that [multi-layer perceptrons (MLPs)]{.uured-bold} could be trained effectively and reliably.

- What changed
  - **Hidden layers learned internal representations**, not hand-engineered features
  - Networks captured **hierarchical structure** in data
  - Non-linear activations enabled expressive decision boundaries
- Key advances
  - Demonstrated solutions to problems like **XOR**, impossible for linear models
  - Showed learning was **distributed across layers**, not localized
  - Provided empirical evidence that neural networks could generalize
- Why this mattered
  - Shifted AI from rule-based reasoning toward **representation learning**
  - Established the conceptual foundation for modern deep learning

---

## 1989 - Universal Function Approximation Theory

The [Universal Approximation Theorem]{.uured-bold} provided a formal theoretical justification for neural networks.

- Core result
  - A feedforward network with:
    - One hidden layer
    - Finite neurons
    - A non-linear activation function
  - Can approximate **any continuous function** on a compact domain
- What the theorem does (and does not) say
  - ✔ Networks are *expressive enough in principle*
  - ✘ Does not guarantee efficient training
  - ✘ Does not imply good generalization or data efficiency
- Why it mattered
  - Countered claims that neural networks were inherently limited
  - Legitimated neural networks as universal function learners
  - Encouraged continued research despite computational constraints


--- 

## Why Didn’t Neural Networks Take Over Immediately?

:::{.columns}
::: {.column width="60%"}

Despite strong theoretical foundations, neural networks did **not** dominate AI in the late 1980s and 1990s.

- **Key constraints**
  - **Limited data:** Large labeled datasets were rare
  - **Insufficient compute:** CPUs were slow; GPUs were not yet available
  - **Optimization challenges:** Training was unstable and slow
  - **Tooling gaps:** No mature frameworks for large-scale experimentation
- **Practical consequence**
  - Neural networks were expressive *in theory*
  - But **difficult to train, scale, and deploy** in practice

:::
::: {.column width="40%"}

### Theory vs Reality

- Capacity ✔
- Learnability ✔
- Scalability ✘
- Reliability ✘

:::
:::


## 1980s-1990s - The $2^{\text {nd}}$ AI Winter

- [Unmet Expectations]{.uured-bold}: Expert systems, initially promising, failed to generalize beyond narrow domains, leading to disillusionment.
- [Economic Recession]{.uured-bold}: The late 1980s recession led to budget cuts in AI research, shifting focus to more immediate technologies.
- [Technological Limits]{.uured-bold}: Insufficient computational power and algorithmic challenges hindered AI development.
- [Reduced Interest]{.uured-bold}: Funding and interest in AI dropped, leading to a slowdown in research and fewer innovations.
- [Legacy and Recovery]{.uured-bold}: Lessons from this period set the stage for the AI resurgence in the late 1990s, paving the way for modern AI advancements


## 2012 — AlexNet and the Deep Learning Breakthrough

**AlexNet** marked a decisive turning point in modern AI by demonstrating that **deep neural networks scale effectively** when paired with sufficient data and computation.

- **What AlexNet showed**
  - Deep convolutional networks could vastly outperform classical vision pipelines
  - End-to-end learning outperformed hand-crafted feature engineering
  - Performance gains came from **scale**, not new theory alone

- **Why ImageNet mattered**
  - 1.2M labeled images across 1,000 classes
  - AlexNet reduced top-5 error by ~10 percentage points
  - Established benchmarks as a driver of progress
- **Infrastructure innovation**
  - Training split across **two GPUs** due to memory limits
  - GPU acceleration enabled faster iteration and deeper architectures
  - Marked the beginning of **hardware–model co-design** in AI

---

## 2017+ — *Attention Is All You Need* and the Transformer Era

The [Transformer architecture]{.uured-bold} revolutionized modern AI by replacing recurrence and convolution with **attention**, fundamentally changing how models process language and sequences.

- **Core innovation**
  - Self-attention enables global context modeling
  - Parallelizable training replaces sequential recurrence
  - Scales efficiently with data and compute
- **Why this mattered**
  - Dramatically improved performance on NLP tasks
  - Enabled large-scale pretraining on unlabeled text
  - Decoupled sequence modeling from strict ordering constraints
- **From models to platforms**
  - Transformers became the backbone of:
    - Language understanding (e.g., BERT-style models)
    - Language generation (e.g., GPT-style models)
  - Established the foundation for **Large Language Models (LLMs)**


## Innovations in Modeling

### Research aims to identify:

- Key architectural drivers of model performance
- Scaling laws and emergent capabilities
- New use-cases enabled by model behavior

### Common research directions:

- Model analysis via interpretability and controlled experiments
- New architectural components with useful properties
- Domain-aware training paired with scalable architectures
- Improved data and modality-specific learning strategies


## Risks of GenAI

**Despite its transformative potential, Generative AI introduces systemic risks to core societal foundations:**

- **Trust & information integrity**
- **Authenticity of digital records and identity**
- **Work, skills, and human purpose**

These risks are amplified by today’s software environment:

- Ubiquitous data availability
- Always-on connectivity
- Cheap, scalable compute

## Risk Mitigation

**Mitigation is complex and often costly, requiring deliberate trade-offs in favor of long-term societal value:**

- **Privacy & security by design**
- **Transparency & accountability mechanisms**
- **Fairness, inclusion, and human dignity**
- **Reliability, validation, and certification**

> Addressing these challenges frequently requires prioritizing legal, ethical, and social safeguards over short-term efficiency or profit.

---

## Generative AI and Knowledge Creation

Generative AI automates problems that were traditionally **hard, skill-intensive, and expensive**, producing outputs that are often *good enough*, scalable, and easily replicated.

This fundamentally lowers the cost of creating content that once required:

- Deep expertise
- Long training and experience
- Significant institutional or financial investment

**Result:**
Knowledge production becomes faster, cheaper, and more widely accessible—but less tightly coupled to expertise.

---

## Enablement vs. Erosion of Knowledge Signals

### Enablement and Scale *(Pros)*

- Rapid synthesis across large and complex datasets
- Broad access to advanced analytical and creative capabilities
- Accelerated experimentation, learning, and innovation
* Empowerment of individuals and small teams at scale

### Slop and Misinformation *(Cons)*

- Convincing text, images, audio, and video can be fabricated at scale
- Expertise can be mimicked without understanding or accountability
- Automated pipelines bypass editorial review and due process
- Long-term effects of widespread synthetic content remain uncertain

> **Key tension:** Generative AI expands knowledge creation
> while weakening traditional signals of credibility, expertise, and trust.


## Societal Impacts from GenAI
- Automation of Routine Tasks:
  - Al is automating repetitive jobs like data entry and manufacturing, leading to job reductions in some areas.
- Impact on Skilled Jobs:
  - AI is increasingly capable of handling complex task, potentially automating niche-skill/high-memory roles.
  - GenAI has been used to lower early career demand due to perceived upper-level role empowerment.
- New Job Opportunities:
  - While some jobs are lost, AI can also create new roles in areas like AI development and data science.
  - Workers need reskilling to adapt, but some careers / demographics will have disproportionate impact.
- Impact on Psychology:
  - Much like the internet (rise of ubiquitous connectivity), psychological impacts of ubiquitous Al is inconclusive.

---

## Broad Risk Mitigation Efforts in AI

AI safety and risk mitigation have become **strategic priorities** across governments, industry, and civil society.
These efforts reflect competing—but overlapping—interests in **national security, economic competitiveness, and public trust**.

- Key characteristics of the current landscape
  - Fragmented but rapidly expanding standards ecosystem
  - Mix of voluntary, industry-led initiatives and state-backed institutions
  - Emphasis on *risk anticipation*, not just post-hoc compliance

- Global coordination trend
  - Early leadership by a small number of states
  - Rapid international replication and diversification
  - Increasing public–private collaboration

---

## Examples of Major Mitigation Initiatives

- [AI Safety Institute (AISI)]{.uured-bold}: Nation-backed institutes focused on systemic AI risks, with early emphasis on national security and frontier models
- [AISI Consortium (AISIC)]{.uured-bold} *(US)*: Formal public–private collaboration with industry to support safety research, evaluations, and standards
- [Adobe Content Authenticity Initiative (CAI)]{.uured-bold}: Founded in 2019 to promote content provenance and authenticity tracking
- [Coalition for Content Provenance and Authenticity (C2PA)]{.uured-bold}: Open technical standard for cryptographically signed content credentials and metadata
- [NVIDIA Model Card++ / Trustworthy AI Efforts]{.uured-bold}: Expanded model documentation covering intended use, testing scope, risks, and deployment guidance
- [Open & Sovereign AI Safety Advocacy]{.uured-bold}: Emphasis on secure self-hosting, open models, and safety practices for sensitive or regulated data contexts

---

## Responsible AI: Core Principles

Responsible AI requires **continuous evaluation**, not one-time compliance.
It integrates technical controls, organizational governance, and ethical commitments.

- [Privacy & Security]{.uured-bold}: Protect data rights, secure sensitive information, and resist misuse or unauthorized access
- [Transparency & Accountability]{.uured-bold}: Enable explainability, document design decisions, assign responsibility, and maintain oversight
- [Fairness & Human Dignity]{.uured-bold}: Identify and mitigate bias; support nondiscrimination and equitable outcomes
- [Reliability & Certification]{.uured-bold}: Ensure consistent performance, fitness for purpose, and ongoing validation in production

# From Analytics to AI

## Evolution of Decision Support Systems

![](./M01_lecture01_figures/enterpriseai.png){width="80%" fig-align="center"}

## Types of Decision Support


![](./M01_lecture01_figures/typesofanalytics.png){width="75%" fig-align="center"}


# Anatomy of Text Mining and Analytics

## Sources of data and Information

- Text Data is a form of unstructured data.
- Text Data is generated by humans and machines.
- Text Data is Massive Data 

![World in Data](./M01_lecture01_figures/what-happens-every-second.gif){width="60%" fig-align="center" #fig-world-in-data fig-alt="What happens every second in the world"}

## Information Backbone

::: {.columns}
::: {.column}

All Information is stored in the form of data. Data is the backbone of information systems.

- In general Web Architecture drives this connection from the user to the data.
- Typically, a web application architecture diagram comprises three core components: 
  - Presentation layer / Client Layer
  - Application Layer / Business Layer
  - Data Layer

:::
::: {.column}

![Web Architecture](./M01_lecture01_figures/web_architecture.png){width="90%" fig-align="center" #fig-web-architecture fig-alt="Web Architecture Diagram"}

:::
:::


## Massive Data Analytics

:::{.callout-note}
Massive Data Analytics is the process of analyzing large data sets generated by social, business, operations, or technical activities to uncover hidden patterns, unknown correlations, market trends, customer preferences, and other useful business information [@shroff2010enterprise].
:::

::: {.columns}
::: {.column}
- Behavioral data on people's interactions with websites, mobile apps, and other physical and digital experiences
- Transactional data
- Social media data
- Metadata about interactions
- Data/Metadata related to the events, clicks
:::
::: {.column}
- Assists companies in creating value by [improving revenue]{.uured-bold} or [reducing cost]{.uured-bold}.
- Assists companies in gaining a [deeper understanding of their customers]{.uured-bold} by analyzing their online behavior.
- Provides insights for [making decisions]{.uured-bold} related to business planning, performance and strategy
- Informs [predictive modeling]{.uured-bold}, [testing]{.uured-bold}, [optimization]{.uured-bold}, [market research]{.uured-bold}, etc.
:::
::: 

---

## Big Data Analytics Process

::: {.columns}
::: {.column width="50%"}
![](./M01_lecture01_figures/analytics-funnel.png){width="90%" fig-align="center"}
:::
::: {.column width="50%"}
- [Data Collection]{.uured-bold}: Data is collected from various sources such as websites, social media, mobile apps, etc. by web scraping or using APIs. 
- [Data Exploration]{.uured-bold}: Data is explored to identify patterns, trends, and relationships. It is processed and analyzed to generate meaningful insights
- [Analysis & Visualization]{.uured-bold}: Exploring the data and variables will provide us with characteristics of the data.
- [Synthesis and Enhancement]{.uured-bold}: Upon analysis, we will eb presented with recommendations to enhance the "process". 
- [Implementation]{.uured-bold}: Once a recommendation is accepted the change needs to be implemented.
- [Monitoring]{.uured-bold}: The process is monitored to ensure that the change is effective.
:::
:::

## Analytics Ecosystem

![](./M01_lecture01_figures/analytics-ecosystem.png){}


# Data Collection and Acquisition

## Data Collection and Acquisition
::: {.columns}
::: {.column width="30%"}

![](./M01_lecture01_figures/analytics-funnel-big-data.png){width="75%" fig-align="center"}

:::

::: {.column width="70%"}

- [Early Web Analytics]{.uured-bold}:
  - Initial tools analyzed server logs, which tracked details such as the host requesting a webpage.
  - [WebTrends]{.uured-bold}: A popular tool for measuring website "hits" (visitor count).

- [Modern Challenges]{.uured-bold}:
  - Understanding detailed user interactions beyond text-based content.
  - Key considerations: Collecting fine-grained user interaction data.

- [Collecting User Interaction Data]{.uured-bold}:
  - Early 2000s: Companies began using JavaScript to capture user interactions via browsers.
  - [Urchin]{.uured-bold}: Acquired by Google in 2005, forming the basis of **Google Analytics**.
  - [JavaScript]{.uured-bold}: Enables programs to execute in browsers and collect client-side data.

:::
:::
---

## Cloud Platforms

::: {.columns}

::: {.column}
![](./M01_lecture01_figures/cloud-platforms.jpeg){width="75%" fig-align="center"}

:::
::: {.column}
- [Amazon Web Services (AWS)]{.uured-bold}: Provides scalable cloud solutions for data storage, analytics, and AI services.
- [Google Cloud Platform (GCP)]{.uured-bold}: Offers services like BigQuery and AI-based analytics for large-scale data analysis.
- [Microsoft Azure]{.uured-bold}: Features cloud-based data solutions, including Azure Data Lake and machine learning tools.
- [IBM Cloud]{.uured-bold}: Provides enterprise cloud solutions with integrated AI and analytics.
- [Oracle Cloud]{.uured-bold}: Focuses on cloud databases and scalable cloud infrastructure.

:::
:::
---

## Data Collection Methods  

| Points | Primary Data | Secondary Data |
| :--- | :--- | :--- |
| Meaning | Data collected by researcher himself | Data collected by other persons. |
| Originality | Original or unique information | Not original or unique information. |
| Adjustment | Doesn't need adjustment, is focused | Needs adjustment to suit actual aim. |
| Sources | Surveys, observations, experiments | Internal records, Govt. published data, etc. |
| Type of data | Qualitative data | Quantitative data |
| Methods | Observation, experiment, interview | Desk research method, searching online, etc. |
| Reliability | More reliable | Less reliable |
| Time consumed | More time consuming | Less time consuming |
| Need of investigators | Needs team of trained investigators | Doesn't need team of investigators |
| Cost effectiveness | Costly | Economical |
| Collected when | Secondary data is inadequate | Before primary data is collected |
| Capability | More capable to solve a problem | Less capable to solve a problem |
| Suitability | Most suitable to achieve objective | May or may not be suitable |
| Bias | Possibility of bias exist | Somewhat safe from bias |
| Collected by | Researcher or his agents | Persons other than who collects primary data |
| Precaution to use | Not Necessary | Quite necessary |
| | | |

## Structured and Unstructured Data

![](./M01_lecture01_figures/data-types-applications.jpeg){width="90%" fig-align="center"}

## Data Organization and Management

::: {.columns}
::: {.column}

::: {.callout-important}

  A [Database Management System]{.uured-bold} (DBMS) is a centralized software program that connects the database to various applications and displays data to the user or customer.

:::


- In this course, you will be exposed to [MySQL and PostgreSQL]{.uured-bold}.
- We will utilize [Google colab]{.uured-bold} to run our code.
- You will also use [PySpark]{.uured-bold}, a highly versatile and lightweight database for local data exploration [as needed]{.uured-bold}.
- And [Spark SQL]{.uured-bold} for Big Data processing.

:::
::: {.column}
- Examples of relational DBMS systems:
  - [Oracle]{.uured-bold}
  - [MySQL]{.uured-bold}
  - [hadoop]{.uured-bold}
  - [MongoDB]{.uured-bold}

![](./M01_lecture01_figures/big-data-tools.jpeg){width="70%" fig-align="center"}


:::
:::


# Anatomy of a Webpage

## HTTP protocol

::: {.columns}
::: {.column}
![](./M01_lecture01_figures/fetching-a-page.svg){width=95% fig-align="center"}
:::
::: {.column}
![](./M01_lecture01_figures/DOM-model.png){width=85% fig-align="center"}
:::
:::

## Web Page Rendering

- Go to [https://www.bu.edu/met/](https://www.bu.edu/met/) and right-click on the page and select `inspect`.
- This will open the `developer tools` in your browser, showing the `DOM` (Document Object Model) of the page

![](./M01_lecture01_figures/bumet-inspect.png){width=70% fig-align="center"}

# Web Scraping/ Web Crawling

## The web as a data-set

- The Web link graph is arguably the most important modern directed network
- [Directed network]{.uured-bold}: 
  - Hyperlinks from page A to page B, while page B may not have a link to page A.
  - Users can link from a page to another **without** worrying about reciprocity 
- [Web Nodes]{.uured-bold} = ANYTHING with a URL (people, images, pages, videos, etc )
- [Link]{.uured-bold} Hyperlinks points from one URL to another.    

![](./M01_lecture01_figures/graph-analytics-for-cluster-marketing.png){width=55% fig-align="center"}
---

## Web Crawlers  

- Information on the Web is always changing and scattered across billions of pages served by millions of servers around the globe
- [Web crawlers]{.uured-bold} are programs that automatically download Web pages, to collect information that can be analyzed and mined in a `central location`.
- The primary application of crawlers are search engines
- A [search engine]{.uured-bold} takes the information collected by a crawler and creates an **index**
- The index maps content (keywords and phrases) to the pages, for rapid retrieval
- Search engine also rank results, to provide high quality results.
- Other applications: business intelligence, digital libraries, webometrics tools, research
- Malicious applications: harvesting email addresses and personal information for spam, phishing, and identity theft 

## How web crawlers work  

:::: {.columns}

::: {.column width="50%"}

- The basic concept of a crawler is simple: a [breadth-first]{.uured-bold} traversal of the Web graph, as in the [BFS]{.uured-bold} algorithm
- Start from a set of high-quality [seed]{.uured-bold} pages, recursively extracts the links within them to fetch more pages
- The queue of unvisited URLs ([frontier]{.uured-bold})
- [Fetch]{.uured-bold} pages dequeued from the frontier, [extract]{.uured-bold} links and add them to the frontier
- Stores the page (and other extracted info, i.e index terms) in a repository.
- [Devil in the details]{.uured-bold}: complications due to scalability, page revisit scheduling, spider traps, canonical URLs, robust HTML parsing, and the server ethics
:::

::: {.column width="50%"}

![](./M01_lecture01_figures/webcrowlers.gif){width=85% fig-align="center"}
:::

:::: 
---

## Web and Cloud Data Sources 

- [Internal Data]{.uured-bold}: Data generated by systems within a company or its data centers.  
- [Digital Analytics Data]{.uured-bold}: Behavioral and transactional data from websites, social networks, emails, mobile devices, IoT, and emerging digital formats.  
- [Social Data]{.uured-bold}: Digital data from social media, capturing user inputs, behaviors, and transactions.  
- [Syndicated Research Data]{.uured-bold}: Surveyed or sampled data on audience behaviors, attitudes, and beliefs.  
- [Audience Data]{.uured-bold}: Geographic and household-level information about specific populations.  
- [Financial Data]{.uured-bold}: Details on cash flow, investments, creditworthiness, and household financials from public and private sources.  
- [B2B Data]{.uured-bold}: Business-to-business (firmographic) data.  
- [Specialized Research Data]{.uured-bold}: Custom or primary research insights on audiences, customers, and prospects.  
- [Television and Cable Data]{.uured-bold}: Subscription-based records showing what content or ads viewers watched and when.  

---

## Cloud and Social Analytics Evolution
- [AD Tech]{.uured-bold}: Machines used data to rapidly serve the right ads to the right audience
- [Personalization and Semantic Web]{.uured-bold}: Focus on delivering personalization at scale
  - [Audience]{.uured-bold}: What type of audience arrive - at your site, devices they use
  - [Acquisition]{.uured-bold}: Where do they come from - Search, Blogs, Ad Campaigns
  - [Behavior]{.uured-bold}: What do they do after they come to the site, which pages they visit
  - [Conversion]{.uured-bold}: Are they meeting the Goals, e.g. Purchase Completed
  - Multiple Channels and Attribution problem
  - [Custom Events]{.uured-bold} – Google Tag Manager

# APIs and Web Services

## Introduction to APIs

- [A]{.uured-bold}pplication [P]{.uured-bold}rogramming [I]{.uured-bold}nterfaces: 
  - the "developer-facing" part of a data pipeline/service
- APIs are essential for modern software development.
- They enable interoperability, integration, and efficient development.
- Think of it like...
  - [Electrical outlet]{.uured-bold}: you just want **electricity** from it, without knowing details of Alternating/Direct Currents
  - [Water fountain]{.uured-bold}: you just want **water** from it, without knowing details of how it's pumped into the fountain
  - [Car]{.uured-bold}: you can **accelerate**, **brake**, & **reverse**, w/o knowledge of the combustion engine


## What Does an API Do?

- [Bridge]{.uured-bold}: It acts as a bridge between different software applications.
- [Communication]{.uured-bold}: Allows different software components to communicate and interact.
- Exposes **endpoints** for use by other developers, without requiring them to know the nuts and bolts of your pipeline/service
  - [Electrical outlet]{.uured-bold}: endpoint is **socket**,
  - [Water fountain]{.uured-bold}: endpoint is **aerator**,
  - [Car]{.uured-bold}: endpoint is **pedals**, **steering wheel**.

## Benefits and key concepts

- Benefits of Using APIs
  - [Efficiency]{.uured-bold}: Reuse existing functionality, saving development time and effort.
  - [Integration]{.uured-bold}: Programmatic nature integrates external services into your applications.
  - [Scalability]{.uured-bold}: Allows applications to scale by utilizing external resources.
- Key Concepts
  - [Requests and Responses]{.uured-bold}: APIs use requests to ask for data or perform actions & respond with the required information.
  - [Data Exchange]{.uured-bold}: Data is exchanged in a structured format, commonly in JSON or XML.
  - [Functionality]{.uured-bold}: Provides access to specific functions or features of a software or service.

---

## Types of APIs

- [Library APIs]{.uured-bold}: APIs provided by programming libraries for specific tasks or functions.
- [Operating System APIs]{.uured-bold}: Enable interaction with the operating system's resources & services.
- [Hardware APIs]{.uured-bold}: Interface with hardware components like cameras, sensors, or printers.
- [Web APIs]{.uured-bold}: Used for web-based apps & services. Accessed using HTTP requests.
   - **This is the case we are most interested in, since they allow us to gather data**
   - RESTful APIs
   - SOAP APIs
   - GraphQL APIs
  
![](./M01_lecture01_figures/api-ui.webp){width="70%" fig-align="center"}

## Closer look at Web APIs

Here are various types of web APIs, each with its own purpose. There are others but Restful APIs are the most common.

- [RESTful APIs]{.uured-bold}:
   - Representational State Transfer (REST) APIs are based on architectural principles.
   - Use standard HTTP methods (GET, POST, PUT, DELETE) to interact with resources.
   - Uses URLs to identify resources, & data is often transferred in JSON or XML format.
- [SOAP APIs]{.uured-bold}:
   - Simple Object Access Protocol (SOAP) APIs use XML for message formatting.
   - Tend to be more rigid and have stricter standards compared to REST.
   - Often used in enterprise-level applications and web services.
- [GraphQL APIs]{.uured-bold}:
   - GraphQL allows clients to request specific data, reducing over-fetching or under-fetching of information.
   - Clients define structure of the response, for more efficient & flexible data retrieval.


## Public vs. Private APIs

- [Open APIs (Public APIs)]{.uured-bold}:
  - Accessible to external developers and the public with minimal restrictions.
  - Typically require an API key or token for use.
  - Enable third-party developers to extend platform functionality (e.g., Twitter, Facebook).
- [Restricted APIs (Private APIs)]{.uured-bold}:
  - Limited to specific users or organizations.
  - Require special permissions or credentials for access.
  - Used for internal systems or trusted partner integrations.

## Common APIs for data gathering

Think of these as Project examples

- [Social Media APIs]{.uured-bold}: Access user data, post updates, or integrate social media features into applications.
- [Payment Gateway APIs]{.uured-bold}: Enable online payments and transactions.
- [Maps APIs]{.uured-bold}: Access maps, location data, and geospatial information.
- [Weather APIs]{.uured-bold}: Retrieve weather forecasts and historical data.
- [News APIs]{.uured-bold}: Aggregate news articles and content from various sources.
- [Stock Market APIs]{.uured-bold}: Access stock prices, market data, and financial information.
- [E-commerce APIs]{.uured-bold}: Integrate e-commerce platforms, product data, and payment processing.

## Common Sources

- [Twitter API]{.uured-bold}: Used for sentiment analysis and trend monitoring; access is now limited.  
- [Facebook Graph API]{.uured-bold}: Gathers data and insights on user interactions.  
- [Reddit API]{.uured-bold}: Accesses Reddit content, user data, and community features.  
- [Google Maps API]{.uured-bold}: Provides maps, geolocation, and routing data.  
- [GitHub API]{.uured-bold}: Retrieves code repositories and collaboration data.  
- [OpenWeatherMap API]{.uured-bold}: Supplies global weather data and forecasts.  
- [News API]{.uured-bold}: Aggregates news for sentiment analysis and trends.  
- [NASA API]{.uured-bold}: Offers space imagery, astronomy, and mission data.  
- [COVID-19 Data APIs]{.uured-bold}: Tracks and analyzes pandemic data.  
- [Web Scraping Libraries]{.uured-bold}: Tools like Beautiful Soup and Scrapy extract website data.  

## Example: Federal Reserve Bank of St. Louis.

- [Base URL]{.uured-bold}: <a href="https://api.stlouisfed.org/fred/" target="_blank">`https://api.stlouisfed.org/fred/`</a>  
- The [endpoint]{.uured-bold}: `series/observations`  
- The [parameters]{.uured-bold}:  
  - [series_id]{.uured-bold}: The unique ID of the economic data series (e.g., `"GDP"`).  
  - [api_key]{.uured-bold}: Your personal API key obtained from the FRED website.  
  - [file_type]{.uured-bold}: Desired response format (`json`, `xml`, etc.).  
- The [example request]{.uured-bold}:  
  <a href="https://api.stlouisfed.org/fred/series/observations?series_id=GDP&api_key=abcdefghijklmnopqrstuvwxyz123456&file_type=json" target="_blank">`https://api.stlouisfed.org/fred/series/observations?series_id=GDP&api_key=abcdefghijklmnopqrstuvwxyz123456&file_type=json`</a>  

**Note**: Replace `abcdefghijklmnopqrstuvwxyz123456` with your actual API key. You can obtain an API key by registering on the FRED website. ([fred.stlouisfed.org](https://fred.stlouisfed.org/docs/api/api_key.html))

---

## Fred API in python    
::: {.columns}
::: {.column}
```{python}
#| code-fold: false
#| echo: true
#| eval: false

import requests
import pandas as pd

# Define the FRED API parameters
base_url = "https://api.stlouisfed.org/fred/series/observations"
api_key = "abcdefghijklmnopqrstuvwxyz123456"  # Replace with your actual FRED API key
params = {
    "series_id": "UNRATE",  # Unemployment Rate series ID
    "api_key": api_key,
    "file_type": "json"  # Request JSON format
}

# Send the request to the FRED API
response = requests.get(base_url, params=params)

# Check if the request was successful
if response.status_code == 200:
    # Parse the JSON response
    data = response.json()
    observations = data.get("observations", [])
    
    # Convert observations to a DataFrame
    df = pd.DataFrame(observations)
    
    # Save the data to a CSV file
    file_path = "/data/UNRATE_data.csv"
    df.to_csv(file_path, index=False)
    print(f"Data saved to {file_path}")
else:
    print(f"Failed to fetch data. Status code: {response.status_code}, Message: {response.text}")

```

:::
::: {.column}
```{python}
#| code-fold: false
#| echo: false
#| eval: true
#| tbl-cap: "Data from the Unemployment Rate series (UNRATE) obtained from the FRED API"
#| label: tbl-unrate
import requests
import pandas as pd

# Define the FRED API parameters
base_url = "https://api.stlouisfed.org/fred/series/observations"
api_key = "ab4a5a187fe66da61622ff9cc9acb130"  # Replace with your actual FRED API key
params = {
    "series_id": "UNRATE",  # Unemployment Rate series ID
    "api_key": api_key,
    "file_type": "json"  # Request JSON format
}

# Send the request to the FRED API
response = requests.get(base_url, params=params)

# Check if the request was successful
if response.status_code == 200:
    # Parse the JSON response
    data = response.json()
    observations = data.get("observations", [])
    
    # Convert observations to a DataFrame
    df = pd.DataFrame(observations)
    
    # Save the data to a CSV file
    file_path = "data/UNRATE_data.csv"
    df.to_csv(file_path, index=False)
    # print(f"Data saved to {file_path}")
    display(df.head())
else:
    print(f"Failed to fetch data. Status code: {response.status_code}, Message: {response.text}")

```

:::

:::

---

## Reddit API 

- Here are some examples of Reddit API Usage. 
- Here are some useful resources:
  - [Scraping Reddit Using Python](https://www.geeksforgeeks.org/python/scraping-reddit-using-python/)
  - [Reddit API Documentation](https://www.reddit.com/dev/api/)
  - [Reddit API](https://www.jcchouinard.com/reddit-api/)
  - [Reddit API](https://pythonprogramming.net/introduction-python-reddit-api-wrapper-praw-tutorial/)

## SEC EDGAR API

```{python}
# | code-fold: false
# | echo: true
# | eval: true
# | output-location: column

import requests
from pathlib import Path

SEC_URL = (
    "https://www.sec.gov/Archives/edgar/data/"
    "320193/000032019325000079/aapl-20250927.htm"
)

OUTPUT_DIR = Path("./data/sec_10k")
OUTPUT_FILE = OUTPUT_DIR / "apple_2025_10k.html"

HEADERS = {
    "User-Agent": "Academic Research - SEC 10K Download (email@example.edu)",
    "Accept-Encoding": "gzip, deflate",
}

TIMEOUT_SECONDS = 30

# Create output directory
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Download
response = requests.get(
    SEC_URL,
    headers=HEADERS,
    timeout=TIMEOUT_SECONDS
)
response.raise_for_status()

OUTPUT_FILE.write_bytes(response.content)

print("Downloaded:", OUTPUT_FILE.resolve())
print("File size (KB):", round(OUTPUT_FILE.stat().st_size / 1024, 2))

```

## References

::: {.refs}

:::