{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "---\n",
        "title: \"Module 1 - Lab 1\"\n",
        "subtitle: \"From Raw Text to NLP Pipelines (SEC 10-K)\"\n",
        "author: \"Nakul R. Padalkar\"\n",
        "number-sections: true\n",
        "date: \"2024-11-21\"\n",
        "date-modified: today\n",
        "date-format: long\n",
        "format: \n",
        "    html:\n",
        "        code-overflow: wrap\n",
        "categories: ['1', 'M01:', 'Lab']\n",
        "description: \"Hands-on lab activity: Interacting with Textual Data in Jupyter and Colab.\"\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab Objective {.unnumbered}\n",
        "\n",
        "In this lab, you will:\n",
        "\n",
        "- Connect **Google Colab** to **VS Code**\n",
        "- Load real-world corporate text data (SEC 10-K filings)\n",
        "- Implement a **classical NLP preprocessing pipeline**\n",
        "- Answer **exploratory questions** about corporate disclosures using text analytics\n",
        "\n",
        "This lab establishes the **computational and conceptual foundation** for later work with embeddings and generative models.\n",
        "\n",
        "\n",
        "## Background Context {.unnumbered}\n",
        "\n",
        "Public companies file **Form 10-K** annually with the U.S. Securities and Exchange Commission (SEC).  \n",
        "These filings contain rich textual information about:\n",
        "\n",
        "- business operations  \n",
        "- risks and uncertainties  \n",
        "- management discussion  \n",
        "- regulatory disclosures  \n",
        "\n",
        "In this lab, we treat each 10-K as **raw text data** and apply a standard NLP pipeline to prepare it for analysis.\n",
        "\n",
        "## Dataset Overview  {.unnumbered}\n",
        "\n",
        "- All data for this lab is located in: [SEC-10K-2024/](https://drive.google.com/drive/folders/1q7BfsNHCewG1zNfnqyCcBj9p_RUt-zW6?usp=drive_link)\n",
        "- You will need to \"copy\" the folder to your own Google Drive\n",
        "- Right click on the folder, and then click \"Add shortcut to Drive\". This will allow you to access the folder from your drive!\n",
        "- This folder contains **plain-text 10-K filings** for multiple publicly traded firms.\n",
        "- Each file represents **one company’s annual report**.\n",
        "\n",
        "![](./M01_lecture02_figures/gdrive-add-folder.png){width=\"80%\" fig-align=\"center\"}\n",
        "\n",
        "## Research Framing (Important) {.unnumbered}\n",
        "\n",
        "You are **not** training a model yet. Instead, think of this lab as **asking structured questions of text**, such as:\n",
        "\n",
        "- What terms dominate risk disclosures?\n",
        "- How consistent is language across companies?\n",
        "- Which words survive aggressive cleaning?\n",
        "- How does preprocessing change the text representation?\n",
        "\n",
        "Your answers will be supported by **intermediate outputs**, not final predictions.\n",
        "\n",
        "## NLP Processing Pipeline {.unnumbered}\n",
        "\n",
        "You will implement the following pipeline **step by step**:\n",
        "\n",
        "1. Raw text  \n",
        "2. Sentence segmentation  \n",
        "3. Tokenization  \n",
        "4. Part-of-Speech (POS) tagging  \n",
        "5. Stop-word removal  \n",
        "6. Stemming / Lemmatization  \n",
        "7. Dependency parsing  \n",
        "8. String metrics & matching  \n",
        "Each stage produces **artifacts** that help you answer analytical questions.\n",
        "\n",
        "## Load and Inspect the Data {.unnumbered}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted successfully. Data directory found.\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "\n",
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount(\"/content/drive\")\n",
        "\n",
        "# DATA_DIR = \"/content/drive/MyDrive/Research/SEC-10K-2024\"\n",
        "\n",
        "# assert os.path.exists(DATA_DIR), (\n",
        "#     \"Google Drive is not mounted or the dataset path is incorrect. \"\n",
        "#     \"Did you run drive.mount()?\"\n",
        "# )\n",
        "\n",
        "# print(\"Drive mounted successfully. Data directory found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 7754 SEC filings\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "SEC_DIR = Path(\"D:/Repositories/AD698-generative-ai-for-BA/data/SEC-10K-2024\")\n",
        "# DATA_ROOT = Path(\"/content/drive/MyDrive/Research\")\n",
        "# SEC_DIR = DATA_ROOT / \"SEC-10K-2024\"\n",
        "\n",
        "assert SEC_DIR.exists(), \"SEC data folder not found. Check Drive mount.\"\n",
        "\n",
        "sec_files = list(SEC_DIR.glob(\"*.txt\"))\n",
        "print(f\"Found {len(sec_files)} SEC filings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Header>\n",
            "<FileStats>\n",
            "    <FileName>20240426_10-K-A_edgar_data_1434524_0001104659-24-053028.txt</FileName>\n",
            "    <GrossFileSize>573357</GrossFileSize>\n",
            "    <NetFileSize>79834</NetFileSize>\n",
            "    <NonText_DocumentType_Chars>125288</NonText_DocumentType_Chars>\n",
            "    <HTML_Chars>219025</HTML_Chars>\n",
            "    <XBRL_Chars>69133</XBRL_Chars>\n",
            "    <XML_Chars>53924</XML_Chars>\n",
            "    <N_Exhibits>8</N_Exhibits>\n",
            "</FileStats>\n",
            "<SEC-Header>\n",
            "0001104659-24-053028.hdr.sgml : 20240426\n",
            "<ACCEPTANCE-DATETIME>20240426164535\n",
            "ACCESSION NUMBER:\t\t0001104659-24-053028\n",
            "CONFORMED SUBMISSION TYPE:\t10-K/A\n",
            "PUBLIC DOCUMENT COUNT:\t\t19\n",
            "CONFORMED PERIOD OF REPORT:\t20231231\n",
            "FILED AS OF DATE:\t\t20240426\n",
            "DATE AS OF CHANGE:\t\t20240426\n",
            "\n",
            "FILER:\n",
            "\n",
            "\tCOMPANY DATA:\t\n",
            "\t\tCOMPANY CONFORMED NAME:\t\t\tClearSign Technologies Corp\n",
            "\t\tCENTRAL INDEX KEY:\t\t\t0001434524\n",
            "\t\tSTANDARD INDUSTRIAL CLASSIFICATION:\tINDUSTRIAL INSTRUMENTS FOR MEASUREMENT, DISPLAY, AND CONTROL [3823]\n",
            "\t\tORGANIZATION NAME:           \t08 Industrial Applications and Services\n",
            "\t\tIRS NUMBER:\t\t\t\t000000000\n",
            "\t\tSTATE OF INCORPORATION:\t\t\tDE\n",
            "\t\tFISCAL YEAR END:\t\t\t1231\n",
            "\n",
            "\tFILING VALUES:\n",
            "\t\tFORM TYPE:\t\t10-K/A\n",
            "\t\tSEC ACT:\t\t1934 Act\n",
            "\t\tSEC FILE NUMBER:\t001-35521\n",
            "\t\tFILM NUMBER:\t\t24884683\n",
            "\n",
            "\tBUSINESS ADDRESS:\t\n",
            "\t\tSTREET 1:\t\t8023 E. 63RD PLACE, SUITE 101\n",
            "\t\tCITY:\t\t\tTULSA\n",
            "\t\tSTATE:\t\t\tOK\n",
            "\t\tZIP:\t\t\t74133\n",
            "\t\tBUSINESS PHONE:\t\t(918) 236-6461\n",
            "\n",
            "\tMAIL ADDRESS:\t\n",
            "\t\tSTREET 1:\t\t8023 E. 63RD PLACE, SUITE 101\n",
            "\t\tCITY:\t\t\tTULSA\n",
            "\t\tSTATE:\t\t\tOK\n",
            "\t\tZIP:\t\t\t74133\n",
            "\n",
            "\tFORMER COMPANY:\t\n",
            "\t\tFORMER CONFORMED NAME:\tCLEARSIGN COMBUSTION CORP\n",
            "\t\tDATE\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Read a sample document\n",
        "sample_text = sec_files[0].read_text(encoding=\"utf-8\")\n",
        "print(sample_text[:1500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What sections of the 10-K appear most frequently in the opening text?\n",
        "This will help you understand the structure of the document and identify key areas for analysis (e.g., risk factors, management discussion). We first start with Sentence Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from zipfile import ZipFile\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\") # Open Multilingual Wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2400942173.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of sentences: {len(sentences)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sentences = sent_tokenize(sample_text)\n",
        "print(f\"Number of sentences: {len(sentences)}\")\n",
        "sentences[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Are sentences in 10-Ks longer or shorter than typical news or social media text?\n",
        "\n",
        "\n",
        "```python\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(sample_text)\n",
        "tokens[:30]\n",
        "```\n",
        "\n",
        "## What kinds of tokens appear that are not “words” (e.g., symbols, numbers, legal references)?\n",
        "\n",
        "```python\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "\n",
        "from nltk import pos_tag\n",
        "\n",
        "pos_tags = pos_tag(tokens[:50])\n",
        "pos_tags\n",
        "```\n",
        "\n",
        "## Which POS categories dominate risk-related sections (nouns, verbs, adjectives)?\n",
        "\n",
        "\n",
        "```python\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "filtered_tokens = [\n",
        "    t.lower() for t in tokens\n",
        "    if t.isalpha() and t.lower() not in stop_words\n",
        "]\n",
        "\n",
        "filtered_tokens[:30]\n",
        "```\n",
        "\n",
        "## Which important business terms survive stop-word removal?\n",
        "\n",
        "```python\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stems = [stemmer.stem(t) for t in filtered_tokens[:20]]\n",
        "lemmas = [lemmatizer.lemmatize(t) for t in filtered_tokens[:20]]\n",
        "\n",
        "list(zip(filtered_tokens[:20], stems, lemmas))\n",
        "```\n",
        "\n",
        "## Which transformation preserves interpretability better for financial text?\n",
        "\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(sentences[0])\n",
        "[(token.text, token.dep_, token.head.text) for token in doc]\n",
        "```\n",
        "\n",
        "## How might dependency relationships help identify risk statements or obligations?\n",
        "\n",
        "```python\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "def similarity(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "similarity(\n",
        "    \"risk management strategy\",\n",
        "    \"enterprise risk management\"\n",
        ")\n",
        "```\n",
        "\n",
        "## Why might approximate string matching be useful for cross-company comparison?\n",
        "\n",
        "\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "Submit word document with answering the questions in addition to the Jupyter notebook with the code and outputs (either `.ipynb` or `.pdf`):\n",
        "\n",
        "## Key Takeaway\n",
        "\n",
        "> Before we can generate language,\n",
        "> we must first **discipline text into structure**.\n",
        "\n",
        "This pipeline is the foundation upon which **Bag of Words, TF-IDF, embeddings, and generative models** are built.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "AD698-generative-ai-for-BA",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
