[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "AD 6698 Generative AI for Business Analytics",
    "section": "",
    "text": "Please adhere to the due dates posted on the course website. Late work is not accepted.\n\n\n\n\n  \n    \n      Day\n      A1\n      A2\n      A3\n    \n  \n  \n    \n      Day 1\n      Tuesday\n      Thursday\n      Wednesday\n    \n    \n      Day 2\n      Wednesday\n      Friday\n      Thursday\n    \n    \n      Day 3\n      Thursday\n      Saturday\n      Friday\n    \n    \n      Day 4\n      Friday\n      Sunday\n      Saturday\n    \n    \n      Day 5\n      Saturday\n      Monday\n      Sunday\n    \n    \n      Day 6\n      Sunday\n      Tuesday\n      Monday\n    \n    \n      Day 7\n      Monday\n      Wednesday\n      Tuesday\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      Lecture\n      Title\n      Description\n      Online\n      On-Campus\n    \n  \n  \n    \n      L0.1\n      Dev Setup: GitHub, Jupyter, VS Code, Colab\n      Course tools setup and basic version control with GitHub, notebooks, and IDEs.\n      nan\n      nan\n    \n    \n      L0.2\n      Introduction to Neural Networks\n      Understanding the basics of neural networks as a foundation for generative models.\n      nan\n      nan\n    \n    \n      L1.1\n      Introduction to Course, Natural Language, and Generative AI Landscape\n      Overview of generative AI, its business relevance, and natural language as a data type.\n      nan\n      nan\n    \n    \n      L1.2\n      Introduction to Natural Language Processing\n      Core NLP concepts: tokenization, POS tagging, parsing, and text pre-processing.\n      nan\n      nan\n    \n    \n      L2.1\n      Prompt Engineering Fundamentals\n      Learn prompt design, zero-/few-shot techniques, and ICL with business use cases.\n      nan\n      nan\n    \n    \n      L2.2\n      Tokenization, Embeddings, and Vector Semantics\n      Deep dive into tokenization methods, word embeddings, and semantic search foundations.\n      nan\n      nan\n    \n    \n      L3.1\n      LLM Internals: Attention, Transformers, and Positional Encoding\n      Explore transformer architecture, attention mechanisms, and context windows.\n      nan\n      nan\n    \n    \n      L3.2\n      Fine-Tuning Pipelines & Data Handling\n      Set up custom model pipelines and format datasets for supervised fine-tuning.\n      nan\n      nan\n    \n    \n      L4.1\n      RAG Basics: Embeddings, Chunking, Indexing\n      Understanding retrieval-augmented generation concepts and corpus preparation.\n      nan\n      nan\n    \n    \n      L4.2\n      RAG Query Pipelines with LangChain & LlamaIndex\n      Build Q&A systems using vector DBs, LangChain chains, and document loaders.\n      nan\n      nan\n    \n    \n      L5.1\n      LoRA, QLoRA, PEFT Techniques\n      Learn parameter-efficient fine-tuning methods to adapt base models.\n      nan\n      nan\n    \n    \n      L5.2\n      Embeddings & Similarity Search\n      Work with sentence embeddings for document clustering, search, and relevance.\n      nan\n      nan\n    \n    \n      L6.1\n      Memory & Context Management\n      Strategies for working with limited context: caching, summaries, and memory buffers.\n      nan\n      nan\n    \n    \n      L6.2\n      Multimodal Models and Use Cases\n      Explore vision-language models and multimodal business applications.\n      nan\n      nan\n    \n    \n      L7.1\n      Model Evaluation & GPT-Eval\n      Evaluate generated outputs using automated tools, manual rubrics, and hallucination detection.\n      nan\n      nan\n    \n    \n      L7.2\n      Responsible AI & Bias Detection\n      Assess bias, fairness, and ethics in generated outputs using toolkits and guidelines.\n      nan\n      nan\n    \n    \n      L8.1\n      Intro to Agents: ReAct, Tool Use\n      Overview of agent-based systems and tool-augmented LLM workflows.\n      nan\n      nan\n    \n    \n      L8.2\n      Multi-Agent Systems (AutoGen, CrewAI)\n      Design and orchestrate multi-agent collaborative systems for enterprise tasks.\n      nan\n      nan"
  },
  {
    "objectID": "schedule.html#course-schedule",
    "href": "schedule.html#course-schedule",
    "title": "AD 6698 Generative AI for Business Analytics",
    "section": "",
    "text": "Please adhere to the due dates posted on the course website. Late work is not accepted.\n\n\n\n\n  \n    \n      Day\n      A1\n      A2\n      A3\n    \n  \n  \n    \n      Day 1\n      Tuesday\n      Thursday\n      Wednesday\n    \n    \n      Day 2\n      Wednesday\n      Friday\n      Thursday\n    \n    \n      Day 3\n      Thursday\n      Saturday\n      Friday\n    \n    \n      Day 4\n      Friday\n      Sunday\n      Saturday\n    \n    \n      Day 5\n      Saturday\n      Monday\n      Sunday\n    \n    \n      Day 6\n      Sunday\n      Tuesday\n      Monday\n    \n    \n      Day 7\n      Monday\n      Wednesday\n      Tuesday\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      Lecture\n      Title\n      Description\n      Online\n      On-Campus\n    \n  \n  \n    \n      L0.1\n      Dev Setup: GitHub, Jupyter, VS Code, Colab\n      Course tools setup and basic version control with GitHub, notebooks, and IDEs.\n      nan\n      nan\n    \n    \n      L0.2\n      Introduction to Neural Networks\n      Understanding the basics of neural networks as a foundation for generative models.\n      nan\n      nan\n    \n    \n      L1.1\n      Introduction to Course, Natural Language, and Generative AI Landscape\n      Overview of generative AI, its business relevance, and natural language as a data type.\n      nan\n      nan\n    \n    \n      L1.2\n      Introduction to Natural Language Processing\n      Core NLP concepts: tokenization, POS tagging, parsing, and text pre-processing.\n      nan\n      nan\n    \n    \n      L2.1\n      Prompt Engineering Fundamentals\n      Learn prompt design, zero-/few-shot techniques, and ICL with business use cases.\n      nan\n      nan\n    \n    \n      L2.2\n      Tokenization, Embeddings, and Vector Semantics\n      Deep dive into tokenization methods, word embeddings, and semantic search foundations.\n      nan\n      nan\n    \n    \n      L3.1\n      LLM Internals: Attention, Transformers, and Positional Encoding\n      Explore transformer architecture, attention mechanisms, and context windows.\n      nan\n      nan\n    \n    \n      L3.2\n      Fine-Tuning Pipelines & Data Handling\n      Set up custom model pipelines and format datasets for supervised fine-tuning.\n      nan\n      nan\n    \n    \n      L4.1\n      RAG Basics: Embeddings, Chunking, Indexing\n      Understanding retrieval-augmented generation concepts and corpus preparation.\n      nan\n      nan\n    \n    \n      L4.2\n      RAG Query Pipelines with LangChain & LlamaIndex\n      Build Q&A systems using vector DBs, LangChain chains, and document loaders.\n      nan\n      nan\n    \n    \n      L5.1\n      LoRA, QLoRA, PEFT Techniques\n      Learn parameter-efficient fine-tuning methods to adapt base models.\n      nan\n      nan\n    \n    \n      L5.2\n      Embeddings & Similarity Search\n      Work with sentence embeddings for document clustering, search, and relevance.\n      nan\n      nan\n    \n    \n      L6.1\n      Memory & Context Management\n      Strategies for working with limited context: caching, summaries, and memory buffers.\n      nan\n      nan\n    \n    \n      L6.2\n      Multimodal Models and Use Cases\n      Explore vision-language models and multimodal business applications.\n      nan\n      nan\n    \n    \n      L7.1\n      Model Evaluation & GPT-Eval\n      Evaluate generated outputs using automated tools, manual rubrics, and hallucination detection.\n      nan\n      nan\n    \n    \n      L7.2\n      Responsible AI & Bias Detection\n      Assess bias, fairness, and ethics in generated outputs using toolkits and guidelines.\n      nan\n      nan\n    \n    \n      L8.1\n      Intro to Agents: ReAct, Tool Use\n      Overview of agent-based systems and tool-augmented LLM workflows.\n      nan\n      nan\n    \n    \n      L8.2\n      Multi-Agent Systems (AutoGen, CrewAI)\n      Design and orchestrate multi-agent collaborative systems for enterprise tasks.\n      nan\n      nan"
  },
  {
    "objectID": "deliverables.html",
    "href": "deliverables.html",
    "title": "AD 6698 Generative AI for Business Analytics",
    "section": "",
    "text": "Day\nA1\nO1\n\n\n\n\nDay 1\nMonday\nTuesday\n\n\nDay 2\nTuesday\nWednesday\n\n\nDay 3\nWednesday\nThursday\n\n\nDay 4\nThursday\nFriday\n\n\nDay 5\nFriday\nSaturday\n\n\nDay 6\nSaturday\nSunday\n\n\nDay 7\nSunday\nMonday\n\n\n\n\n\n\nSection { first_section }\n\nThe first day of the week (Day 1) for section { first_section } is { first_day }, this is the day we will conduct our live classroom sessions.\nThe last day of the week (Day 7) for section { first_section } is { last_day_first_section }.\n\nSection { second_section }\n\nThe first day of the week (Day 1) for section { second_section } is { second_day }, this is the day we will conduct our live classroom sessions.\nThe last day of the week (Day 7) for section { second_section } is { last_day_second_section }.\n\n\nPlease adhere to the due dates posted on the course website. Late work is not accepted.\n\nAll assignments (for on campus) will open one to two weeks before the due date.\nAll assignments (for online) will open in the first two weeks.\nAll Labs (for on campus) will open one week before the due date.\nAll Labs (for online) will open in the first two weeks and are ungraded."
  },
  {
    "objectID": "deliverables.html#course-schedule",
    "href": "deliverables.html#course-schedule",
    "title": "AD 6698 Generative AI for Business Analytics",
    "section": "",
    "text": "Day\nA1\nO1\n\n\n\n\nDay 1\nMonday\nTuesday\n\n\nDay 2\nTuesday\nWednesday\n\n\nDay 3\nWednesday\nThursday\n\n\nDay 4\nThursday\nFriday\n\n\nDay 5\nFriday\nSaturday\n\n\nDay 6\nSaturday\nSunday\n\n\nDay 7\nSunday\nMonday\n\n\n\n\n\n\nSection { first_section }\n\nThe first day of the week (Day 1) for section { first_section } is { first_day }, this is the day we will conduct our live classroom sessions.\nThe last day of the week (Day 7) for section { first_section } is { last_day_first_section }.\n\nSection { second_section }\n\nThe first day of the week (Day 1) for section { second_section } is { second_day }, this is the day we will conduct our live classroom sessions.\nThe last day of the week (Day 7) for section { second_section } is { last_day_second_section }.\n\n\nPlease adhere to the due dates posted on the course website. Late work is not accepted.\n\nAll assignments (for on campus) will open one to two weeks before the due date.\nAll assignments (for online) will open in the first two weeks.\nAll Labs (for on campus) will open one week before the due date.\nAll Labs (for online) will open in the first two weeks and are ungraded."
  },
  {
    "objectID": "deliverables.html#online-o1-spring",
    "href": "deliverables.html#online-o1-spring",
    "title": "AD 6698 Generative AI for Business Analytics",
    "section": "Online (O1) Spring",
    "text": "Online (O1) Spring"
  },
  {
    "objectID": "deliverables.html#on-campus-a1-spring",
    "href": "deliverables.html#on-campus-a1-spring",
    "title": "AD 6698 Generative AI for Business Analytics",
    "section": "On Campus (A1) Spring",
    "text": "On Campus (A1) Spring\n\n\n\n\n\n#\nA1\nLab (Due)\nAssignments\nProject Milestones\nParticipation\n\n\n\n\nL1\n26-Jan (Mon)\n\n\n\n\n\n\nL2\n02-Feb (Mon)\nLab 1\n\n\n\n\n\nL3\n09-Feb (Mon)\nLab 2\nA1 (Due: Feb 10)\nUngraded Milestone 1\n\n\n\nL4\n17-Feb (Tue)\nLab 3\n\n\n\n\n\nL5\n23-Feb (Mon)\n\nA2 (Due: Feb 24)\nUngraded Milestone 2\n\n\n\nL6\n02-Mar (Mon)\nLab 5\n\n\n\n\n\nL7\n16-Mar (Mon)\nLab 6\nA3 (Due: Mar 17)\nUngraded Milestone 3\n\n\n\nL8\n23-Mar (Mon)\nLab 7\n\n\n\n\n\nL9\n30-Mar (Mon)\nLab 8\nA4 (Due: Mar 31)\nUngraded Milestone 4\n\n\n\nL10\n06-Apr (Mon)\nLab 9\n\n\n\n\n\nL11\n13-Apr (Mon)\n\nA5 (Due: Apr 14)\nUngraded Milestone 5\n\n\n\nL12\n20-Apr (Mon)\n\n\nPresentation\nParticipation 1 + Participation 2\n\n\nL13\n27-Apr (Mon)\n\n\nFinal Project Report + Presentation"
  },
  {
    "objectID": "M07/M07_Wrapup.html",
    "href": "M07/M07_Wrapup.html",
    "title": "M07 Wrapup",
    "section": "",
    "text": "1 M07 Wrapup"
  },
  {
    "objectID": "M07/M07_Lab02.html",
    "href": "M07/M07_Lab02.html",
    "title": "M07 Lab02",
    "section": "",
    "text": "1 M07 Lab02"
  },
  {
    "objectID": "M07/M07_LN02.html",
    "href": "M07/M07_LN02.html",
    "title": "M07 Lecture Note 02",
    "section": "",
    "text": "1 M07 Lecture Note 02"
  },
  {
    "objectID": "M07/M07_Highlights.html",
    "href": "M07/M07_Highlights.html",
    "title": "M07 Highlights",
    "section": "",
    "text": "1 M07 Highlights"
  },
  {
    "objectID": "M06/M06_Wrapup.html",
    "href": "M06/M06_Wrapup.html",
    "title": "M06 Wrapup",
    "section": "",
    "text": "1 M06 Wrapup"
  },
  {
    "objectID": "M06/M06_Lab02.html",
    "href": "M06/M06_Lab02.html",
    "title": "M06 Lab02",
    "section": "",
    "text": "1 M06 Lab02"
  },
  {
    "objectID": "M06/M06_LN02.html",
    "href": "M06/M06_LN02.html",
    "title": "M06 Lecture Note 02",
    "section": "",
    "text": "1 M06 Lecture Note 02"
  },
  {
    "objectID": "M06/M06_Highlights.html",
    "href": "M06/M06_Highlights.html",
    "title": "M06 Highlights",
    "section": "",
    "text": "1 M06 Highlights"
  },
  {
    "objectID": "M05/M05_Wrapup.html",
    "href": "M05/M05_Wrapup.html",
    "title": "M05 Wrapup",
    "section": "",
    "text": "1 M05 Wrapup"
  },
  {
    "objectID": "M05/M05_Lab02.html",
    "href": "M05/M05_Lab02.html",
    "title": "M05 Lab02",
    "section": "",
    "text": "1 M05 Lab02"
  },
  {
    "objectID": "M05/M05_LN02.html",
    "href": "M05/M05_LN02.html",
    "title": "M05 Lecture Note 02",
    "section": "",
    "text": "1 M05 Lecture Note 02"
  },
  {
    "objectID": "M05/M05_Highlights.html",
    "href": "M05/M05_Highlights.html",
    "title": "M05 Highlights",
    "section": "",
    "text": "1 M05 Highlights"
  },
  {
    "objectID": "M04/M04_Wrapup.html",
    "href": "M04/M04_Wrapup.html",
    "title": "M04 Wrapup",
    "section": "",
    "text": "1 M04 Wrapup"
  },
  {
    "objectID": "M04/M04_Lab02.html",
    "href": "M04/M04_Lab02.html",
    "title": "M04 Lab02",
    "section": "",
    "text": "1 M04 Lab02"
  },
  {
    "objectID": "M04/M04_LN02.html",
    "href": "M04/M04_LN02.html",
    "title": "M04 Lecture Note 02",
    "section": "",
    "text": "1 M04 Lecture Note 02"
  },
  {
    "objectID": "M04/M04_Highlights.html",
    "href": "M04/M04_Highlights.html",
    "title": "M04 Highlights",
    "section": "",
    "text": "1 M04 Highlights"
  },
  {
    "objectID": "M03/M03_Wrapup.html",
    "href": "M03/M03_Wrapup.html",
    "title": "M03 Wrapup",
    "section": "",
    "text": "1 M03 Wrapup"
  },
  {
    "objectID": "M03/M03_Lab02.html",
    "href": "M03/M03_Lab02.html",
    "title": "M03 Lab02",
    "section": "",
    "text": "1 M03 Lab02"
  },
  {
    "objectID": "M03/M03_LN02.html",
    "href": "M03/M03_LN02.html",
    "title": "M03 Lecture Note 02",
    "section": "",
    "text": "1 M03 Lecture Note 02"
  },
  {
    "objectID": "M03/M03_Highlights.html",
    "href": "M03/M03_Highlights.html",
    "title": "M03 Highlights",
    "section": "",
    "text": "1 M03 Highlights"
  },
  {
    "objectID": "M02/M02_Wrapup.html",
    "href": "M02/M02_Wrapup.html",
    "title": "M02 Wrapup",
    "section": "",
    "text": "1 M02 Wrapup"
  },
  {
    "objectID": "M02/M02_Lab02.html",
    "href": "M02/M02_Lab02.html",
    "title": "M02 Lab02",
    "section": "",
    "text": "1 M02 Lab02"
  },
  {
    "objectID": "M02/M02_LN02.html",
    "href": "M02/M02_LN02.html",
    "title": "M02 Lecture Note 02",
    "section": "",
    "text": "1 M02 Lecture Note 02"
  },
  {
    "objectID": "M02/M02_Highlights.html",
    "href": "M02/M02_Highlights.html",
    "title": "M02 Highlights",
    "section": "",
    "text": "1 M02 Highlights"
  },
  {
    "objectID": "M01/M01_Wrapup.html",
    "href": "M01/M01_Wrapup.html",
    "title": "M01 Wrapup",
    "section": "",
    "text": "1 M01 Wrapup"
  },
  {
    "objectID": "M01/M01_Lab02.html",
    "href": "M01/M01_Lab02.html",
    "title": "M01 Lab02",
    "section": "",
    "text": "1 M01 Lab02"
  },
  {
    "objectID": "M01/M01_LN02.html",
    "href": "M01/M01_LN02.html",
    "title": "M01 Lecture Note 02",
    "section": "",
    "text": "1 M01 Lecture Note 02"
  },
  {
    "objectID": "M01/M01_Highlights.html",
    "href": "M01/M01_Highlights.html",
    "title": "M01 Highlights",
    "section": "",
    "text": "1 M01 Highlights"
  },
  {
    "objectID": "M00/M00_Wrapup.html",
    "href": "M00/M00_Wrapup.html",
    "title": "M00 Wrapup",
    "section": "",
    "text": "1 M00 Wrapup"
  },
  {
    "objectID": "M00/M00_Lab02.html",
    "href": "M00/M00_Lab02.html",
    "title": "M00 Lab02",
    "section": "",
    "text": "1 M00 Lab02"
  },
  {
    "objectID": "M00/M00_LN02.html",
    "href": "M00/M00_LN02.html",
    "title": "M00 Lecture Note 02",
    "section": "",
    "text": "1 M00 Lecture Note 02"
  },
  {
    "objectID": "M00/M00_Highlights.html",
    "href": "M00/M00_Highlights.html",
    "title": "M00 Highlights",
    "section": "",
    "text": "1 M00 Highlights"
  },
  {
    "objectID": "M00/M00_Proj.html",
    "href": "M00/M00_Proj.html",
    "title": "M00 Project",
    "section": "",
    "text": "1 M00 Project"
  },
  {
    "objectID": "M01/M01_A.html",
    "href": "M01/M01_A.html",
    "title": "M01 Assignment",
    "section": "",
    "text": "1 M01 Assignment"
  },
  {
    "objectID": "M01/M01_LN01.html#modules-07",
    "href": "M01/M01_LN01.html#modules-07",
    "title": "M01: Lecture Notes",
    "section": "5.1 Modules 0–7",
    "text": "5.1 Modules 0–7\nEach module has two lectures, two labs, and one assignment. Topics include:\n\nModule 0 - AI Foundations & Developer Setup\nModule 1 - Natural Language and Generative AI Landscape\nModule 2 - Prompt Engineering & Vector Representations\nModule 3 - LLM Architecture & Fine-Tuning\nModule 4 - Retrieval-Augmented Generation (RAG) Systems\nModule 5 - Efficient Tuning & Similarity Search\nModule 6 - LLM Memory and Multimodal Intelligence\nModule 7 - Evaluation, Bias, and Responsible AI\nModule 8 - Autonomous Agents and Multi-Agent Systems"
  },
  {
    "objectID": "M01/M01_LN01.html#participation-components",
    "href": "M01/M01_LN01.html#participation-components",
    "title": "M01: Lecture Notes",
    "section": "6.1 Participation Components",
    "text": "6.1 Participation Components\n\n6.1.1 LLM Lab Practice\n\nWeekly hands-on exercises using LLMs\nFocus on prompting, automation, and applied business tasks\n\n\n\n6.1.2 Tooling: GitHub, Python, APIs\n\nYou will maintain a public portfolio repository\nEvery assignment and project will be submitted via GitHub"
  },
  {
    "objectID": "M01/M01_LN01.html#in-class-labs",
    "href": "M01/M01_LN01.html#in-class-labs",
    "title": "M01: Lecture Notes",
    "section": "6.2 In-Class Labs",
    "text": "6.2 In-Class Labs\nLabs are short, guided activities where you:\n\nWork directly with LLM models\nExplore generative workflows\nBuild first-draft AI tools that feed into assignments\nAnalyze data, automate tasks, and generate outputs\nWrite practical reports and visualizations\n\nOn-campus: must submit weekly Online: optional but recommended for practice"
  },
  {
    "objectID": "M01/M01_LN01.html#individual-assignments",
    "href": "M01/M01_LN01.html#individual-assignments",
    "title": "M01: Lecture Notes",
    "section": "6.3 Individual Assignments",
    "text": "6.3 Individual Assignments\nFour structured assignments that build core competencies:\n\nPrompt Engineering & Automated Analytics\nRAG and Vector Search\nAI Pipeline for Business Intelligence\nAutomated Workflow + API Integration"
  },
  {
    "objectID": "M01/M01_LN01.html#group-project",
    "href": "M01/M01_LN01.html#group-project",
    "title": "M01: Lecture Notes",
    "section": "6.4 Group Project",
    "text": "6.4 Group Project\nA semester-long generative-AI solution to a business problem.\n\n6.4.1 Components\n\n\n\n\n\n\n\n\nComponent\nPoints\nDescription\n\n\n\n\nMilestones via GitHub\n80\nRepo setup, design docs, data prep, RAG prototype, LLM workflow, final output\n\n\nPresentation\n40\nDemonstrates model workflow and business impact\n\n\nPeer Evaluation\n40\nCollaboration, clarity, and contributions\n\n\n\nTotal: 160 points"
  },
  {
    "objectID": "M01/M01_LN01.html#introduction",
    "href": "M01/M01_LN01.html#introduction",
    "title": "M01: Lecture Notes",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nGenerative artificial intelligence represents one of the most significant paradigm shifts in the history of computing. For decades, computers have been designed to follow instructions - perform calculations, retrieve information, classify inputs, and obey predefined rules. Generative AI introduces something profoundly new: the ability for machines to produce original content, synthesize knowledge, reason through language, and interact conversationally in a way that resembles human creativity and communication.\nThese systems do not simply automate tasks; they reshape how people think, work, and engage with information. While their roots lie in classical machine learning, generative models extend far beyond traditional predictive analytics. They do not merely recognize patterns - they generate new ones."
  },
  {
    "objectID": "M01/M01_LN01.html#what-is-generative-ai",
    "href": "M01/M01_LN01.html#what-is-generative-ai",
    "title": "M01: Lecture Notes",
    "section": "9.2 What Is Generative AI?",
    "text": "9.2 What Is Generative AI?\nGenerative AI refers to models capable of creating new artifacts that did not previously exist in the training data. These artifacts may include:\n\nEssays, summaries, stories, and explanations\nImages, sketches, animations, or design concepts\nScientific hypotheses or molecular structures\nCode, scripts, and executable workflows\nSynthetic data for simulation or experimentation\nAudio, music, or environmental soundscapes\nVideo sequences and motion dynamics\n\nWhat distinguishes generative AI from earlier forms of artificial intelligence is its ability to create content that is coherent, context-aware, adaptive, and semantically meaningful.\nA generative model learns the underlying structure of the data it is trained on - the grammar of language, the geometry of images, the relationships in chemistry, the cadence of music - and uses this learned representation to create new outputs consistent with these structures."
  },
  {
    "objectID": "M01/M01_LN01.html#generative-ai-vs.-traditional-artificial-intelligence",
    "href": "M01/M01_LN01.html#generative-ai-vs.-traditional-artificial-intelligence",
    "title": "M01: Lecture Notes",
    "section": "9.3 Generative AI vs. Traditional Artificial Intelligence",
    "text": "9.3 Generative AI vs. Traditional Artificial Intelligence\nArtificial intelligence is a broad umbrella that includes:\n\ncomputer vision\nspeech recognition\nclassical natural language processing\nreinforcement learning\nrobotics\npredictive modeling\noptimization algorithms\n\nThese systems primarily classify, detect, optimize, or recommend based on the data they receive.\nGenerative AI is a subset of AI that explicitly focuses on creation. It does not simply categorize (“This is a dog”), nor does it merely predict a numeric outcome (“This house should be priced at $820,000”). Instead, it asks:\n\n“Given what I’ve learned about the world, what can I produce that fits the patterns, rules, and structures I’ve internalized?”\n\nThis ability places generative AI closer to tasks traditionally seen as the domain of human cognition - drafting, designing, summarizing, reasoning, theorizing, and imagining."
  },
  {
    "objectID": "M01/M01_LN01.html#the-emergence-of-generative-ai",
    "href": "M01/M01_LN01.html#the-emergence-of-generative-ai",
    "title": "M01: Lecture Notes",
    "section": "9.4 The Emergence of Generative AI",
    "text": "9.4 The Emergence of Generative AI\nAlthough the idea of generative modeling has existed for decades, several breakthroughs in the 2010s transformed it into a practical, powerful technology.\n\n9.4.1 Variational Autoencoders (2013)\nVAEs introduced the idea of compressing data into a hidden “latent space” that captures essential features. By sampling from this latent space, the model could reconstruct or vary the input, generating new samples that resemble the originals. This was the first demonstration that deep networks could create.\n\n\n9.4.2 Generative Adversarial Networks (2014)\nGANs marked a dramatic leap. Two networks - a generator and a discriminator - compete:\n\nthe generator synthesizes new data\nthe discriminator evaluates authenticity\n\nThrough this adversarial training, the generator becomes increasingly skilled at creating highly realistic output. GANs powered early deepfakes, artistic style-transfer, and synthetic photography.\n\n\n9.4.3 Transformer Models (2017)\nThe most transformative innovation came with the transformer architecture. Transformers introduced self-attention, a mechanism that allows models to consider relationships between all elements of a sequence simultaneously. This made it possible to train models on much larger corpora of text and to capture subtle patterns in:\n\nmeaning\nsyntax\ncontext\ndiscourse structure\nworld knowledge\n\nTransformers reshaped natural language processing and enabled the development of large-scale generative models.\n\n\n9.4.4 The Foundational Model Era\nThe last several years have given rise to large generative models capable of:\n\nfollowing instructions\ncarrying conversation\nintegrating reasoning with generation\nexplaining data\nproducing creative or analytical artifacts\n\nThese models are now central to applications in business, science, healthcare, and government."
  },
  {
    "objectID": "M01/M01_LN01.html#foundation-models",
    "href": "M01/M01_LN01.html#foundation-models",
    "title": "M01: Lecture Notes",
    "section": "9.5 foundation Models",
    "text": "9.5 foundation Models\nFoundation models are large neural networks trained on diverse, broad datasets so they can serve as general-purpose systems that perform many tasks. They are not built to solve a single narrowly defined problem. Instead, they learn representations of the world that can be adapted to countless tasks, including ones they have never seen before.\nKey characteristics of foundation models include:\n\nMassive scale in parameters\nMulti-domain learning (technical, literary, conversational, scientific text)\nGeneralization across tasks, languages, and writing styles\nEmergent abilities that arise as models grow in size\nFlexibility through prompting, tuning, and retrieval\n\nFoundation models represent a new computational abstraction: systems that understand and generate knowledge in natural language."
  },
  {
    "objectID": "M01/M01_LN01.html#how-generative-models-work",
    "href": "M01/M01_LN01.html#how-generative-models-work",
    "title": "M01: Lecture Notes",
    "section": "9.6 How Generative Models Work",
    "text": "9.6 How Generative Models Work\nAlthough generative AI appears magical, its mechanisms are grounded in mathematical principles.\n\n9.6.1 Representing Data as Distributions\nTraditional machine learning estimates the probability of a label given some features (p(y|x)). Generative models instead estimate the joint distribution of inputs and outputs - the probability of both occurring together (p(x, y)). This allows the model to generate new x-values (new examples) that are consistent with the learned distribution.\n\n\n9.6.2 Latent Space and Meaning\nMany generative models map inputs into a “latent space,” a compressed representation that captures the essence of the data. For language models, latent vectors represent:\n\nmeaning\ntone\nrelationships\ncontext\nintent\n\nFor images, latent vectors represent:\n\nshapes\ntextures\nlighting\ngeometry\n\nThe model navigates this latent space to synthesize new outputs.\n\n\n9.6.3 6.3 Self-Attention and Contextual Understanding\nTransformers use self-attention to determine how each token (word or subword) relates to every other token. This process allows the model to maintain coherence, resolve references, and follow long, complex instructions.\n\n\n9.6.4 6.4 Next-Token Prediction\nAt its core, a generative language model predicts the most likely next token given all previous tokens. Despite its simplicity, the depth of training enables it to produce:\n\nessays\ncode\nanalysis\nrecommended actions\nexplanations\nsummaries\n\nComplex reasoning emerges because next-token prediction is stacked across billions of parameters and trillions of training tokens."
  },
  {
    "objectID": "M01/M01_LN01.html#types-of-generative-models-expanded",
    "href": "M01/M01_LN01.html#types-of-generative-models-expanded",
    "title": "M01: Lecture Notes",
    "section": "9.7 Types of Generative Models (Expanded)",
    "text": "9.7 Types of Generative Models (Expanded)\n\n9.7.1 Diffusion Models\nDiffusion models generate data by simulating the process of gradually adding noise to an image and then learning to reverse the process. They produce highly detailed and coherent images and are widely used for AI art and simulation.\n\n\n9.7.2 Generative Adversarial Networks (GANs)\nGANs remain influential for producing realistic imagery, enhancing video, and generating synthetic datasets.\n\n\n9.7.3 Variational Autoencoders (VAEs)\nVAEs are prized in scientific domains where interpretability and structured latent representations are crucial.\n\n\n9.7.4 Transformer-Based Generative Models\nTransformers have become the dominant architecture for:\n\ntext generation\nmultimodal generation\ncode synthesis\ndocument understanding\ndata reasoning\n\nThey are the backbone of today’s most powerful generative AI systems."
  },
  {
    "objectID": "M01/M01_LN01.html#applications-of-generative-ai-across-domains",
    "href": "M01/M01_LN01.html#applications-of-generative-ai-across-domains",
    "title": "M01: Lecture Notes",
    "section": "9.8 Applications of Generative AI Across Domains",
    "text": "9.8 Applications of Generative AI Across Domains\n\n9.8.1 Scientific Research and Discovery\nGenerative models accelerate:\n\nmolecular design\nprotein engineering\nscientific hypothesis generation\nliterature summarization\ncomplex simulations\n\n\n\n9.8.2 Healthcare and Life Sciences\nApplications include:\n\ngenerating synthetic patient data\nassisting clinical documentation\nanalyzing biological sequences\nsupporting drug discovery workflows\n\n\n\n9.8.3 Engineering, Automotive, and Manufacturing\nGenerative AI supports:\n\ncreating optimized part geometries\nsimulating rare scenarios\npredicting outcomes of design changes\ngenerating synthetic training data for vision models\n\n\n\n9.8.4 Media, Arts, and Entertainment\nGenerative tools:\n\naccelerate scriptwriting and storyboarding\nassist with animation, music, and sound design\nenable personalized storytelling and world-building\n\n\n\n9.8.5 Business Operations and Customer Experience\nGenerative AI enhances:\n\ncustomer support\ndocument summarization\ninternal knowledge search\ntraining, onboarding, and reporting\nstatistical analysis and planning\n\n\n\n9.8.6 Energy and Infrastructure\nModels support:\n\npredictive insights\nanomaly detection\ndocumentation generation\nsystem modeling and optimization"
  },
  {
    "objectID": "M01/M01_LN01.html#benefits-of-generative-ai",
    "href": "M01/M01_LN01.html#benefits-of-generative-ai",
    "title": "M01: Lecture Notes",
    "section": "9.9 Benefits of Generative AI",
    "text": "9.9 Benefits of Generative AI\n\n9.9.1 Acceleration of Knowledge Work\nGenerative AI reduces cognitive load by:\n\nreorganizing information\nsummarizing long documents\ndrafting initial analyses\ntransforming data into narrative insights\n\n\n\n9.9.2 Enhanced Creativity\nBy generating diverse alternatives and variations, models expand the creative space available to humans.\n\n\n9.9.3 Operational Efficiency\nGenerative AI automates repetitive writing, documentation, reporting, and analysis tasks.\n\n\n9.9.4 Personalization\nModels can refine content for specific audiences, goals, tones, or domains.\n\n\n9.9.5 Synthetic Data Generation\nGenerative models provide high-quality synthetic datasets that support training, testing, and experimentation without risking privacy.\n\n\n9.9.6 Scalability\nGenerative workflows allow organizations to respond faster and operate at greater scale with fewer manual bottlenecks."
  },
  {
    "objectID": "M01/M01_LN01.html#limitations-and-challenges",
    "href": "M01/M01_LN01.html#limitations-and-challenges",
    "title": "M01: Lecture Notes",
    "section": "9.10 Limitations and Challenges",
    "text": "9.10 Limitations and Challenges\nDespite their capabilities, generative AI systems face important constraints:\n\n9.10.1 Hallucinations\nModels occasionally fabricate details or inconsistencies when they lack reliable context.\n\n\n9.10.2 Bias\nBiases in training data can appear in model outputs.\n\n\n9.10.3 Lack of True Understanding\nGenerative models manipulate patterns statistically; they do not possess consciousness, intent, or genuine comprehension.\n\n\n9.10.4 Explainability\nThe reasoning behind model outputs may be difficult to trace.\n\n\n9.10.5 Resource Demands\nTraining and deploying generative models require substantial computational infrastructure."
  },
  {
    "objectID": "M01/M01_LN01.html#best-practices-for-using-generative-ai",
    "href": "M01/M01_LN01.html#best-practices-for-using-generative-ai",
    "title": "M01: Lecture Notes",
    "section": "9.11 Best Practices for Using Generative AI",
    "text": "9.11 Best Practices for Using Generative AI\n\nBegin with internal workflows before deploying externally\nCommunicate clearly when AI-generated content is used\nEmploy guardrails to prevent leakage of sensitive information\nTest models extensively using a range of input scenarios\nEducate users about limitations, biases, and failure modes\nEnsure that generative outputs undergo human review in critical contexts"
  },
  {
    "objectID": "M01/M01_LN01.html#learning-pathways-for-generative-ai",
    "href": "M01/M01_LN01.html#learning-pathways-for-generative-ai",
    "title": "M01: Lecture Notes",
    "section": "9.12 Learning Pathways for Generative AI",
    "text": "9.12 Learning Pathways for Generative AI\nBeginners benefit from understanding:\n\nmachine learning foundations\nneural networks and attention mechanisms\nPython and model experimentation\nprompt engineering and evaluation\nembedding systems and vector search\nethics, governance, and responsible use\n\nThis foundation supports deeper study in model tuning, RAG systems, and applied generative workflows."
  },
  {
    "objectID": "M01/M01_P01.html#course-welcome",
    "href": "M01/M01_P01.html#course-welcome",
    "title": "AD698 - Applied Generative AI",
    "section": "Course Welcome",
    "text": "Course Welcome\n\nUnderstand the breadth of generative AI technologies\nExplore core generative model architectures\nGain insights into practical applications"
  },
  {
    "objectID": "M01/M01_P01.html#what-is-generative-ai",
    "href": "M01/M01_P01.html#what-is-generative-ai",
    "title": "AD698 - Applied Generative AI",
    "section": "What is Generative AI?",
    "text": "What is Generative AI?\n\nAI systems that can create new content\n\nGenerative AI can be thought of as a machine-learning model that is trained to create new data, rather than making a prediction about a specific dataset. A generative AI system is one that learns to generate more objects that look like the data it was trained on (Zewe 2023)."
  },
  {
    "objectID": "M01/M01_P01.html#what-is-generative-ai-1",
    "href": "M01/M01_P01.html#what-is-generative-ai-1",
    "title": "AD698 - Applied Generative AI",
    "section": "What is Generative AI?",
    "text": "What is Generative AI?\n\nCore Characteristics:\n\nLearning from existing data\nGenerating novel, contextually relevant outputs\nSpanning multiple modalities (text, image, code, audio)"
  },
  {
    "objectID": "M01/M01_P01.html#generative-model-landscape-key-generative-model-architectures",
    "href": "M01/M01_P01.html#generative-model-landscape-key-generative-model-architectures",
    "title": "AD698 - Applied Generative AI",
    "section": "Generative Model Landscape: Key Generative Model Architectures",
    "text": "Generative Model Landscape: Key Generative Model Architectures\n\nFoundation Models and Algorithms\n\nConvolutional Neural Networks (CNNs)\nRecurrent Neural Networks (RNNs)\nTransformer Architecture\nGenerative Adversarial Networks (GANs)\n\nLanguage Models\n\nGPT (Generative Pre-trained Transformer)\nBERT (Bidirectional Encoder Representations)\n\nImage Generation Models\n\nDALL-E\nStable Diffusion\nMidjourney\n\nMultimodal Models\n\nAmazon Nova\nMeta Llama\nGemini\nGPT"
  },
  {
    "objectID": "M01/M01_P01.html#nlp-prior-to-embeddings-and-transformers",
    "href": "M01/M01_P01.html#nlp-prior-to-embeddings-and-transformers",
    "title": "AD698 - Applied Generative AI",
    "section": "NLP prior to embeddings and transformers",
    "text": "NLP prior to embeddings and transformers\n\nPre-Embedding NLP Representations: Bag of Words\nDiscrete, non-contextual representation of text\nTreats documents as unordered collections of words\nLoses semantic meaning and word order\nHigh dimensionality with sparse vectors\nNo understanding of word relationships"
  },
  {
    "objectID": "M01/M01_P01.html#origins-of-modern-embeddings",
    "href": "M01/M01_P01.html#origins-of-modern-embeddings",
    "title": "AD698 - Applied Generative AI",
    "section": "Origins of Modern Embeddings",
    "text": "Origins of Modern Embeddings\n\nWord2Vec paper Paper, Explainer video for word embeddings\nBreakthrough in sequence-to-sequence learning\nReplaced previous RNN and LSTM architectures\nEnabled dense, contextual word representations\nCaptured semantic relationships between words"
  },
  {
    "objectID": "M01/M01_P01.html#core-components-of-transformer-architecture",
    "href": "M01/M01_P01.html#core-components-of-transformer-architecture",
    "title": "AD698 - Applied Generative AI",
    "section": "Core Components of Transformer Architecture",
    "text": "Core Components of Transformer Architecture\n\nIntroduced in “Attention Is All You Need” (Google, 2017). Paper, Explainer video and also this video\nMUST READ: The Illustrated Transformer\nKey Building Blocks\nSelf-Attention Mechanism\n\n\nAllows model to weigh importance of different parts of input\nCaptures contextual relationships dynamically\nEnables parallel processing of entire sequences\n\n\nPositional Encoding\n\n\nAdds location information to input embeddings\nCrucial for understanding sequence order\nEnables models to understand context beyond word positioning"
  },
  {
    "objectID": "M01/M01_P01.html#detailed-self-attention-mechanism",
    "href": "M01/M01_P01.html#detailed-self-attention-mechanism",
    "title": "AD698 - Applied Generative AI",
    "section": "Detailed Self-Attention Mechanism",
    "text": "Detailed Self-Attention Mechanism"
  },
  {
    "objectID": "M01/M01_P01.html#how-self-airieniion-works",
    "href": "M01/M01_P01.html#how-self-airieniion-works",
    "title": "AD698 - Applied Generative AI",
    "section": "How Self-Airieniion Works",
    "text": "How Self-Airieniion Works\n\nKey Components:\nQuery (Q)\nKey (K)\nValue (V)\nAttention Calculation:\n\n\nGenerate Q, K, V matrices\nCompute attention scores\nApply softmax\nCreate weighted representation"
  },
  {
    "objectID": "M01/M01_P01.html#transformer-architecture-visualization",
    "href": "M01/M01_P01.html#transformer-architecture-visualization",
    "title": "AD698 - Applied Generative AI",
    "section": "Transformer Architecture Visualization",
    "text": "Transformer Architecture Visualization"
  },
  {
    "objectID": "M01/M01_P01.html#encoder-decoder-siruciure",
    "href": "M01/M01_P01.html#encoder-decoder-siruciure",
    "title": "AD698 - Applied Generative AI",
    "section": "Encoder-Decoder Siruciure",
    "text": "Encoder-Decoder Siruciure\n\nEncoder\nProcesses input sequence\nGenerates contextual representations\nDecoder\nGenerates output sequence\nUses encoder representations\nMulti-Head Attention\nMultiple attention mechanisms in parallel\nCaptures different types of dependencies"
  },
  {
    "objectID": "M01/M01_P01.html#transformer-advantages",
    "href": "M01/M01_P01.html#transformer-advantages",
    "title": "AD698 - Applied Generative AI",
    "section": "Transformer Advantages",
    "text": "Transformer Advantages"
  },
  {
    "objectID": "M01/M01_P01.html#why-transformers-changed-everything",
    "href": "M01/M01_P01.html#why-transformers-changed-everything",
    "title": "AD698 - Applied Generative AI",
    "section": "Why Transformers Changed Everything",
    "text": "Why Transformers Changed Everything\n\nParallel Processing\nUnlike RNNs, can process entire sequences simultaneously\nLong-Range Dependencies\nEffectively capture distant contextual relationships\nScalability\nEasily parallelizable\nSupports massive model architectures"
  },
  {
    "objectID": "M01/M01_P01.html#limitations-and-challenges",
    "href": "M01/M01_P01.html#limitations-and-challenges",
    "title": "AD698 - Applied Generative AI",
    "section": "Limitations and Challenges",
    "text": "Limitations and Challenges"
  },
  {
    "objectID": "M01/M01_P01.html#transformer-architecture-considerations",
    "href": "M01/M01_P01.html#transformer-architecture-considerations",
    "title": "AD698 - Applied Generative AI",
    "section": "Transformer Architecture Considerations",
    "text": "Transformer Architecture Considerations\n\nComputational Complexity\nQuadratic complexity with sequence length\nMemory Requirements\nLarge models need significant computational resources\nPotential Mitigation Strategies\nSparse Attention\nEfficient Transformer variants\nModel distillation techniques"
  },
  {
    "objectID": "M01/M01_P01.html#practical-implications",
    "href": "M01/M01_P01.html#practical-implications",
    "title": "AD698 - Applied Generative AI",
    "section": "Practical Implications",
    "text": "Practical Implications"
  },
  {
    "objectID": "M01/M01_P01.html#transformers-in-real-world-applications",
    "href": "M01/M01_P01.html#transformers-in-real-world-applications",
    "title": "AD698 - Applied Generative AI",
    "section": "Transformers in Real-World Applications",
    "text": "Transformers in Real-World Applications\n\nNatural Language Processing\nMachine Translation\nCode Generation\nMultimodal AI Systems\nConversational AI"
  },
  {
    "objectID": "M01/M01_P01.html#references-and-deep-dive-resources",
    "href": "M01/M01_P01.html#references-and-deep-dive-resources",
    "title": "AD698 - Applied Generative AI",
    "section": "References and Deep Dive Resources",
    "text": "References and Deep Dive Resources\nRecommended Learning Materials\n\nFoundational Papers\n\n\n“Efficient Estimation of Word Representations in Vector Space” (Mikolov et al., 2013)\nWord2Vec: Pioneering word embedding techniques\n“Attention Is All You Need” (Vaswani et al., 2017)\nOriginal Transformer architecture paper\n\n\nPractical Implementation Resources\n\n\nKarpathy’s nanoGPT\nGitHub: https://github.com/karpathy/nanoGPT\nMinimalist GPT implementation\nEducational reference for transformer internals\n\n\nVideo Explanations\n\n\nAndrej Karpathy’s “LLMs in a Hurry”\nvideo: Comprehensive overview of LLM internals\n3Blue1Brown Transformer Visualization\nvideo: Intuitive mathematical explanation\n\n\nOnline Resources\n\n\nHugging Face Transformer Documentation\nJay Alammar’s “Illustrated Transformer”"
  },
  {
    "objectID": "M01/M01_P01.html#next-week-preview",
    "href": "M01/M01_P01.html#next-week-preview",
    "title": "AD698 - Applied Generative AI",
    "section": "Next Week Preview",
    "text": "Next Week Preview"
  },
  {
    "objectID": "M01/M01_P01.html#week-2-focus-in-context-learning-icl",
    "href": "M01/M01_P01.html#week-2-focus-in-context-learning-icl",
    "title": "AD698 - Applied Generative AI",
    "section": "Week 2 Focus: In-Context’ Learning (ICL)",
    "text": "Week 2 Focus: In-Context’ Learning (ICL)\n\nFew-shot learning mechanisms\nPractical ICL implementation\nAdvanced prompt engineering techniques\n\n\n\n\n\nZewe, A. 2023. “Explained: Generative AI,” MIT News. (https://news.mit.edu/2023/explained-generative-ai-1109)."
  },
  {
    "objectID": "M02/M02_A.html",
    "href": "M02/M02_A.html",
    "title": "M02 Assignment",
    "section": "",
    "text": "1 M02 Assignment"
  },
  {
    "objectID": "M03/M03_A.html",
    "href": "M03/M03_A.html",
    "title": "M03 Assignment",
    "section": "",
    "text": "1 M03 Assignment"
  },
  {
    "objectID": "M05/M05_A.html",
    "href": "M05/M05_A.html",
    "title": "M05 Assignment",
    "section": "",
    "text": "1 M05 Assignment"
  },
  {
    "objectID": "M06/M06_A.html",
    "href": "M06/M06_A.html",
    "title": "M06 Assignment",
    "section": "",
    "text": "1 M06 Assignment"
  },
  {
    "objectID": "M1/M1_presentation01.html#course-welcome",
    "href": "M1/M1_presentation01.html#course-welcome",
    "title": "Name of the Course",
    "section": "Course Welcome",
    "text": "Course Welcome\n\nKey Objectives:\n\nUnderstand the breadth of generative AI technologies\nExplore core generative model architectures\nGain insights into practical applications"
  },
  {
    "objectID": "M1/M1_presentation01.html#what-is-generative-ai",
    "href": "M1/M1_presentation01.html#what-is-generative-ai",
    "title": "Name of the Course",
    "section": "What is Generative AI?",
    "text": "What is Generative AI?\n\nDefinition: AI systems that can create new content\n\nGenerative Al can be thought of as a machine-learning model that is trained to create new data, rather than making a prediction about a specific dataset. A generative AI system is one that learns to generate more objects that look like the data it was trained on.\nSource: https://news.mit.edu/2023/explained-generative-ai-7109"
  },
  {
    "objectID": "M1/M1_presentation01.html#what-is-generative-al-contd.",
    "href": "M1/M1_presentation01.html#what-is-generative-al-contd.",
    "title": "Name of the Course",
    "section": "What is Generative Al (contd.)?",
    "text": "What is Generative Al (contd.)?\n\nCore Characteristics:\nLearning from existing data\nGenerating novel, contextually relevant outputs\nSpanning multiple modalities (text, image, code, audio)"
  },
  {
    "objectID": "M1/M1_presentation01.html#generative-model-landscape-key-generative-model-architectures",
    "href": "M1/M1_presentation01.html#generative-model-landscape-key-generative-model-architectures",
    "title": "Name of the Course",
    "section": "Generative Model Landscape: Key Generative Model Architectures",
    "text": "Generative Model Landscape: Key Generative Model Architectures\n\nLanguage Models\n\n\nGPT (Generative Pre-trained Transformer)\nBERT (Bidirectional Encoder Representations)\n\n\nImage Generation Models\n\n\nDALL-E\nStable Diffusion\nMidjourney\n\n\nMultimodal Models\n\n\nAmazon Nova\nMeta Llama 3.2 and 3.3\nGemini\nGPT-4\n\nTransformer Architecture - Fundamental Concepts"
  },
  {
    "objectID": "M1/M1_presentation01.html#nlp-prior-to-embeddings-and-transformers",
    "href": "M1/M1_presentation01.html#nlp-prior-to-embeddings-and-transformers",
    "title": "Name of the Course",
    "section": "NLP prior to embeddings and transformers",
    "text": "NLP prior to embeddings and transformers\n\nPre-Embedding NLP Representations: Bag of Words\nDiscrete, non-contextual representation of text\nTreats documents as unordered collections of words\nLoses semantic meaning and word order\nHigh dimensionality with sparse vectors\nNo understanding of word relationships"
  },
  {
    "objectID": "M1/M1_presentation01.html#origins-of-modern-embeddings",
    "href": "M1/M1_presentation01.html#origins-of-modern-embeddings",
    "title": "Name of the Course",
    "section": "Origins of Modern Embeddings",
    "text": "Origins of Modern Embeddings\n\nWord2Vec paper Paper, Explainer video for word embeddings\nBreakthrough in sequence-to-sequence learning\nReplaced previous RNN and LSTM architectures\nEnabled dense, contextual word representations\nCaptured semantic relationships between words"
  },
  {
    "objectID": "M1/M1_presentation01.html#core-components-of-transformer-architecture",
    "href": "M1/M1_presentation01.html#core-components-of-transformer-architecture",
    "title": "Name of the Course",
    "section": "Core Components of Transformer Architecture",
    "text": "Core Components of Transformer Architecture\n\nIntroduced in “Attention Is All You Need” (Google, 2017). Paper, Explainer video and also this video\nMUST READ: The Illustrated Transformer\nKey Building Blocks\nSelf-Attention Mechanism\n\n\nAllows model to weigh importance of different parts of input\nCaptures contextual relationships dynamically\nEnables parallel processing of entire sequences\n\n\nPositional Encoding\n\n\nAdds location information to input embeddings\nCrucial for understanding sequence order\nEnables models to understand context beyond word positioning"
  },
  {
    "objectID": "M1/M1_presentation01.html#detailed-self-attention-mechanism",
    "href": "M1/M1_presentation01.html#detailed-self-attention-mechanism",
    "title": "Name of the Course",
    "section": "Detailed Self-Attention Mechanism",
    "text": "Detailed Self-Attention Mechanism"
  },
  {
    "objectID": "M1/M1_presentation01.html#how-self-aitenion-works",
    "href": "M1/M1_presentation01.html#how-self-aitenion-works",
    "title": "Name of the Course",
    "section": "How Self-Aitenion Works",
    "text": "How Self-Aitenion Works\n\nKey Components:\nQuery (Q)\nKey (K)\nValue (V)\nAttention Calculation:\n\n\nGenerate Q, K, V matrices\nCompute attention scores\nApply softmax\nCreate weighted representation"
  },
  {
    "objectID": "M1/M1_presentation01.html#transformer-architecture-visualization",
    "href": "M1/M1_presentation01.html#transformer-architecture-visualization",
    "title": "Name of the Course",
    "section": "Transformer Architecture Visualization",
    "text": "Transformer Architecture Visualization"
  },
  {
    "objectID": "M1/M1_presentation01.html#encoder-decoder-siruciure",
    "href": "M1/M1_presentation01.html#encoder-decoder-siruciure",
    "title": "Name of the Course",
    "section": "Encoder-Decoder Siruciure",
    "text": "Encoder-Decoder Siruciure\n\nEncoder\nProcesses input sequence\nGenerates contextual representations\nDecoder\nGenerates output sequence\nUses encoder representations\nMulti-Head Attention\nMultiple attention mechanisms in parallel\nCaptures different types of dependencies"
  },
  {
    "objectID": "M1/M1_presentation01.html#transformer-advantages",
    "href": "M1/M1_presentation01.html#transformer-advantages",
    "title": "Name of the Course",
    "section": "Transformer Advantages",
    "text": "Transformer Advantages"
  },
  {
    "objectID": "M1/M1_presentation01.html#why-transformers-changed-everything",
    "href": "M1/M1_presentation01.html#why-transformers-changed-everything",
    "title": "Name of the Course",
    "section": "Why Transformers Changed Everything",
    "text": "Why Transformers Changed Everything\n\nParallel Processing\nUnlike RNNs, can process entire sequences simultaneously\nLong-Range Dependencies\nEffectively capture distant contextual relationships\nScalability\nEasily parallelizable\nSupports massive model architectures"
  },
  {
    "objectID": "M1/M1_presentation01.html#limitations-and-challenges",
    "href": "M1/M1_presentation01.html#limitations-and-challenges",
    "title": "Name of the Course",
    "section": "Limitations and Challenges",
    "text": "Limitations and Challenges"
  },
  {
    "objectID": "M1/M1_presentation01.html#transformer-architecture-considerations",
    "href": "M1/M1_presentation01.html#transformer-architecture-considerations",
    "title": "Name of the Course",
    "section": "Transformer Architecture Considerations",
    "text": "Transformer Architecture Considerations\n\nComputational Complexity\nQuadratic complexity with sequence length\nMemory Requirements\nLarge models need significant computational resources\nPotential Mitigation Strategies\nSparse Attention\nEfficient Transformer variants\nModel distillation techniques"
  },
  {
    "objectID": "M1/M1_presentation01.html#practical-implications",
    "href": "M1/M1_presentation01.html#practical-implications",
    "title": "Name of the Course",
    "section": "Practical Implications",
    "text": "Practical Implications"
  },
  {
    "objectID": "M1/M1_presentation01.html#transformers-in-real-world-applications",
    "href": "M1/M1_presentation01.html#transformers-in-real-world-applications",
    "title": "Name of the Course",
    "section": "Transformers in Real-World Applications",
    "text": "Transformers in Real-World Applications\n\nNatural Language Processing\nMachine Translation\nCode Generation\nMultimodal Al Systems\nConversational Al"
  },
  {
    "objectID": "M1/M1_presentation01.html#references-and-deep-dive-resources",
    "href": "M1/M1_presentation01.html#references-and-deep-dive-resources",
    "title": "Name of the Course",
    "section": "References and Deep Dive Resources",
    "text": "References and Deep Dive Resources"
  },
  {
    "objectID": "M1/M1_presentation01.html#recommended-learning-materials",
    "href": "M1/M1_presentation01.html#recommended-learning-materials",
    "title": "Name of the Course",
    "section": "Recommended Learning Materials",
    "text": "Recommended Learning Materials\n\nFoundational Papers\n\n\n“Efficient Estimation of Word Representations in Vector Space” (Mikolov et al., 2013)\nWord2Vec: Pioneering word embedding techniques\n“Attention Is All You Need” (Vaswani et al., 2017)\nOriginal Transformer architecture paper\n\n\nPractical Implementation Resources\n\n\nKarpathy’s nanoGPT\nGitHub: https://github.com/karpathy/nanoGPT\nMinimalist GPT implementation\nEducational reference for transformer internals\n\n\nVideo Explanations\n\n\nAndrej Karpathy’s “LLMs in a Hurry”\nvideo: Comprehensive overview of LLM internals\n3BluelBrown Transformer Visualization\nvideo: Intuitive mathematical explanation\n\n\nOnline Resources\n\n\nHugging Face Transformer Documentation\nJay Alammar’s “Illustrated Transformer”"
  },
  {
    "objectID": "M1/M1_presentation01.html#next-week-preview",
    "href": "M1/M1_presentation01.html#next-week-preview",
    "title": "Name of the Course",
    "section": "Next Week Preview",
    "text": "Next Week Preview"
  },
  {
    "objectID": "M1/M1_presentation01.html#week-2-focus-in-coniexi-learning-icl",
    "href": "M1/M1_presentation01.html#week-2-focus-in-coniexi-learning-icl",
    "title": "Name of the Course",
    "section": "Week 2 Focus: In-Coniexi Learning (ICL)",
    "text": "Week 2 Focus: In-Coniexi Learning (ICL)\n\nFew-shot learning mechanisms\nPractical ICL implementation\nAdvanced prompt engineering techniques"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AD 698 Applied Generative AI Business Analytics",
    "section": "",
    "text": "Section\nTitle\nInstructor\nDay\nTime\n\n\n\n\nA1\nGenerative Artificial Intelligence for Business Analytics\nNakul Padalkar\nMon\n14:30:00\n\n\nO1\nGenerative Artificial Intelligence for Business Analytics\nNakul Padalkar\nTue\n7:00PM"
  },
  {
    "objectID": "index.html#course-sections",
    "href": "index.html#course-sections",
    "title": "AD 698 Applied Generative AI Business Analytics",
    "section": "",
    "text": "Section\nTitle\nInstructor\nDay\nTime\n\n\n\n\nA1\nGenerative Artificial Intelligence for Business Analytics\nNakul Padalkar\nMon\n14:30:00\n\n\nO1\nGenerative Artificial Intelligence for Business Analytics\nNakul Padalkar\nTue\n7:00PM"
  },
  {
    "objectID": "index.html#course-content-and-objectives",
    "href": "index.html#course-content-and-objectives",
    "title": "AD 698 Applied Generative AI Business Analytics",
    "section": "2 Course Content and Objectives",
    "text": "2 Course Content and Objectives\n\n2.1 Course Description\nThis course is designed for analytics developers and advanced business students seeking to build practical, production-ready applications using Generative AI (GenAI). Emphasizing hands-on learning, students will explore key techniques such as prompt engineering, in-context learning (ICL), retrieval-augmented generation (RAG), AI agents, and responsible AI design.\nStudents will work with open-source tools and APIs including LangChain, LlamaIndex, and embedding models, while learning to implement GenAI systems across the entire development lifecycle — from prompt design and knowledge retrieval to fine-tuning and deployment.\nThe course will emphasize practical development over theoretical model internals and will cover topics such as:\n\nModel optimization strategies (e.g., quantization, adapter-based tuning)\nInference pipelines and GenAI app deployment\nEvaluation and benchmarking of model performance\nResponsible and explainable GenAI usage in enterprise settings\n\nStudents will complete five applied assignments and a capstone project, culminating in the deployment of a custom GenAI-powered analytics application.\nThis course bridges the gap between business analytics fluency and technical AI development, equipping students to build domain-specific LLM applications that solve real-world problems."
  },
  {
    "objectID": "index.html#course-learning-objectives",
    "href": "index.html#course-learning-objectives",
    "title": "AD 698 Applied Generative AI Business Analytics",
    "section": "3 Course Learning Objectives",
    "text": "3 Course Learning Objectives\nBy the end of this course, students will be able to:\n\nDesign and implement prompt workflows to extract relevant, accurate, and useful outputs from large language models using in-context learning (ICL) strategies.\nBuild retrieval-augmented generation (RAG) systems that combine language models with structured and unstructured knowledge bases.\nDevelop autonomous AI agents that can reason, retrieve, and execute tasks across business workflows using frameworks such as LangChain and AutoGen.\nFine-tune and customize foundation models (e.g., LLaMA, Mistral) using parameter-efficient techniques like LoRA and QLoRA to adapt to business-specific domains.\nEvaluate and audit generative models for quality, risk, bias, and alignment using both manual and automated tools (e.g., OpenAI Evals, GPT-judge, Trulens).\nDeploy GenAI applications using lightweight serverless pipelines (e.g., Streamlit, FastAPI, GitHub Actions) and optimize for cost and performance.\nCritically assess ethical and governance implications of deploying GenAI in enterprise settings, including risks of hallucination, misinformation, and data leakage.\nCollaborate using modern software development workflows, including version control (Git/GitHub), cloud-based inference, and reproducible notebooks."
  },
  {
    "objectID": "index.html#course-resources",
    "href": "index.html#course-resources",
    "title": "AD 698 Applied Generative AI Business Analytics",
    "section": "4 Course Resources",
    "text": "4 Course Resources\nThere is no required textbook for this course. All core readings, technical documentation, and video tutorials will be provided on the course site (Canvas or Quarto site).\nRecommended (not required):\n\nDesigning LLM Applications with LangChain (docs.langchain.com)\nOpenAI Cookbook (cookbook.openai.com)\nThe Hitchhiker’s Guide to LLMOps (arize.com)\nGenerative AI with Python and Hugging Face Transformers (available via BU Library or Hugging Face Hub)\n\nTools & Platforms Used:\n\nJupyter, VS Code, GitHub, Python (LangChain, LlamaIndex, OpenAI, Hugging Face)\nAPIs: OpenAI, Cohere, Claude (Anthropic), Gemini (Google)\nDeployment tools: Streamlit, FastAPI, GitHub Actions\nVector DBs: FAISS, ChromaDB\nEvaluation tools: GPT-Eval, Trulens, PromptLayer\n\nAll resources are available through GitHub Education and GitHub Classrooms. Students are required to create a GitHub account using their Boston University email address in the first week."
  },
  {
    "objectID": "index.html#course-schedule",
    "href": "index.html#course-schedule",
    "title": "AD 698 Applied Generative AI Business Analytics",
    "section": "5 Course Schedule",
    "text": "5 Course Schedule\nThe class will generally run for 12 weeks with 6 modules. Module 0 will be a prep module open at the same time as Module 1. Module 7 and 8 are advanced contain materials and are optional.\n\n5.1 Class Schedule\n\n\n\n\n\n\n\nLecture\nTitle\nDescription\nOnline\nOn-Campus\n\n\n\n\nL0.1\nDev Setup: GitHub, Jupyter, VS Code, Colab\nCourse tools setup and basic version control with GitHub, notebooks, and IDEs.\n-\n-\n\n\nL0.2\nIntroduction to Neural Networks\nUnderstanding the basics of neural networks as a foundation for generative models.\n-\n-\n\n\nL1.1\nIntroduction to Course, Natural Language, and Generative AI Landscape\nOverview of generative AI, its business relevance, and natural language as a data type.\n-\n-\n\n\nL1.2\nIntroduction to Natural Language Processing\nCore NLP concepts: tokenization, POS tagging, parsing, and text pre-processing.\n-\n-\n\n\nL2.1\nPrompt Engineering Fundamentals\nLearn prompt design, zero-/few-shot techniques, and ICL with business use cases.\n-\n-\n\n\nL2.2\nTokenization, Embeddings, and Vector Semantics\nDeep dive into tokenization methods, word embeddings, and semantic search foundations.\n-\n-\n\n\nL3.1\nLLM Internals: Attention, Transformers, and Positional Encoding\nExplore transformer architecture, attention mechanisms, and context windows.\n-\n-\n\n\nL3.2\nFine-Tuning Pipelines & Data Handling\nSet up custom model pipelines and format datasets for supervised fine-tuning.\n-\n-\n\n\nL4.1\nRAG Basics: Embeddings, Chunking, Indexing\nUnderstanding retrieval-augmented generation concepts and corpus preparation.\n-\n-\n\n\nL4.2\nRAG Query Pipelines with LangChain & LlamaIndex\nBuild Q&A systems using vector DBs, LangChain chains, and document loaders.\n-\n-\n\n\nL5.1\nLoRA, QLoRA, PEFT Techniques\nLearn parameter-efficient fine-tuning methods to adapt base models.\n-\n-\n\n\nL5.2\nEmbeddings & Similarity Search\nWork with sentence embeddings for document clustering, search, and relevance.\n-\n-\n\n\nL6.1\nMemory & Context Management\nStrategies for working with limited context: caching, summaries, and memory buffers.\n-\n-\n\n\nL6.2\nMultimodal Models and Use Cases\nExplore vision-language models and multimodal business applications.\n-\n-\n\n\nL7.1\nModel Evaluation & GPT-Eval\nEvaluate generated outputs using automated tools, manual rubrics, and hallucination detection.\n-\n-\n\n\nL7.2\nResponsible AI & Bias Detection\nAssess bias, fairness, and ethics in generated outputs using toolkits and guidelines.\n-\n-\n\n\nL8.1\nIntro to Agents: ReAct, Tool Use\nOverview of agent-based systems and tool-augmented LLM workflows.\n-\n-\n\n\nL8.2\nMulti-Agent Systems (AutoGen, CrewAI)\nDesign and orchestrate multi-agent collaborative systems for enterprise tasks.\n-\n-"
  },
  {
    "objectID": "index.html#boston-university-library-information",
    "href": "index.html#boston-university-library-information",
    "title": "AD 698 Applied Generative AI Business Analytics",
    "section": "6 Boston University Library Information",
    "text": "6 Boston University Library Information\nAs Boston University students, you have full access to various articles and resources within the BU Library. From any computer, you can gain access to anything at the library that is electronically formatted. These articles are downloadable without extra charges from the BU Library by following the steps. They are easily downloadable from BU Library, e-Journals (name & password protected). You may be asked to login with your BU login name and Kerberos password.\n\nGo to BU Library main page http://www.bu.edu/library and from Library Location on the top right of the page, select Pardee Management Library.\nFrom the Pardee Management Library main page, select eJournals under Related Links (right-hand side of page).\nIn the Advanced Search window, type “Harvard business review” or “MIT Sloan Management Review” or your preferred title of a journal and click Search.\nSelect the title of the journal Online Access Available.\nOn the page that opens, click on Business Source Complete (for HBR) or ProQuest (for MIT Sloan MR).\nLogin with your BU name and Kerberos account\nClick on the icon PDF Full text.\nClick on the Download icon (in Mozilla Firefox the location of the icon is on the top right of the main screen).\nSave the file on your computer.\n\nPlease note that you are not to post attachments of the required or other readings in the water cooler or other areas of the course, as it is an infringement on copyright laws and department policy. All students have access to the library system and will need to develop research skills that include how to find articles through library systems and databases."
  },
  {
    "objectID": "index.html#blackboard-course-website",
    "href": "index.html#blackboard-course-website",
    "title": "AD 698 Applied Generative AI Business Analytics",
    "section": "7 Blackboard Course Website",
    "text": "7 Blackboard Course Website\nTo access your course website, go to http://learn.bu.edu/ and select AD688 Web Analytics for Business\n\n7.1 Software Applications & Remote Access to Virtual Labs\nIn this course, students will be using AWS Academy and a massive dataset connected through Azure cloud. The instructions can be found in the Module 0 Lecture 1 video. As part of the tuition, all BU students can use these software applications free of charge. For directions to get free remote access to our BU MET Virtual Labs, please visit\nhttps://www.bu.edu/metit/services/client-technology/virtual-lab/\n\n\n7.2 Live Classroom in Zoom\nSelect Live Classroom from the left-hand navigator of the course website and follow the instructions."
  },
  {
    "objectID": "index.html#grading-and-assessment",
    "href": "index.html#grading-and-assessment",
    "title": "AD 698 Applied Generative AI Business Analytics",
    "section": "8 Grading and Assessment",
    "text": "8 Grading and Assessment\n\n8.1 Grading Structure\nOn the course homepage, all of the material is presented by modules and each module covers two lectures. In the on-campus version of this course, each week we will have a live classroom session that will cover one lecture. In addition, students will work in teams in our virtual labs and will participate in group discussion boards. Please review the Course Calendar & Outline document to get the full schedule of this course."
  },
  {
    "objectID": "index.html#grading-breakdown",
    "href": "index.html#grading-breakdown",
    "title": "AD 698 Applied Generative AI Business Analytics",
    "section": "9 Grading Breakdown",
    "text": "9 Grading Breakdown\n\n9.1 Certification Points:\n\nAWS Academy Generative AI Foundations\nGithub + githubpages ePortfolio Creation\n\nAWS Academy Generative AI Foundations is a self-paced course that will be completed in the first 10 weeks of the course. The final deliverable is completion badge from the AWS Academy. Points are based on the AWS cloud module check grades translated to 100 points.\n\n\n9.2 Weekly Labs (On Campus Class Only):\nThe labs consist of Weekly Quizzes and Problem Solving Exercises. Each week, students will take part in a set of focused weekly in class exercise that concern the material covered during that week’s content. Participation requirements include the jupyter or quarto notebook. These assignments will be done individually. The exercises are due by Day 6 at 11:59 PM.\n\n\n9.3 Individual Assignments:\nAssignments are associated with the modules and will be submitted in the “Assignments” section accessible from the left-hand course menu. All assignment dates are given in the course Study Guide.\n\n\n9.4 Group Project:\nThe Term Project consists of one midterm check-in, a managerial report and a final presentation. These assignments strengthen your understanding of the core concepts introduced in this course and build on their knowledge acquired in the foundations of a business analytics course. The Term Project and some of the assignments require students to work with a conceptual website that you will build over the assignments.\n\nProject Milestones through GitHub\n\nGitHub setup, team charter, repo planning\nData + Corpus Engineering\nGenAI Pipeline + Insights Prototype\nModel Evaluation + Deployment Readiness\n\nManagerial report - In the form of a GitHub Website\nPresentation\nGroup Feedback\n\n\n\n9.5 Grade Points\n\n\n\n\n\n\n\nClass Activity\nCount\nPoints\nMax Points\n\n\n\n\nAWS Academy Generative AI Foundations\n1\n100\n100\n\n\nGithub+githubpages ePortfolio Creation\n1\n40\n40\n\n\nLabs*\n10\n20\n200\n\n\nAssignment\n5\n50\n250\n\n\nManagerial report with Application Demo\n1\n80\n80\n\n\nGit and git website setup\n1\n-\n-\n\n\nApi and data gathering\n1\n-\n-\n\n\nData cleaning and EDA\n1\n-\n-\n\n\nAnalytics, including full website\n1\n-\n-\n\n\nGroup Project Presentation\n1\n40\n40\n\n\nGroup Feedback\n1\n40\n40\n\n\nTotal\n-\n-\n750\n\n\n\n\n\n*=On Campus Class Only"
  },
  {
    "objectID": "index.html#class-policies",
    "href": "index.html#class-policies",
    "title": "AD 698 Applied Generative AI Business Analytics",
    "section": "10 Class Policies",
    "text": "10 Class Policies\n\n10.1 Attendance & Absences\nClass attendance is mandatory. Please inform your instructor if you know in advance that you will need to miss a class. More than two absences will influence your participation grade in this class.\nRegularly engaging in discussions on course-related topics, any cases and readings (during the class and on the Discussion Board), asking questions that lead to better understanding of a concept by the class as a whole, clarifying concepts, readiness to help peers during the consultation sessions and between classes, and sharing professional experience about course-related topics constitute superior class participation and contribute to our collective learning.\n\n\n10.2 Submission Format\nAll written contributions should follow the APA writing style, in particular, the requirements how to lay out a paper, as well as how to cite and reference correctly You can get an excellent guide to APA style here.\n\n\n10.3 Assignment Completion & Due Dates & Late Work\nStudents should submit completed assignments on the Blackboard course website. All work requests from the instructor (quizzes, assignments, contributions in the teamwork, etc.) have due dates. These are the last dates that stated material is due. This means that it is a good idea to set personal targets before then as your personal completion date to avoid difficulties. Dates are often viewed by students as the date to turn in an assignment. We view assignment due dates as the last date on which to turn in an assignment. With this warning, please note that we are not inclined to accept late work; if late work should be accepted it will be done only after considerable weighing of rationale, and with penalty.\n\n\n10.4 Academic Conduct Code\nStudents are expected to adhere to the highest standards of honesty and integrity for this course. Cheating and plagiarism will not be tolerated in any Metropolitan College course. They will result in no credit for the assignment or examination and may lead to disciplinary actions. Please take the time to review the Student Academic Code of Conduct\nThis should not be understood as a discouragement for discussing the material or your particular approach to a problem with other students in the class. On the contrary – you should share your thoughts, questions and solutions. Naturally, if you choose to work in a group, you will be expected to come up with more than one and highly original solutions rather than the same mistakes.\n\n\n10.5 Request for Accommodations\nIf you have a disability and will be requesting accommodations for this course, please inform the instructor early in the semester. Advance notice and appropriate documentation are required for accommodations."
  }
]