[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Course_Microsite",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "deliverables.html",
    "href": "deliverables.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Course_Microsite",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "M1/M1_presentation01.html#subtitle",
    "href": "M1/M1_presentation01.html#subtitle",
    "title": "Name of the Course",
    "section": "Subtitle",
    "text": "Subtitle"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course_Microsite",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "M1/M1_presentation01.html#course-welcome",
    "href": "M1/M1_presentation01.html#course-welcome",
    "title": "Name of the Course",
    "section": "Course Welcome",
    "text": "Course Welcome\n\nCourse Title: Applied Generative AI for AI Developers\nWeek 1 Focus: Generative Al Landscape and Fundamentals\nKey Objectives:\nUnderstand the breadth of generative AI technologies\nExplore core generative model architectures\nGain insights into practical applications"
  },
  {
    "objectID": "M1/M1_presentation01.html#what-is-generative-ai",
    "href": "M1/M1_presentation01.html#what-is-generative-ai",
    "title": "Name of the Course",
    "section": "What is Generative AI?",
    "text": "What is Generative AI?\n\nDefinition: Al systems that can create new content\n\nGenerative Al can be thought of as a machine-learning model that is trained to create new data, rather than making a prediction about a specific dataset. A generative AI system is one that learns to generate more objects that look like the data it was trained on.\nSource: https://news.mit.edu/2023/explained-generative-ai-7109"
  },
  {
    "objectID": "M1/M1_presentation01.html#what-is-generative-al-contd.",
    "href": "M1/M1_presentation01.html#what-is-generative-al-contd.",
    "title": "Name of the Course",
    "section": "What is Generative Al (contd.)?",
    "text": "What is Generative Al (contd.)?\n\nCore Characteristics:\nLearning from existing data\nGenerating novel, contextually relevant outputs\nSpanning multiple modalities (text, image, code, audio)"
  },
  {
    "objectID": "M1/M1_presentation01.html#generative-model-landscape-key-generative-model-architectures",
    "href": "M1/M1_presentation01.html#generative-model-landscape-key-generative-model-architectures",
    "title": "Name of the Course",
    "section": "Generative Model Landscape: Key Generative Model Architectures",
    "text": "Generative Model Landscape: Key Generative Model Architectures\n\nLanguage Models\n\n\nGPT (Generative Pre-trained Transformer)\nBERT (Bidirectional Encoder Representations)\n\n\nImage Generation Models\n\n\nDALL-E\nStable Diffusion\nMidjourney\n\n\nMultimodal Models\n\n\nAmazon Nova\nMeta Llama 3.2 and 3.3\nGemini\nGPT-4\n\nTransformer Architecture - Fundamental Concepts"
  },
  {
    "objectID": "M1/M1_presentation01.html#nlp-prior-to-embeddings-and-transformers",
    "href": "M1/M1_presentation01.html#nlp-prior-to-embeddings-and-transformers",
    "title": "Name of the Course",
    "section": "NLP prior to embeddings and transformers",
    "text": "NLP prior to embeddings and transformers\n\nPre-Embedding NLP Representations: Bag of Words\nDiscrete, non-contextual representation of text\nTreats documents as unordered collections of words\nLoses semantic meaning and word order\nHigh dimensionality with sparse vectors\nNo understanding of word relationships"
  },
  {
    "objectID": "M1/M1_presentation01.html#origins-of-modern-embeddings",
    "href": "M1/M1_presentation01.html#origins-of-modern-embeddings",
    "title": "Name of the Course",
    "section": "Origins of Modern Embeddings",
    "text": "Origins of Modern Embeddings\n\nWord2Vec paper Paper, Explainer video for word embeddings\nBreakthrough in sequence-to-sequence learning\nReplaced previous RNN and LSTM architectures\nEnabled dense, contextual word representations\nCaptured semantic relationships between words"
  },
  {
    "objectID": "M1/M1_presentation01.html#core-components-of-transformer-architecture",
    "href": "M1/M1_presentation01.html#core-components-of-transformer-architecture",
    "title": "Name of the Course",
    "section": "Core Components of Transformer Architecture",
    "text": "Core Components of Transformer Architecture\n\nIntroduced in “Attention Is All You Need” (Google, 2017). Paper, Explainer video and also this video\nMUST READ: The Illustrated Transformer\nKey Building Blocks\nSelf-Attention Mechanism\n\n\nAllows model to weigh importance of different parts of input\nCaptures contextual relationships dynamically\nEnables parallel processing of entire sequences\n\n\nPositional Encoding\n\n\nAdds location information to input embeddings\nCrucial for understanding sequence order\nEnables models to understand context beyond word positioning"
  },
  {
    "objectID": "M1/M1_presentation01.html#detailed-self-attention-mechanism",
    "href": "M1/M1_presentation01.html#detailed-self-attention-mechanism",
    "title": "Name of the Course",
    "section": "Detailed Self-Attention Mechanism",
    "text": "Detailed Self-Attention Mechanism"
  },
  {
    "objectID": "M1/M1_presentation01.html#how-self-aitenion-works",
    "href": "M1/M1_presentation01.html#how-self-aitenion-works",
    "title": "Name of the Course",
    "section": "How Self-Aitenion Works",
    "text": "How Self-Aitenion Works\n\nKey Components:\nQuery (Q)\nKey (K)\nValue (V)\nAttention Calculation:\n\n\nGenerate Q, K, V matrices\nCompute attention scores\nApply softmax\nCreate weighted representation"
  },
  {
    "objectID": "M1/M1_presentation01.html#transformer-architecture-visualization",
    "href": "M1/M1_presentation01.html#transformer-architecture-visualization",
    "title": "Name of the Course",
    "section": "Transformer Architecture Visualization",
    "text": "Transformer Architecture Visualization"
  },
  {
    "objectID": "M1/M1_presentation01.html#encoder-decoder-siruciure",
    "href": "M1/M1_presentation01.html#encoder-decoder-siruciure",
    "title": "Name of the Course",
    "section": "Encoder-Decoder Siruciure",
    "text": "Encoder-Decoder Siruciure\n\nEncoder\nProcesses input sequence\nGenerates contextual representations\nDecoder\nGenerates output sequence\nUses encoder representations\nMulti-Head Attention\nMultiple attention mechanisms in parallel\nCaptures different types of dependencies"
  },
  {
    "objectID": "M1/M1_presentation01.html#transformer-advantages",
    "href": "M1/M1_presentation01.html#transformer-advantages",
    "title": "Name of the Course",
    "section": "Transformer Advantages",
    "text": "Transformer Advantages"
  },
  {
    "objectID": "M1/M1_presentation01.html#why-transformers-changed-everything",
    "href": "M1/M1_presentation01.html#why-transformers-changed-everything",
    "title": "Name of the Course",
    "section": "Why Transformers Changed Everything",
    "text": "Why Transformers Changed Everything\n\nParallel Processing\nUnlike RNNs, can process entire sequences simultaneously\nLong-Range Dependencies\nEffectively capture distant contextual relationships\nScalability\nEasily parallelizable\nSupports massive model architectures"
  },
  {
    "objectID": "M1/M1_presentation01.html#limitations-and-challenges",
    "href": "M1/M1_presentation01.html#limitations-and-challenges",
    "title": "Name of the Course",
    "section": "Limitations and Challenges",
    "text": "Limitations and Challenges"
  },
  {
    "objectID": "M1/M1_presentation01.html#transformer-architecture-considerations",
    "href": "M1/M1_presentation01.html#transformer-architecture-considerations",
    "title": "Name of the Course",
    "section": "Transformer Architecture Considerations",
    "text": "Transformer Architecture Considerations\n\nComputational Complexity\nQuadratic complexity with sequence length\nMemory Requirements\nLarge models need significant computational resources\nPotential Mitigation Strategies\nSparse Attention\nEfficient Transformer variants\nModel distillation techniques"
  },
  {
    "objectID": "M1/M1_presentation01.html#practical-implications",
    "href": "M1/M1_presentation01.html#practical-implications",
    "title": "Name of the Course",
    "section": "Practical Implications",
    "text": "Practical Implications"
  },
  {
    "objectID": "M1/M1_presentation01.html#transformers-in-real-world-applications",
    "href": "M1/M1_presentation01.html#transformers-in-real-world-applications",
    "title": "Name of the Course",
    "section": "Transformers in Real-World Applications",
    "text": "Transformers in Real-World Applications\n\nNatural Language Processing\nMachine Translation\nCode Generation\nMultimodal Al Systems\nConversational Al"
  },
  {
    "objectID": "M1/M1_presentation01.html#references-and-deep-dive-resources",
    "href": "M1/M1_presentation01.html#references-and-deep-dive-resources",
    "title": "Name of the Course",
    "section": "References and Deep Dive Resources",
    "text": "References and Deep Dive Resources"
  },
  {
    "objectID": "M1/M1_presentation01.html#recommended-learning-materials",
    "href": "M1/M1_presentation01.html#recommended-learning-materials",
    "title": "Name of the Course",
    "section": "Recommended Learning Materials",
    "text": "Recommended Learning Materials\n\nFoundational Papers\n\n\n“Efficient Estimation of Word Representations in Vector Space” (Mikolov et al., 2013)\nWord2Vec: Pioneering word embedding techniques\n“Attention Is All You Need” (Vaswani et al., 2017)\nOriginal Transformer architecture paper\n\n\nPractical Implementation Resources\n\n\nKarpathy’s nanoGPT\nGitHub: https://github.com/karpathy/nanoGPT\nMinimalist GPT implementation\nEducational reference for transformer internals\n\n\nVideo Explanations\n\n\nAndrej Karpathy’s “LLMs in a Hurry”\nvideo: Comprehensive overview of LLM internals\n3BluelBrown Transformer Visualization\nvideo: Intuitive mathematical explanation\n\n\nOnline Resources\n\n\nHugging Face Transformer Documentation\nJay Alammar’s “Illustrated Transformer”"
  },
  {
    "objectID": "M1/M1_presentation01.html#next-week-preview",
    "href": "M1/M1_presentation01.html#next-week-preview",
    "title": "Name of the Course",
    "section": "Next Week Preview",
    "text": "Next Week Preview"
  },
  {
    "objectID": "M1/M1_presentation01.html#week-2-focus-in-coniexi-learning-icl",
    "href": "M1/M1_presentation01.html#week-2-focus-in-coniexi-learning-icl",
    "title": "Name of the Course",
    "section": "Week 2 Focus: In-Coniexi Learning (ICL)",
    "text": "Week 2 Focus: In-Coniexi Learning (ICL)\n\nFew-shot learning mechanisms\nPractical ICL implementation\nAdvanced prompt engineering techniques"
  }
]