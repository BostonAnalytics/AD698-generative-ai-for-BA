{
  "hash": "56044ff6a1f6142124aa55746bdcbc3e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"AD698 - Applied Generative AI\"\nsubtitle: \"Language, Probability, and Generative Systems\"\nlogo: \"../theme/figures/met_logotype_black.png\"\ndate: 03/12/2024\ndate-modified: today\ndate-format: long\nauthor:\n  - name: Nakul R. Padalkar\n    affiliations:\n      - name: Boston University\n        city: Boston\n        state: MA\nformat: \n    revealjs:\n        theme: [../theme/presentation.scss]\n        html-math-method: katex\n        slide-number: c/t\n        toc: true\n        toc-depth: 1\n        auto-stretch: false\n        from: markdown+emoji\n        code-line-numbers: true\n    pptx:\n        reference-doc: ../theme/presentation_template.pptx\nself-contained-math: true\ncode-annotations: below\nfig-align: center\nmonofont: Roboto\ntitle-slide-attributes:\n    data-background-image: \"../theme/blank_red.png\"\n    data-background-size: 103% auto\n    data-background-opacity: \"0.95\"\nexecute: \n  echo: false\n  warning: false\n  message: false\n  freeze: auto\n  keep-ipynb: true\nbibliography: ../references.bib\ncsl: ../mis-quarterly.csl\n---\n\n# Text Analytics and Mining\n\n## Text Analytics\n\n![Text Analytics [@talib2016text]](./M01_lecture02_figures/text_mining_overview.jpg){width=80% fig-align=\"center\" #fig-text-mining-overview fig-alt=\"Text Mining Overview\"}\n\n## Text Mining Process\n\n```{dot}\ndigraph NLP_Pipeline {\n\n    /* =========================\n       Global graph settings\n       ========================= */\n\n    newrank=\"true\";\n    rankdir=\"TB\";\n    splines=\"spline\";\n\n    graph [\n        fontname=\"Helvetica\"\n        fontsize=10\n        bgcolor=\"white\"\n        ranksep=\"1.25\"\n        pad=\"0.5\"\n    ];\n\n    node [\n        shape=\"box\"\n        style=\"rounded,filled\"\n        fontname=\"Helvetica\"\n        fontsize=11\n        color=\"#1f3a8a\"\n        fillcolor=\"#e8f0ff\"\n    ];\n\n    edge [\n        fontname=\"Helvetica\"\n        fontsize=9\n        color=\"#1f3a8a\"\n    ];\n\n    /* =========================\n       TOP ROW — Lexical pipeline\n       ========================= */\n\n    subgraph cluster_lexical {\n        label=\"Lexical processing\";\n        style=\"filled,rounded\";\n        color=\"grey95\";\n        rank=same;\n\n        Characters;\n        Tokens;\n        TaggedTokens [ label=\"Tagged tokens\" ];\n\n        Characters -> Tokens;\n        Tokens -> TaggedTokens;\n    }\n\n    /* =========================\n       BOTTOM ROW — Structural pipeline\n       ========================= */\n\n    subgraph cluster_structural {\n        label=\"Structural representation\";\n        style=\"filled,rounded\";\n        color=\"grey95\";\n        rank=same;\n\n        EntityRelations [ label=\"Entity relationships\" ];\n        SyntaxTree      [ label=\"Syntax tree\" ];\n        KnowledgeBase   [ label=\"Knowledge base\" ];\n\n        /* Right-to-left logical flow */\n        SyntaxTree    -> EntityRelations;\n        EntityRelations -> KnowledgeBase;\n\n    }\n    \n    subgraph cluster_algorithm {\n        label=\"Algorithm\";\n        style=\"filled,rounded\";\n        color=\"grey95\";\n        rank=same;\n\n        fst_regex[color=\"#99CC99\" fillcolor=\"#D6EBD6\" label=\"Regular expression\" ];\n        fst_pos  [color=\"#99CC99\" fillcolor=\"#D6EBD6\" label=\"Part-of-Speech\" ];\n        fst_logic[color=\"#99CC99\" fillcolor=\"#D6EBD6\" label=\"Logic compiler\" ];\n        fst_ie   [color=\"#99CC99\" fillcolor=\"#D6EBD6\" label=\"Information extractor\" ];\n    }\n\n    /* =========================\n       Invisible merge anchors\n       ========================= */\n\n    merge_tokens [\n        shape=\"circle\"\n        width=\".25\"\n        fixedsize=\"true\"\n        label=\"\"\n        style=\"invis\"\n    ];\n\n    merge_tagged [\n        shape=\"circle\"\n        width=\".25\"\n        fixedsize=\"true\"\n        label=\"\"\n        style=\"invis\"\n    ];\n\n    merge_syntax [\n        shape=\"circle\"\n        width=\".25\"\n        fixedsize=\"true\"\n        label=\"\"\n        style=\"invis\"\n    ];\n\n    /* =========================\n       Controlled merges\n       ========================= */\n\n    Tokens        -> SyntaxTree      [ arrowhead=\"vee\"];\n    TaggedTokens -> SyntaxTree       [ arrowhead=\"vee\"];\n\n    /* =========================\n       FST annotations (semantic)\n       ========================= */\n\n    /* Invisible anchors for labels */\n    // fst_regex  [ shape=\"circle\" width=\".25\" fixedsize=\"true\" color=\"invis\" label=\"\" ];\n    // fst_pos    [ shape=\"circle\" width=\".25\" fixedsize=\"true\" color=\"invis\" label=\"\" ];\n    // fst_logic  [ shape=\"circle\" width=\".25\" fixedsize=\"true\" color=\"invis\" label=\"\" ];\n    // fst_ie     [ shape=\"circle\" width=\".25\" fixedsize=\"true\" color=\"invis\" label=\"\" ];\n\n    fst_regex -> Tokens       [ style=\"dashed\" arrowhead=\"vee\" constraint=\"false\" color=\"#99CC99\" penwidth=2.0];\n    fst_regex -> TaggedTokens [ style=\"dashed\" arrowhead=\"vee\" constraint=\"false\" color=\"#99CC99\" penwidth=2.0];\n    SyntaxTree   -> fst_pos [ style=\"dashed\" arrowhead=\"vee\" constraint=\"false\" color=\"#99CC99\" penwidth=2.0];\n    KnowledgeBase-> fst_ie    [ style=\"dashed\" arrowhead=\"vee\" constraint=\"false\" color=\"#99CC99\" penwidth=2.0];\n    KnowledgeBase-> fst_logic [ style=\"dashed\" arrowhead=\"vee\" constraint=\"false\" color=\"#99CC99\" penwidth=2.0];\n\n    /* Labels */\n    // fst_regex  [ xlabel=\"Regular expressions\" ];\n    // fst_pos    [ xlabel=\"POS tagger (FST)\" ];\n    // fst_logic  [ xlabel=\"Logic compiler (FST)\" ];\n    // fst_ie     [ xlabel=\"Information extractor (FST)\" ];\n\n    /* =========================\n       Rank alignment\n       ========================= */\n\n    { rank=\"same\"; Characters; Tokens; TaggedTokens; }\n    { rank=\"same\"; EntityRelations; SyntaxTree; KnowledgeBase; }\n    { rank=\"same\"; fst_regex; fst_pos; fst_logic; fst_ie}\n}\n\n```\n\n## Text Analytics\n\n- Knowledge Discovery in Textual Databases\n- Text Mining is a subfield of Text Analytics and Natural Language Processing (NLP)\n- Text Analytics is a broader term that encompasses various techniques for analyzing and extracting insights from text data.\n- Borrows from various fields such as information retrieval, machine learning, statistics, linguistics, and others.\n$$\n\\begin{align}\n\\text{Text Mining} &= \\text{Information Extraction} + \\nonumber \\\\\n      &\\quad{}  \\text{Data Mining} + \\text{Web Mining} \\\\\n\\text{Text Analytics} &= \\text{Information Retrieval} + \\text{Text Mining}\n\\end{align}\n$$\n\n## Application Areas of Text Mining\n\n- Information extraction\n- Topic tracking\n- Summarization\n- Categorization\n- Clustering\n- Concept linking\n- Question answering\n\n# Text Mining and Analytics Process \n\n![NLP Pipeline](M01_lecture02_figures/NLP-Pipeline.jpeg){width=80% fig-align=\"center\" #fig-nlp-pipeline fig-alt=\"General NLP Pipeline\"}\n\n## Sentiment Analysis\n\n::: {.columns}\n::: {.column}\n![Sentiment Analysis: tasks, tools, methods, and applications](M01_lecture02_figures/Sentiment_Analysis.jpg){width=80% fig-align=\"center\" fig-alt=\"Sentiment Analysis Process\" #fig-sentiment-analysis}\n\n:::\n::: {.column}\n\n- Methods\n    - Lexicon-based\n    - Machine Learning\n    - Deep Learning\n    - Hybrid\n- Applications\n    - Domain Applications\n    - Large Language Models\n- Challenges\n    - Methodological Challenges\n    - Text Context Challenges\n:::\n:::\n\n## Sentiment Classification Algorithms\n\n\n```{dot}\ndigraph Sentiment_Approaches {\n\n    /* =========================\n       Global settings\n       ========================= */\n\n    rankdir=TD;\n    newrank=true;\n    splines=curved;\n\n    graph [\n        fontname=\"Handlee\"\n        fontsize=11\n        bgcolor=\"white\"\n        pad=\"0.0\"\n        margin=\"0\"\n        nodesep=\"0.6\"\n        ranksep=\"1.2\"\n    ];\n\n    node [\n        shape=box\n        style=\"rounded,filled\"\n        fontname=\"Handlee\"\n        fontsize=11\n        color=\"#555555\"\n    ];\n\n    edge [\n        fontname = \"Handlee\"\n        color=\"#333333\"\n        penwidth=1.6\n        arrowsize=0.8\n    ];\n\n    /* =========================\n       ROOT NODES\n       ========================= */\n\n    ML_Approach [\n        label=\"Machine Learning\\nApproach\"\n        fillcolor=\"#ffd37f\"\n        fontsize=12\n    ];\n\n    Lexicon_Approach [\n        label=\"Lexicon-based\\nApproach\"\n        fillcolor=\"#cde6a3\"\n        fontsize=12\n    ];\n\n    /* =========================\n       MACHINE LEARNING BRANCH\n       ========================= */\n\n    Supervised [\n        label=\"Supervised\\nLearning\"\n        fillcolor=\"#ffd37f\"\n    ];\n\n    Unsupervised [\n        label=\"Unsupervised\\nLearning\"\n        fillcolor=\"#ffd37f\"\n    ];\n\n    ML_Approach -> Supervised;\n    ML_Approach -> Unsupervised;\n\n    /* ---- Classifier families ---- */\n\n    DecisionTree [\n        label=\"Decision Tree\\nClassifiers\"\n        fillcolor=\"#ffd37f\"\n    ];\n\n    LinearCls [\n        label=\"Linear\\nClassifiers\"\n        fillcolor=\"#ffd37f\"\n    ];\n\n    RuleBased [\n        label=\"Rule-based\\nClassifiers\"\n        fillcolor=\"#ffd37f\"\n    ];\n\n    Probabilistic [\n        label=\"Probabilistic\\nClassifiers\"\n        fillcolor=\"#ffd37f\"\n    ];\n\n    Supervised -> DecisionTree;\n    Supervised -> LinearCls;\n    Supervised -> RuleBased;\n    Unsupervised -> Probabilistic;\n\n    /* ---- Specific algorithms ---- */\n\n    SVM [\n        label=\"Support Vector\\nMachine (SVM)\"\n        fillcolor=\"#ffb000\"\n    ];\n\n    NN [\n        label=\"Neural Network\\n(NN)\"\n        fillcolor=\"#ffb000\"\n    ];\n\n    DL [\n        label=\"Deep Learning\\n(DL)\"\n        fillcolor=\"#ffb000\"\n    ];\n\n    NB [\n        label=\"Naïve Bayes\\n(NB)\"\n        fillcolor=\"#ffb000\"\n    ];\n\n    BN [\n        label=\"Bayesian Network\\n(BN)\"\n        fillcolor=\"#ffb000\"\n    ];\n\n    ME [\n        label=\"Maximum\\nEntropy (ME)\"\n        fillcolor=\"#ffb000\"\n    ];\n\n    DecisionTree -> SVM;\n    LinearCls   -> NN;\n    LinearCls   -> DL;\n\n    Probabilistic -> NB;\n    Probabilistic -> BN;\n    Probabilistic -> ME;\n\n    /* =========================\n       LEXICON-BASED BRANCH\n       ========================= */\n\n    Dictionary [\n        label=\"Dictionary-based\\nApproach\"\n        fillcolor=\"#cde6a3\"\n    ];\n\n    Corpus [\n        label=\"Corpus-based\\nApproach\"\n        fillcolor=\"#cde6a3\"\n    ];\n\n    Lexicon_Approach -> Dictionary;\n    Lexicon_Approach -> Corpus;\n\n    Statistical [\n        label=\"Statistical\"\n        fillcolor=\"#bfe3ea\"\n    ];\n\n    Semantic [\n        label=\"Semantic\"\n        fillcolor=\"#bfe3ea\"\n    ];\n\n    Corpus -> Statistical;\n    Corpus -> Semantic;\n\n    /* =========================\n       Rank alignment\n       ========================= */\n\n    { rank=same; ML_Approach; Lexicon_Approach; }\n    { rank=same; Supervised; Unsupervised; Dictionary; Corpus; }\n    { rank=same; DecisionTree; LinearCls; RuleBased; Probabilistic; Statistical; Semantic; }\n    { rank=same; SVM; NN; DL; NB; BN; ME; }\n}\n\n```\n\n## Types of Sentiment Analysis\n\n- Aspects-based\n- Emotion-based\n- Fine-grained\n- Intent-based\n\n:::{.r-stack}\n\n![](./M01_lecture02_figures/types-of-sentiment-analysis - 1.png){width=80% fig-align=\"center\" fig-alt=\"Types of Sentiment Analysis, describes Aspects-based, Emotion-based, Fine-grained, Intent-based sentiment analysis\" .fragment .fade-out}\n\n![](./M01_lecture02_figures/types-of-sentiment-analysis - 2.png){width=80% fig-align=\"center\"  fig-alt=\"Types of Sentiment Analysis, describes Aspects-based, Emotion-based, Fine-grained, Intent-based sentiment analysis\" .fragment .fade-in}\n\n![](./M01_lecture02_figures/types-of-sentiment-analysis - 3.png){width=80% fig-align=\"center\" fig-alt=\"Types of Sentiment Analysis, describes Aspects-based, Emotion-based, Fine-grained, Intent-based sentiment analysis\" .fragment .fade-in}\n\n![](./M01_lecture02_figures/types-of-sentiment-analysis - 4.png){width=80% fig-align=\"center\" fig-alt=\"Types of Sentiment Analysis, describes Aspects-based, Emotion-based, Fine-grained, Intent-based sentiment analysis\" .fragment .fade-in}\n:::\n\n# Web Mining, personalization, Social Analytics\n\n## Web Mining \n\n- Web mining is the application of data mining techniques to discover patterns from the World Wide Web.\n\n```{dot}\ndigraph Weblog_Mining_Process {\n\n  /* =========================\n     Global styling\n     ========================= */\n  graph [\n    fontname=\"Helvetica,Arial,sans-serif\",\n    fontsize=14,\n    bgcolor=\"white\",\n    pad=\"0.2\",\n    ranksep=\"1.1\",\n    nodesep=\"0.9\",\n    splines=curved\n  ];\n\n  node [\n    shape=box,\n    style=\"rounded,filled\",\n    fontname=\"Helvetica,Arial,sans-serif\",\n    fontsize=14,\n    fillcolor=\"#f7f7f7\",\n    color=\"#333333\",\n    margin=\"0.25,0.18\"\n  ];\n\n  edge [\n    color=\"#333333\",\n    penwidth=1.4,\n    arrowsize=0.8\n  ];\n\n  /* =========================\n     Cluster 1 — Data Collection\n     ========================= */\n  subgraph cluster_collection {\n    label=\"Data Collection\";\n    style=\"filled,rounded\";\n    color=\"#e3f2fd\";\n\n    RawData     [ label=\"Raw Data\" ];\n    Collection  [ label=\"Weblog Data Collection\" ];\n\n    RawData -> Collection;\n  }\n\n  /* =========================\n     Cluster 2 — Pre-processing\n     ========================= */\n  subgraph cluster_preprocessing {\n    label=\"Pre-processing\";\n    style=\"filled,rounded\";\n    color=\"#e8f5e9\";\n\n    Integration   [ label=\"Data Integration\" ];\n    Preprocessing [ label=\"Data Pre-processing\" ];\n\n    Integration -> Preprocessing;\n  }\n\n  /* =========================\n     Cluster 3 — Pattern Discovery\n     ========================= */\n  subgraph cluster_discovery {\n    label=\"Pattern Discovery\";\n    style=\"filled,rounded\";\n    color=\"#fff3e0\";\n\n    Extraction [ label=\"Pattern Extraction\" ];\n    Analysis   [ label=\"Pattern Analysis\" ];\n\n    Extraction -> Analysis;\n  }\n\n  /* =========================\n     Cluster 4 — Output\n     ========================= */\n  subgraph cluster_output {\n    label=\"Output\";\n    style=\"filled,rounded\";\n    color=\"#fce4ec\";\n\n    Output [ label=\"Patterns Formed\" ];\n  }\n\n  /* =========================\n     Cross-cluster flow\n     ========================= */\n  Collection   -> Integration;\n  Preprocessing -> Extraction;\n  Analysis     -> Output;\n\n  /* =========================\n     Rank alignment (2 rows)\n     ========================= */\n  { rank=same; RawData; Integration; Extraction; }\n  { rank=same; Collection; Preprocessing; Analysis; Output; }\n}\n\n```\n\n## Web Content Mining (Web Scraping) {background-color=\"#eff8ff\"}\n\n![Web Scraping Process](./M01_lecture02_figures/web-scrapping.png){width=80% fig-align=\"center\" #fig-web-scrapping fig-alt=\"Web Scraping Process\"}\n\n\n## Web Usage Mining (Web Analytics)\n\n- Web usage mining (Web analytics) is the extraction of useful information from data generated through Web page visits and transactions.\n\n![](./M01_lecture02_figures/Web_Usage_Mining.png){width=80% fig-align=\"center\" fig-alt=\"Web Usage Mining Process\"}\n\n\n## Social Analytics\n\n- Social analytics is defined as monitoring, analyzing, measuring and interpreting digital interactions and relationships of people, topics, ideas and content.\n\n```{dot}\ndigraph Social_Analytics {\n\n    /* =========================\n       Global settings\n       ========================= */\n\n    rankdir=LR;\n    newrank=true;\n    splines=curved;\n\n    graph [\n        fontname=\"Helvetica\"\n        fontsize=14\n        bgcolor=\"white\"\n        pad=\"0.2\"\n        nodesep=\"1.0\"\n        ranksep=\"1.4\"\n    ];\n\n    node [\n        shape=box\n        style=\"rounded,filled\"\n        fontname=\"Helvetica\"\n        fontsize=16\n        color=\"black\"\n        fillcolor=\"#cfe1f7\"\n        margin=\"0.35,0.25\"\n    ];\n\n    edge [\n        color=\"black\"\n        penwidth=2.0\n        arrowsize=0.8\n    ];\n\n    /* =========================\n       Nodes\n       ========================= */\n\n    SocialAnalytics [\n        label=\"Social Analytics\"\n        fontsize=18\n    ];\n\n    SNA [\n        label=\"Social Network Analysis\\n(SNA)\"\n    ];\n\n    SMA [\n        label=\"Social Media Analytics\"\n    ];\n\n    /* =========================\n       Structure\n       ========================= */\n\n    SocialAnalytics -> SNA;\n    SocialAnalytics -> SMA;\n\n    /* =========================\n       Rank alignment\n       ========================= */\n\n    { rank=same; SNA; SMA; }\n}\n\n\n```\n\n\n# Text mining\n\n## Text Mining Concepts\n\n- 85-90 percent of all corporate data is in some kind of unstructured form (e.g., text)\n- Unstructured corporate data is doubling in size every 18 months\n- Tapping into these information sources is not an option, but a need to stay competitive\n\n:::{.callout-note}\n\nText mining is the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning.\n\n:::\n\n## Knowledge Discovery from Web Data\n\n![Example Data exploration workflow for textual data extraction [@Gupta2024]](./M01_lecture02_figures/knowledge_extraction.jpg){width=80% fig-align=\"center\" fig-alt=\"Knowledge Discovery from Web Data\" #fig-knowledge-discovery-web-data}\n\n\n# Natural Language Processing (NLP)\n\n## What is Natural Language Processing (NLP)?\n\n- Technology that enables computers to process, generate, and interact with language (e.g., text). Some key aspects:\n- [Learn useful representations]{.uublue-bold}: capture meaning in a structured way that can be used for downstream tasks (e.g., embeddings used to classify a document)\n- [Generate language]{.uublue-bold}: create language (e.g., text, \n- code) for tasks like dialogue, translation, or question answering.\n- [Bridge language and action]{.uublue-bold}: Use language to perform tasks, solve problems, interact with environments (e.g., a code IDE)\n\n## General NLP Framework \n\n```{dot}\n\ndigraph NLP_Tasks {\n  node [shape=plaintext]\n\n  nlp_table [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"4\">\n      <TR>\n        <TD bgcolor=\"#137dcb\"><B>Input X</B></TD>\n        <TD bgcolor=\"lightcoral\"><B>Output Y</B></TD>\n        <TD bgcolor=\"#0ccc90\"><B>Task</B></TD>\n      </TR>\n      <TR>\n        <TD>Text</TD>\n        <TD>Continuing Text</TD>\n        <TD>Language Modeling</TD>\n      </TR>\n      <TR>\n        <TD>Text</TD>\n        <TD>Text in Other Language</TD>\n        <TD>Translation</TD>\n      </TR>\n      <TR>\n        <TD>Text</TD>\n        <TD>Label</TD>\n        <TD>Text Classification</TD>\n      </TR>\n      <TR>\n        <TD>Text</TD>\n        <TD>Linguistic Structure</TD>\n        <TD>Language Analysis</TD>\n      </TR>\n      <TR>\n        <TD>Image</TD>\n        <TD>Text</TD>\n        <TD>Image Captioning</TD>\n      </TR>\n    </TABLE>\n  >]\n\n  title [label=<\n    <TABLE BORDER=\"0\" CELLBORDER=\"0\">\n      <TR><TD><B>Create a function to map an input X into an output Y, where X and/or Y involve language.</B></TD></TR>\n    </TABLE>\n  >]\n\n  title -> nlp_table\n}\n\n```\n\n## Building NLP Systems\n\n- Rules: Manual creation of rules\n\n  ```\n  def classify(x: str) -> str:\n      sports_keywords = [\"baseball\", \"soccer\", \"football\", \"tennis\"]\n      if any(keyword in x for keyword in sports_keywords):\n          return \"sports\"\n      else:\n          return \"other\"\n  ```\n\n\n- Prompting: Prompting a language model w/o training\n\n```{dot}\ndigraph PromptLogic {\n  rankdir=LR\n  node [shape=plaintext]\n\n  decision_box [label=<\n    <TABLE BORDER=\"0.50\" CELLBORDER=\"0.5\" CELLSPACING=\"0\" CELLPADDING=\"2\" bgcolor=\"lightyellow\">\n      <TR><TD width=\"400\" fixedsize=\"false\"><B>If the following sentence is about 'sports', reply 'sports'. Otherwise reply 'other'.</B></TD></TR>\n    </TABLE>\n  >]\n\n  lm_node [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"2\" bgcolor=\"lightblue\">\n      <TR><TD width=\"75\" fixedsize=\"true\"><B>LM</B></TD></TR>\n    </TABLE>\n  >]\n\n  decision_box -> lm_node\n}\n\n```\n\n## Building NLP Systems\n\n- Fine-tuning: Machine learning from paired data $\\langle X, Y\\rangle$\n\n```{dot}\ndigraph TextClassificationTraining {\n    rankdir=LR\n\n  node [shape=plaintext]\n\n  samples [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"2\">\n      <TR><TD><B>Sentence</B></TD><TD><B>Label</B></TD></TR>\n      <TR><TD>\"I love to play baseball.\"</TD><TD>sports</TD></TR>\n      <TR><TD>\"The stock price is going up.\"</TD><TD>other</TD></TR>\n      <TR><TD>\"He got a hat-trick yesterday.\"</TD><TD>sports</TD></TR>\n      <TR><TD>\"He is wearing tennis shoes.\"</TD><TD>other</TD></TR>\n    </TABLE>\n  >]\n\n  training [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"2\" bgcolor=\"lightgray\">\n      <TR><TD><B>Training</B></TD></TR>\n    </TABLE>\n  >]\n\n  model [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"2\">\n      <TR>\n        <TD><IMG SRC=\"M01_lecture02_figures/reshot-icon-engineering-6XYGVMCJ59.png\"/></TD>\n        <TD><B>Model</B></TD>\n      </TR>\n    </TABLE>\n  >]\n\n  samples -> training -> model\n}\n\n```\n\n## Data Requirements for System Building\n\n-  [Rules/prompting based on intuition]{.uublue-bold}: No data needed, but also no performance guarantees  \n- [Rules/prompting based on spot-checks]{.uublue-bold}: A small amount of data with input $X$ only\n- [Rules/prompting with rigorous evaluation]{.uublue-bold}: Development set with input $X$ and output $Y$ (e.g. 200-2000 examples). Additional held-out test set also preferable.\n- [Fine-tuning]{.uublue-bold}: Additional train set. More is often better - constant accuracy increase when data size doubles.\n\n```{dot}\ndigraph DataSplit {\n  rankdir=LR\n  node [shape=plaintext]\n\n  split_table [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"6\">\n      <TR>\n        <TD bgcolor=\"lightgreen\" width=\"378\">\n          <B>Train Data</B><BR/><FONT COLOR=\"red\">X_train, y_train</FONT><BR/>60%\n        </TD>\n        <TD bgcolor=\"lightblue\" width=\"108\">\n          <B>Test Data</B><BR/><FONT COLOR=\"red\">X_test, y_test</FONT><BR/>20%\n        </TD>\n        <TD bgcolor=\"lightyellow\" width=\"54\">\n          <B>Validation (Dev)</B><BR/><FONT COLOR=\"red\">X_val, y_val</FONT><BR/>20%\n        </TD>\n      </TR>\n    </TABLE>\n  >]\n}\n\n```\n\n## Natural Language Processing (NLP) Pipeline \n\n```{dot}\ndigraph Pipeline {\n\n  rankdir=LR;\n  splines=ortho;\n  concentrate=true;\n  nodesep=0.8;\n  ranksep=1.2;\n  fontname=\"Helvetica\";\n\n  node [\n    shape=box,\n    style=\"filled,rounded\",\n    fontname=\"Helvetica\",\n    fillcolor=\"blanchedalmond\"\n  ];\n\n  edge [\n    arrowhead=vee,\n    penwidth=1.2\n  ];\n\n  /* =========================\n     STAGE 1 — DATA SOURCES\n     ========================= */\n  subgraph cluster_data {\n    label=\"Data Sources\";\n    color=\"#f4cccc\";\n    style=filled;\n    rank=same;\n\n    ds1 [label=\"Survey Data\"];\n    ds2 [label=\"Logs\"];\n    ds3 [label=\"APIs\"];\n    ds4 [label=\"Databases\"];\n    ds5 [label=\"Files\"];\n    ds6 [label=\"External Feeds\"];\n  }\n\n  /* =========================\n     STAGE 2 — INGESTION\n     ========================= */\n  subgraph cluster_ingest {\n    label=\"Ingestion & Parsing\";\n    color=\"#cfe2f3\";\n    style=filled;\n    rank=same;\n\n    p1 [label=\"Load\"];\n    p2 [label=\"Parse\"];\n    p3 [label=\"Validate\"];\n    p4 [label=\"Normalize\"];\n    p5 [label=\"Enrich\"];\n  }\n\n  /* =========================\n     STAGE 3 — TRANSFORMATION\n     ========================= */\n  subgraph cluster_transform {\n    label=\"Transformation\";\n    color=\"#fff2cc\";\n    style=filled;\n    rank=same;\n\n    t1 [label=\"Clean\"];\n    t2 [label=\"Aggregate\"];\n    t3 [label=\"Feature Engineering\"];\n  }\n\n  /* =========================\n     STAGE 4 — DATA PRODUCTS\n     ========================= */\n  subgraph cluster_products {\n    label=\"Data Products\";\n    color=\"#d9ead3\";\n    style=filled;\n    rank=same;\n    splines=curved;\n\n\n    o1 [label=\"Structured Tables\"];\n    o2 [label=\"Feature Store\"];\n    o3 [label=\"Analytics Dataset\"];\n  }\n\n  /* =========================\n     STAGE 5 — CONSUMPTION\n     ========================= */\n  subgraph cluster_consume {\n    label=\"Consumption\";\n    color=\"#ead1dc\";\n    style=filled;\n    rank=same;\n\n    c1 [label=\"Dashboards\"];\n    c2 [label=\"ML Models\"];\n    c3 [label=\"Reports\"];\n  }\n\n  /* =========================\n     FLOWS\n     ========================= */\n  ds1 -> p1;\n  ds2 -> p2;\n  ds3 -> p2;\n  ds4 -> p2;\n  ds5 -> p3;\n  ds6 -> p4;\n\n  p1 -> t1;\n  p2 -> t1;\n  p3 -> t2;\n  p4 -> t2;\n  p5 -> t3;\n\n  t1 -> o1;\n  t2 -> o2;\n  t3 -> o3;\n\n  o1 -> c1;\n  o2 -> c2;\n  o3 -> c3;\n}\n\n```\n\n## Text Summarization\n\n```{dot}\ndigraph TextSummarization {\n\n  rankdir=TB;\n  splines=ortho;\n  nodesep=0.5;\n  ranksep=0.7;\n  fontname=\"Helvetica\";\n  \n  width=3.5;\n  height=0.9;\n  margin=\"0.35,0.25\";\n\n  node [\n    fontname=\"Helvetica\",\n    fontsize=11,\n    shape=box,\n    style=\"rounded,filled\",\n    fillcolor=\"#f5f5f5\",\n    color=\"#333333\"\n  ];\n\n  edge [\n    arrowhead=vee,\n    color=\"#333333\",\n    penwidth=1.2\n  ];\n\n  /* =========================\n     INPUTS\n     ========================= */\n\n  TextInput [\n    label=\"Text Input\",\n    shape=parallelogram,\n    fillcolor=\"#e8f3ff\"\n  ];\n\n  Dictionary [\n    label=\"Dictionary /\\nThesaurus\",\n    shape=cylinder,\n    fillcolor=\"#fff2cc\"\n  ];\n\n  /* =========================\n     CORE PIPELINE\n     ========================= */\n\n  Preprocess [\n    label=\"Pre-processing\"\n  ];\n\n  StructureAnalysis [\n    label=\"Text Structure Analysis\"\n  ];\n\n  WordSegmentation [\n    label=\"Word Segmentation\"\n  ];\n\n  OccurrenceStats [\n    label=\"Occurrence Statistics\"\n  ];\n\n  POSTagging [\n    label=\"Chinese POS Tagging\"\n  ];\n\n  KeywordExtract [\n    label=\"Keyword Extracting\"\n  ];\n\n  Weighting [\n    label=\"Weight Words & Sentences\"\n  ];\n\n  SentenceSelection [\n    label=\"Sentence Selection\"\n  ];\n\n  RoughSummary [\n    label=\"Rough Summary Generation\"\n  ];\n\n  Smoothing [\n    label=\"Smoothing\"\n  ];\n\n  SummaryOutput [\n    label=\"Summary Output\",\n    shape=parallelogram,\n    fillcolor=\"#e8f3ff\"\n  ];\n\n  /* =========================\n     MAIN FLOW\n     ========================= */\n\n  TextInput -> Preprocess;\n  Preprocess -> StructureAnalysis;\n\n  StructureAnalysis -> WordSegmentation;\n  StructureAnalysis -> OccurrenceStats;\n\n  WordSegmentation -> POSTagging;\n  POSTagging -> KeywordExtract;\n  OccurrenceStats -> KeywordExtract;\n\n  KeywordExtract -> Weighting;\n  Weighting -> SentenceSelection;\n  SentenceSelection -> RoughSummary;\n  RoughSummary -> Smoothing;\n  Smoothing -> SummaryOutput;\n\n  /* =========================\n     KNOWLEDGE BASE LINKS\n     ========================= */\n\n  Dictionary -> WordSegmentation;\n  Dictionary -> POSTagging;\n  Dictionary -> KeywordExtract;\n\n}\n\n```\n\n## Documents\n\n# Topic proportions and assignments \n\n## Seeking Life's Bare (Genetic) Necessities\n\nCold Spring Harbor, New York- \"are not all that far apart,\" especially in How many genes does an organism need to comparison to the 75,000 genes in the husurvive? Last week at the genome meeting here,* two genome researchers with radically different approaches presented complementary views of the basic genes needed for life. One research team, using computer analyses to compare known genomes, concluded that today's organisms can be sustained with just 250 genes, and that the earliest life forms required a mere 128 genes. The other researcher mapped genes in a simple parasite and estimated that for this organism, 800 genes are plenty to do the job-but that anything short of 100 wouldn't be enough.\n\nAlthough the numbers don't match precisely, those predictions\n\n* Genome Mapping and Sequencing, Cold Spring Harbor, New York, May 8 to 12.\n\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-053.jpg?height=469&width=1389&top_left_y=894&top_left_x=933)\nStripping down. Computer analysis yields an estimate of the minimum modern and ancient genomes.\n\n[^0]\n## Natural Language Processing (NLP)\n\n- Part-of-speech tagging\n- Text segmentation\n- Word sense disambiguation\n- Syntactic ambiguity\n- Imperfect or irregular input\n- Speech acts\n\n\n## NLP Tasks\n\n- Question answering\n- Automatic summarization\n- Natural language generation\n- Natural language understanding\n- Machine translation\n- Foreign language reading\n- Foreign language writing.\n- Speech recognition\n- Text-to-speech\n- Text proofing\n- Optical character recognition\n\n\n## Classical NLP\n\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-056.jpg?height=504&width=2020&top_left_y=409&top_left_x=243)\n\n## Deep Learning-based NLP\n\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-056.jpg?height=643&width=2090&top_left_y=1164&top_left_x=244)\n\n## Modern NLP Pipeline\n\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-057.jpg?height=575&width=2356&top_left_y=203&top_left_x=41)\nTask / Output\n\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-057.jpg?height=396&width=601&top_left_y=1130&top_left_x=48)\n\nTask / Output\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-057.jpg?height=691&width=1278&top_left_y=1128&top_left_x=626)\n\n## Modern NLP Pipeline\n\nTask / Output\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-058.jpg?height=675&width=2442&top_left_y=753&top_left_x=21)\n\n## Deep Learning NLP\n\nTask / Output\n\nClassification\n\nSentiment Analysis\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-059.jpg?height=446&width=1976&top_left_y=847&top_left_x=23)\n\nEntity Extraction\n\nTopic Modeling\n\nDocument Similarity\n\n\n\n## Rule Based Sentiment\n\n:::{.callout-note}\n Given a review ($X$) on a movie ratings website, decide whether its label ($y$) is positive (1), negative (-1) or neutral (0).\n:::\n\n```{dot}\ndigraph SentimentAnalysis {\n  rankdir=LR\n  node [shape=plaintext, scale=0.60]\n\n  sentence1 [label=<\n    <TABLE BORDER=\"0\" CELLBORDER=\"0\" CELLSPACING=\"0\">\n      <TR><TD>\"I hate this movie\"</TD></TR>\n    </TABLE>\n  >]\n\n  sentence2 [label=<\n    <TABLE BORDER=\"0\" CELLBORDER=\"0\" CELLSPACING=\"0\">\n      <TR><TD>\"I love this movie\"</TD></TR>\n    </TABLE>\n  >]\n\n  sentence3 [label=<\n    <TABLE BORDER=\"0\" CELLBORDER=\"0\" CELLSPACING=\"0\">\n      <TR><TD>\"I saw this movie\"</TD></TR>\n    </TABLE>\n  >]\n\n  sentiment1 [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"0\" CELLSPACING=\"0\" CELLPADDING=\"4\">\n      <TR>\n        <TD><FONT COLOR=\"red\">negative</FONT></TD>\n        <TD><FONT COLOR=\"green\">positive</FONT></TD>\n        <TD><FONT COLOR=\"black\">neutral</FONT></TD>\n      </TR>\n    </TABLE>\n  >]\n\n  sentiment2 [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"0\" CELLSPACING=\"0\" CELLPADDING=\"4\">\n      <TR>\n        <TD><FONT COLOR=\"green\">positive</FONT></TD>\n        <TD><FONT COLOR=\"black\">neutral</FONT></TD>\n        <TD><FONT COLOR=\"red\">negative</FONT></TD>\n      </TR>\n    </TABLE>\n  >]\n\n  sentiment3 [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"0\" CELLSPACING=\"0\" CELLPADDING=\"4\">\n      <TR>\n        <TD><FONT COLOR=\"black\">neutral</FONT></TD>\n        <TD><FONT COLOR=\"green\">positive</FONT></TD>\n        <TD><FONT COLOR=\"red\">negative</FONT></TD>\n      </TR>\n    </TABLE>\n  >]\n\n  sentence1 -> sentiment1\n  sentence2 -> sentiment2\n  sentence3 -> sentiment3\n}\n\n```\n\n\n\n# Natural Language Processing Pipeline\n\n## Natural Language Processing Pipeline\n\n![NLP Pipeline](M01_lecture02_figures/NLP-Pipeline.jpeg){width=80% fig-align=\"center\" #fig-nlp-pipeline fig-alt=\"General NLP Pipeline\"}\n\n# Sentiment Classification \n\n## Text Information\n\n::: {.cell output-location='column' execution_count=1}\n``` {.python .cell-code}\ndef read_xy_data(filename: str) -> tuple[list[str], list[int]]:\n    x_data = []\n    y_data = []\n    with open(filename, 'r') as f:\n        for line in f:\n            label, text = line.strip().split(' ||| ')\n            x_data.append(text)\n            y_data.append(int(label))\n    return x_data, y_data\n\n\nx_train, y_train = read_xy_data('./data/sentiment-treebank/train.txt')\nx_test, y_test = read_xy_data('./data/sentiment-treebank/dev.txt')\n\n\nprint(\"Document:-\", x_train[0])\nprint(\"Label:-\", y_train[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument:- The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\nLabel:- 1\n```\n:::\n:::\n\n\n## Segmentation, Tokenization, and Cleaning\n\n::: {.cell output-location='column' execution_count=2}\n``` {.python .cell-code}\ndef extract_features(x: str) -> dict[str, float]:\n    features = {}\n    x_split = x.split(' ')\n\n    # Count the number of \"good words\" and \"bad words\" in the text\n    good_words = ['love', 'good', 'nice', 'great', 'enjoy', 'enjoyed']  # <1>\n    bad_words = ['hate', 'bad', 'terrible',\n                 'disappointing', 'sad', 'lost', 'angry']  # <1>\n    for x_word in x_split:  # <2>\n        if x_word in good_words:  # <2>\n            features['good_word_count'] = features.get(\n                'good_word_count', 0) + 1  # <2>\n        if x_word in bad_words:  # <2>\n            features['bad_word_count'] = features.get(\n                'bad_word_count', 0) + 1  # <2>\n\n    # The \"bias\" value is always one, to allow us to assign a \"default\" score to the text\n    features['bias'] = 1  # <3>\n\n    return features\n\n\nfeature_weights = {'good_word_count': 1.0, 'bad_word_count': -1.0, 'bias': 0.5}\n```\n:::\n\n\n1. We list the words that represent sentiment,\n2. We count the number of good words and bad words in the text,\n3. We add a bias term to allow us to assign a \"default\" score to the text.\n   1. Think of $\\beta_{0}$ in OLS calculation where we add an array of $[\\mathbb{1}]$\n\n## Decision ML Algorithm\n\n::: {.cell output-location='column' execution_count=3}\n``` {.python .cell-code}\ndef run_classifier(x: str) -> int:\n    score = 0\n    for feat_name, feat_value in extract_features(x).items():\n        score = score + feat_value * feature_weights.get(feat_name, 0)\n    if score > 0:\n        return 1\n    elif score < 0:\n        return -1\n    else:\n        return 0\n\ndef calculate_accuracy(x_data: list[str], y_data: list[int]) -> float:\n    total_number = 0\n    correct_number = 0\n    for x, y in zip(x_data, y_data):\n        y_pred = run_classifier(x)\n        total_number += 1\n        if y == y_pred:\n            correct_number += 1\n    return correct_number / float(total_number)\n\n```\n:::\n\n\n## Results \n\n::: {.cell output-location='column' execution_count=4}\n``` {.python .cell-code}\nlabel_count = {}\nfor y in y_test:\n    if y not in label_count:\n        label_count[y] = 0\n    label_count[y] += 1\nprint(label_count)\n\ntrain_accuracy = calculate_accuracy(x_train, y_train)\ntest_accuracy = calculate_accuracy(x_test, y_test)\n\nprint(f'Train accuracy: {train_accuracy}')\nprint(f'Dev/test accuracy: {test_accuracy}')\n\n# Display 4 decimal\nprint(f'Train accuracy: {train_accuracy:.4f}')\nprint(f'Dev/test accuracy: {test_accuracy:.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{1: 444, 0: 229, -1: 428}\nTrain accuracy: 0.4345739700374532\nDev/test accuracy: 0.4214350590372389\nTrain accuracy: 0.4346\nDev/test accuracy: 0.4214\n```\n:::\n:::\n\n\n## Model Evaluation\n\n::: {.cell output-location='column' execution_count=5}\n``` {.python .cell-code}\nimport random\n\n\ndef find_errors(x_data, y_data):\n    error_ids = []\n    y_preds = []\n    for i, (x, y) in enumerate(zip(x_data, y_data)):\n        y_preds.append(run_classifier(x))\n        if y != y_preds[-1]:\n            error_ids.append(i)\n    for _ in range(5):\n        my_id = random.choice(error_ids)\n        x, y, y_pred = x_data[my_id], y_data[my_id], y_preds[my_id]\n        print(f'{x}\\ntrue label: {y}\\npredicted label: {y_pred}\\n')\n\n\nfind_errors(x_train, y_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBut it has an ambition to say something about its subjects , but not a willingness .\ntrue label: -1\npredicted label: 1\n\nNone of this is half as moving as the filmmakers seem to think .\ntrue label: -1\npredicted label: 1\n\nWhile Glover , the irrepressible eccentric of River 's Edge , Dead Man and Back to the Future , is perfect casting for the role , he represents Bartleby 's main overall flaw .\ntrue label: 0\npredicted label: 1\n\nMalone does have a gift for generating nightmarish images that will be hard to burn out of your brain .\ntrue label: 0\npredicted label: 1\n\nIt 's hard to imagine acting that could be any flatter .\ntrue label: -1\npredicted label: 1\n\n```\n:::\n:::\n\n\n## Improving the Model\n\n1. What's going wrong with my system?\n2. Modify the system (featurization, scoring function, etc.)\n3. Measure accuracy improvements, accept/reject change\n4. Repeat from 1\n5. Finally, when satisfied with dev accuracy, evaluate on test\n\n# Extreme or Rare Cases\n\n## Linguistic Barriers\n\n- Low-frequency Words\n- Conjugation\n- Negation \n- Metaphor \n- Analogy\n- Symbolic Languages\n\n:::{.callout-tip}\nCan we think of solutions for these?\n:::\n\n\n# Probabilistic Topic Modeling \n\n## Probabilistic Topic Modeling {background-color=\"#f6e1d7\"}\n\n![Probabilistic Topic Modeling](M01_lecture02_figures/Probabilistic-Topic-Modeling.png){width=80% fig-align=\"center\" #fig-topic-modeling fig-alt=\"screenshot of papers from Science magazine about topic modeling\"}\n\n## Machine Learning\n-  We want to [estimate]{.uugreen-bold} a function that will [predict]{.uugreen-bold} the label of a given text relatively well.\n- The function $f(x)$ can be [linear]{.uugreen-bold} or [non-linear]{.uugreen-bold}.\n- It can be [defined by humans]{.uugreen-bold} or [learned from data]{.uugreen-bold}.\n\n![Machine Learning](M01_lecture02_figures/MLsteps.png){width=80% fig-align=\"center\" #fig-machine-learning fig-alt=\"Machine Learning end to end pipeline\"}\n\n# Bag of Words approach\n\n## What is Bag of Words?\n\n- Text is treated as a **collection (bag)** of words\n- Word order is discarded\n- Each document becomes a **vector of word counts**\n\n![Bag of Words](M01_lecture02_figures/bag-of-words.png){width=60% fig-align=\"center\" #fig-bag-of-words fig-alt=\"Bag of Words\"}\n\n## Why We Need Bag of Words\n\n- Machines do not understand text\n- Models require **fixed-length numeric vectors**\n- Bag of Words is the **first workable bridge** between language and math\n\n> \"Meaning is ignored; frequency is preserved.\"\n\n---\n\n## Text Cleaning\n\n::: {.columns}\n::: {.column width=\"50%\" .fragment}\n:::{.callout}\nDespite suffering a sense-of-humour failure , The Man Who Wrote Rocky does not deserve to go down with a ship as leaky as this .\n:::\n\n:::{.callout}\ndespite suffering a sense of humour failure the man who wrote rocky does not deserve to go down with a ship as leaky as this\n:::\n\n:::\n::: {.column width=\"50%\" .fragment}\n\n- Lowercasing\n- Removing punctuation\n- Removing numbers\n- Removing stopwords (optional)\n:::\n:::\n\n---\n\n## Stanford Text Corpus Import\n\n::: {.cell output-location='column' execution_count=6}\n``` {.python .cell-code}\nimport random\n\ndef sample_sentences(x, y, n=4, seed=42):\n    random.seed(seed)\n    idx = random.sample(range(len(x)), n)\n    return [(y[i], x[i]) for i in idx]\n\nsamples = sample_sentences(x_train, y_train, n=4)\n\nfor i, (label, text) in enumerate(samples, 1):\n    print(f\"S{i} [label={label}]: {text}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nS1 [label=1]: With Dickens ' words and writer-director Douglas McGrath 's even-toned direction , a ripping good yarn is told .\nS2 [label=0]: Maybe Thomas Wolfe was right : You ca n't go home again .\nS3 [label=-1]: Despite suffering a sense-of-humour failure , The Man Who Wrote Rocky does not deserve to go down with a ship as leaky as this .\nS4 [label=1]: It will guarantee to have you leaving the theater with a smile on your face .\n```\n:::\n:::\n\n\n- Import all documents in the corpus.\n- Make sure they are stored in the right format.\n\n## Tokenization\n\n- Breaking text into units\n  - Split text into tokens (usually words)\n  - Simple whitespace tokenization is often enough\n  - More advanced methods exist (e.g., subword tokenization)\n  - Tokens are the **building blocks** for Bag of Words\n  - Example:  \"I love NLP!\"  $\\rightarrow$  [\"I\", \"love\", \"NLP\"]\n\n---\n\n## Building `CountVectorizer`\n\n::: {.cell output-location='column' execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n\ndocs = [text for _, text in samples]\n\nvectorizer = CountVectorizer(\n    lowercase=True,\n    stop_words=None   # keep everything for teaching clarity\n)\n\nX = vectorizer.fit_transform(docs)\n\nbow_df = pd.DataFrame(\n    X.toarray(),\n    columns=vectorizer.get_feature_names_out(),\n    index=[f\"S{i+1}\" for i in range(len(docs))]\n)\n\nbow_df.iloc[:, 0:8]\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>again</th>\n      <th>and</th>\n      <th>as</th>\n      <th>ca</th>\n      <th>deserve</th>\n      <th>despite</th>\n      <th>dickens</th>\n      <th>direction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>S1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>S2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>S3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>S4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Vocabulary Construction\n\n- Vocabulary = **unique words across all documents**\n- Vocabulary size grows quickly\n- Rare words may be removed\n- Vocabulary ≈ **feature space**\n- Too large → sparse, inefficient models\n\n## Counting Words - Document-Term Matrix (DTM)\n\n- Rows = documents\n- Columns = vocabulary terms\n- Values = word counts\n\nThis is the [actual model input]{.uublue-bold}.\n\n## Word Frequencies\n\n::: {.cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](M01_P2_files/figure-pptx/cell-9-output-1.png){}\n:::\n:::\n\n\n---\n\n## What BoW Gets Right\n\n::: {.columns}\n::: {.column}\n### Strengths\n\n- Simple and interpretable\n- Fast to compute\n- Works surprisingly well for:\n  - sentiment analysis\n  - topic classification\n  - spam detection\n\n\n:::\n::: {.column}\n### Limitations\n\n- Ignores word order\n- Ignores meaning\n- Vocabulary explosion\n- Sparse matrices\n- Classic failure: \"not good\" ≈ \"good\"\n\n:::\n:::\n\n---\n\n\n\n## BoW in Practice (Sentiment Analysis)\n\n```{dot}\ndigraph SentimentPipeline {\n\n    rankdir=LR;\n    splines=ortho;\n    bgcolor=\"white\";\n    nodesep=0.6;\n    ranksep=0.9;\n    fontname=\"Helvetica\";\n\n    node [\n        shape=box,\n        style=\"rounded,filled\",\n        fontname=\"Helvetica\",\n        fontsize=10,\n        fillcolor=\"#F7F7F7\",\n        color=\"#555555\"\n    ];\n\n    edge [\n        fontname=\"Helvetica\",\n        fontsize=9,\n        color=\"#555555\"\n    ];\n\n    /* ===== Input Layer ===== */\n    Text [\n        label=\"Textual Data\\n(A statement)\",\n        fillcolor=\"#E8F1FA\"\n    ];\n\n    /* ===== Lexicons ===== */\n    Lex1 [\n        label=\"Corpus\",\n        shape=folder,\n        fillcolor=\"#FFF3CD\"\n    ];\n\n    Lex2 [\n        label=\"Lexicon\",\n        shape=cylinder,\n        fillcolor=\"#FFF3CD\"\n    ];\n\n    /* ===== Processing Steps ===== */\n    Step1 [\n        label=\"Step 1:\\nCalculate O–S Polarity\",\n        fillcolor=\"#E3F2FD\"\n    ];\n\n    Decision [\n        label=\"Is there a sentiment?\",\n        shape=diamond,\n        fillcolor=\"#FDEDEC\"\n    ];\n\n    Step2 [\n        label=\"Step 2:\\nCalculate N–P Polarity\\nof the sentiment\",\n        fillcolor=\"#E3F2FD\"\n    ];\n\n    Step3 [\n        label=\"Step 3:\\nIdentify the target\\nfor the sentiment\",\n        fillcolor=\"#E3F2FD\"\n    ];\n\n    /* ===== Output Layer ===== */\n    Record [\n        label=\"Record Polarity,\\nStrength,\\nand Target\",\n        fillcolor=\"#E8F8F5\"\n    ];\n\n    Step4 [\n        label=\"Step 4:\\nTabulate & aggregate\\nsentiment analysis results\",\n        fillcolor=\"#D5F5E3\"\n    ];\n\n    /* ===== Layout Control (Two Rows) ===== */\n    { rank=same; Text; Step1; Decision }\n    { rank=same; Step2; Step3; Record; Step4 }\n\n    /* ===== Edges ===== */\n    Text -> Step1;\n    Lex1 -> Step1;\n\n    Step1 -> Decision;\n\n    Decision -> Text   [label=\"No\"];\n    Decision -> Step2  [label=\"Yes\"];\n\n    Lex2 -> Step2;\n\n    Step2 -> Step3;\n\n    Step1 -> Record [label=\"O–S Polarity\"];\n    Step2 -> Record [label=\"N–P Polarity\"];\n    Step3 -> Record [label=\"Target\"];\n\n    Record -> Step4;\n}\n\n```\n\n\n---\n\n## When Should You Use BoW?\n\n::: {.columns}\n::: {.column}\n### When to Use Bag of Words\n-  Dataset is small to medium\n-  Interpretability matters\n-  You need a fast baseline\n-  Text is short and structured\n:::\n::: {.column}\n### When Not to Use Bag of Words\n- Long documents\n- Semantic nuance matters\n- Context is critical\n\n:::\n:::\n---\n\n## Conceptual Takeaway\n\n\n> Bag of Words is not about language—it is about [counting]{.uublue-bold}.\n\n- Bag of words is a stepping stone to more advanced techniques:\n  - TF-IDF (weighting counts)\n  - Embeddings (learning meaning)\n  - Transformers (learning context)\n\n\n\n# References\n\n:::{.refs}\n\n:::\n\n",
    "supporting": [
      "M01_P2_files\\figure-pptx"
    ],
    "filters": []
  }
}