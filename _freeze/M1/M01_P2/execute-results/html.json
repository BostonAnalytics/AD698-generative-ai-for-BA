{
  "hash": "9606fa47d30f7d10b0458d233042fb01",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"AD698 - Applied Generative AI\"\nsubtitle: \"Language, Probability, and Generative Systems\"\nlogo: \"../theme/figures/met_logotype_black.png\"\ndate: 03/12/2024\ndate-modified: today\ndate-format: long\nauthor:\n  - name: Nakul R. Padalkar\n    affiliations:\n      - name: Boston University\n        city: Boston\n        state: MA\nformat: \n    revealjs:\n        theme: [../theme/presentation.scss]\n        html-math-method: katex\n        slide-number: c/t\n        toc: true\n        toc-depth: 1\n        auto-stretch: false\n        from: markdown+emoji\n        code-line-numbers: true\n    pptx:\n        reference-doc: ../theme/presentation_template.pptx\nself-contained-math: true\ncode-annotations: below\nfig-align: center\nmonofont: Roboto\ntitle-slide-attributes:\n    data-background-image: \"../theme/blank_red.png\"\n    data-background-size: 103% auto\n    data-background-opacity: \"0.95\"\nexecute: \n  echo: false\n  warning: false\n  message: false\n  freeze: auto\n  keep-ipynb: true\nbibliography: ../references.bib\ncsl: ../mis-quarterly.csl\n---\n\n# Text Analytics and Mining\n\n## Text Analytics\n\n![Text Analytics [@talib2016text]](./M01_lecture02_figures/text_mining_overview.jpg){width=80% fig-align=\"center\" #fig-text-mining-overview fig-alt=\"Text Mining Overview\"}\n\n## Text Mining Process\n\n```{dot}\ndigraph NLP_Pipeline {\n\n    /* =========================\n       Global graph settings\n       ========================= */\n\n    newrank=\"true\";\n    rankdir=\"TB\";\n    splines=\"spline\";\n\n    graph [\n        fontname=\"Helvetica\"\n        fontsize=10\n        bgcolor=\"white\"\n        ranksep=\"1.25\"\n        pad=\"0.5\"\n    ];\n\n    node [\n        shape=\"box\"\n        style=\"rounded,filled\"\n        fontname=\"Helvetica\"\n        fontsize=11\n        color=\"#1f3a8a\"\n        fillcolor=\"#e8f0ff\"\n    ];\n\n    edge [\n        fontname=\"Helvetica\"\n        fontsize=9\n        color=\"#1f3a8a\"\n    ];\n\n    /* =========================\n       TOP ROW — Lexical pipeline\n       ========================= */\n\n    subgraph cluster_lexical {\n        label=\"Lexical processing\";\n        style=\"filled,rounded\";\n        color=\"grey95\";\n        rank=same;\n\n        Characters;\n        Tokens;\n        TaggedTokens [ label=\"Tagged tokens\" ];\n\n        Characters -> Tokens;\n        Tokens -> TaggedTokens;\n    }\n\n    /* =========================\n       BOTTOM ROW — Structural pipeline\n       ========================= */\n\n    subgraph cluster_structural {\n        label=\"Structural representation\";\n        style=\"filled,rounded\";\n        color=\"grey95\";\n        rank=same;\n\n        EntityRelations [ label=\"Entity relationships\" ];\n        SyntaxTree      [ label=\"Syntax tree\" ];\n        KnowledgeBase   [ label=\"Knowledge base\" ];\n\n        /* Right-to-left logical flow */\n        SyntaxTree    -> EntityRelations;\n        EntityRelations -> KnowledgeBase;\n\n    }\n    \n    subgraph cluster_algorithm {\n        label=\"Algorithm\";\n        style=\"filled,rounded\";\n        color=\"grey95\";\n        rank=same;\n\n        fst_regex[color=\"#99CC99\" fillcolor=\"#D6EBD6\" label=\"Regular expression\" ];\n        fst_pos  [color=\"#99CC99\" fillcolor=\"#D6EBD6\" label=\"Part-of-Speech\" ];\n        fst_logic[color=\"#99CC99\" fillcolor=\"#D6EBD6\" label=\"Logic compiler\" ];\n        fst_ie   [color=\"#99CC99\" fillcolor=\"#D6EBD6\" label=\"Information extractor\" ];\n    }\n\n    /* =========================\n       Invisible merge anchors\n       ========================= */\n\n    merge_tokens [\n        shape=\"circle\"\n        width=\".25\"\n        fixedsize=\"true\"\n        label=\"\"\n        style=\"invis\"\n    ];\n\n    merge_tagged [\n        shape=\"circle\"\n        width=\".25\"\n        fixedsize=\"true\"\n        label=\"\"\n        style=\"invis\"\n    ];\n\n    merge_syntax [\n        shape=\"circle\"\n        width=\".25\"\n        fixedsize=\"true\"\n        label=\"\"\n        style=\"invis\"\n    ];\n\n    /* =========================\n       Controlled merges\n       ========================= */\n\n    Tokens        -> SyntaxTree      [ arrowhead=\"vee\"];\n    TaggedTokens -> SyntaxTree       [ arrowhead=\"vee\"];\n\n    /* =========================\n       FST annotations (semantic)\n       ========================= */\n\n    /* Invisible anchors for labels */\n    // fst_regex  [ shape=\"circle\" width=\".25\" fixedsize=\"true\" color=\"invis\" label=\"\" ];\n    // fst_pos    [ shape=\"circle\" width=\".25\" fixedsize=\"true\" color=\"invis\" label=\"\" ];\n    // fst_logic  [ shape=\"circle\" width=\".25\" fixedsize=\"true\" color=\"invis\" label=\"\" ];\n    // fst_ie     [ shape=\"circle\" width=\".25\" fixedsize=\"true\" color=\"invis\" label=\"\" ];\n\n    fst_regex -> Tokens       [ style=\"dashed\" arrowhead=\"vee\" constraint=\"false\" color=\"#99CC99\" penwidth=2.0];\n    fst_regex -> TaggedTokens [ style=\"dashed\" arrowhead=\"vee\" constraint=\"false\" color=\"#99CC99\" penwidth=2.0];\n    SyntaxTree   -> fst_pos [ style=\"dashed\" arrowhead=\"vee\" constraint=\"false\" color=\"#99CC99\" penwidth=2.0];\n    KnowledgeBase-> fst_ie    [ style=\"dashed\" arrowhead=\"vee\" constraint=\"false\" color=\"#99CC99\" penwidth=2.0];\n    KnowledgeBase-> fst_logic [ style=\"dashed\" arrowhead=\"vee\" constraint=\"false\" color=\"#99CC99\" penwidth=2.0];\n\n    /* Labels */\n    // fst_regex  [ xlabel=\"Regular expressions\" ];\n    // fst_pos    [ xlabel=\"POS tagger (FST)\" ];\n    // fst_logic  [ xlabel=\"Logic compiler (FST)\" ];\n    // fst_ie     [ xlabel=\"Information extractor (FST)\" ];\n\n    /* =========================\n       Rank alignment\n       ========================= */\n\n    { rank=\"same\"; Characters; Tokens; TaggedTokens; }\n    { rank=\"same\"; EntityRelations; SyntaxTree; KnowledgeBase; }\n    { rank=\"same\"; fst_regex; fst_pos; fst_logic; fst_ie}\n}\n\n```\n\n## What is Natural Language Processing (NLP)?\n\n- Technology that enables computers to process, generate, and interact with language (e.g., text). Some key aspects:\n- [Learn useful representations]{.uublue-bold}: capture meaning in a structured way that can be used for downstream tasks (e.g., embeddings used to classify a document)\n- [Generate language]{.uublue-bold}: create language (e.g., text, \n- code) for tasks like dialogue, translation, or question answering.\n- [Bridge language and action]{.uublue-bold}: Use language to perform tasks, solve problems, interact with environments (e.g., a code IDE)\n\n## General NLP Framework \n\n```{dot}\n\ndigraph NLP_Tasks {\n  node [shape=plaintext]\n\n  nlp_table [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"4\">\n      <TR>\n        <TD bgcolor=\"#137dcb\"><B>Input X</B></TD>\n        <TD bgcolor=\"lightcoral\"><B>Output Y</B></TD>\n        <TD bgcolor=\"#0ccc90\"><B>Task</B></TD>\n      </TR>\n      <TR>\n        <TD>Text</TD>\n        <TD>Continuing Text</TD>\n        <TD>Language Modeling</TD>\n      </TR>\n      <TR>\n        <TD>Text</TD>\n        <TD>Text in Other Language</TD>\n        <TD>Translation</TD>\n      </TR>\n      <TR>\n        <TD>Text</TD>\n        <TD>Label</TD>\n        <TD>Text Classification</TD>\n      </TR>\n      <TR>\n        <TD>Text</TD>\n        <TD>Linguistic Structure</TD>\n        <TD>Language Analysis</TD>\n      </TR>\n      <TR>\n        <TD>Image</TD>\n        <TD>Text</TD>\n        <TD>Image Captioning</TD>\n      </TR>\n    </TABLE>\n  >]\n\n  title [label=<\n    <TABLE BORDER=\"0\" CELLBORDER=\"0\">\n      <TR><TD><B>Create a function to map an input X into an output Y, where X and/or Y involve language.</B></TD></TR>\n    </TABLE>\n  >]\n\n  title -> nlp_table\n}\n\n```\n\n## Building NLP Systems\n\n- Rules: Manual creation of rules\n\n  ```\n  def classify(x: str) -> str:\n      sports_keywords = [\"baseball\", \"soccer\", \"football\", \"tennis\"]\n      if any(keyword in x for keyword in sports_keywords):\n          return \"sports\"\n      else:\n          return \"other\"\n  ```\n\n\n- Prompting: Prompting a language model w/o training\n\n```{dot}\ndigraph PromptLogic {\n  rankdir=LR\n  node [shape=plaintext]\n\n  decision_box [label=<\n    <TABLE BORDER=\"0.50\" CELLBORDER=\"0.5\" CELLSPACING=\"0\" CELLPADDING=\"2\" bgcolor=\"lightyellow\">\n      <TR><TD width=\"400\" fixedsize=\"false\"><B>If the following sentence is about 'sports', reply 'sports'. Otherwise reply 'other'.</B></TD></TR>\n    </TABLE>\n  >]\n\n  lm_node [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"2\" bgcolor=\"lightblue\">\n      <TR><TD width=\"75\" fixedsize=\"true\"><B>LM</B></TD></TR>\n    </TABLE>\n  >]\n\n  decision_box -> lm_node\n}\n\n```\n\n## Building NLP Systems\n\n- Fine-tuning: Machine learning from paired data $\\langle X, Y\\rangle$\n\n```{dot}\ndigraph TextClassificationTraining {\n    rankdir=LR\n\n  node [shape=plaintext]\n\n  samples [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"4\">\n      <TR><TD><B>Sentence</B></TD><TD><B>Label</B></TD></TR>\n      <TR><TD>\"I love to play baseball.\"</TD><TD>sports</TD></TR>\n      <TR><TD>\"The stock price is going up.\"</TD><TD>other</TD></TR>\n      <TR><TD>\"He got a hat-trick yesterday.\"</TD><TD>sports</TD></TR>\n      <TR><TD>\"He is wearing tennis shoes.\"</TD><TD>other</TD></TR>\n    </TABLE>\n  >]\n\n  training [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"6\" bgcolor=\"lightgray\">\n      <TR><TD><B>Training</B></TD></TR>\n    </TABLE>\n  >]\n\n  model [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"6\">\n      <TR>\n        <TD><IMG SRC=\"M01_lecture02_figures/reshot-icon-engineering-6XYGVMCJ59.png\"/></TD>\n        <TD><B>Model</B></TD>\n      </TR>\n    </TABLE>\n  >]\n\n  samples -> training -> model\n}\n\n```\n\n## Data Requirements for System Building\n\n-  [Rules/prompting based on intuition]{.uublue-bold}: No data needed, but also no performance guarantees  \n- [Rules/prompting based on spot-checks]{.uublue-bold}: A small amount of data with input $X$ only\n- [Rules/prompting with rigorous evaluation]{.uublue-bold}: Development set with input $X$ and output $Y$ (e.g. 200-2000 examples). Additional held-out test set also preferable.\n- [Fine-tuning]{.uublue-bold}: Additional train set. More is often better - constant accuracy increase when data size doubles.\n\n```{dot}\ndigraph DataSplit {\n  rankdir=LR\n  node [shape=plaintext]\n\n  split_table [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"6\">\n      <TR>\n        <TD bgcolor=\"lightgreen\" width=\"378\">\n          <B>Train Data</B><BR/><FONT COLOR=\"red\">X_train, y_train</FONT><BR/>60%\n        </TD>\n        <TD bgcolor=\"lightblue\" width=\"108\">\n          <B>Test Data</B><BR/><FONT COLOR=\"red\">X_test, y_test</FONT><BR/>20%\n        </TD>\n        <TD bgcolor=\"lightyellow\" width=\"54\">\n          <B>Validation (Dev)</B><BR/><FONT COLOR=\"red\">X_val, y_val</FONT><BR/>20%\n        </TD>\n      </TR>\n    </TABLE>\n  >]\n}\n\n```\n\n\n## Text Analytics\n\n- Knowledge Discovery in Textual Databases\n- Text Mining is a subfield of Text Analytics and Natural Language Processing (NLP)\n- Text Analytics is a broader term that encompasses various techniques for analyzing and extracting insights from text data.\n- Borrows from various fields such as information retrieval, machine learning, statistics, linguistics, and others.\n$$\n\\begin{align}\n\\text{Text Mining} &= \\text{Information Extraction} + \\nonumber \\\\\n      &\\quad{}  \\text{Data Mining} + \\text{Web Mining} \\\\\n\\text{Text Analytics} &= \\text{Information Retrieval} + \\text{Text Mining}\n\\end{align}\n$$\n\n## Application Areas of Text Mining\n\n- Information extraction\n- Topic tracking\n- Summarization\n- Categorization\n- Clustering\n- Concept linking\n- Question answering\n\n# Text Mining and Analytics Process \n\n![NLP Pipeline](M01_lecture02_figures/NLP-Pipeline.jpeg){width=80% fig-align=\"center\" #fig-nlp-pipeline fig-alt=\"General NLP Pipeline\"}\n\n## Sentiment Analysis\n\n```{dot}\ndigraph Sentiment_Analysis_Map {\n\n    /* =========================\n       Global settings\n       ========================= */\n\n    rankdir=LR;\n    newrank=true;\n    splines=ortho;\n\n    graph [\n        fontname=\"Helvetica\"\n        fontsize=11\n        bgcolor=\"white\"\n        pad=\"0.0\"\n        margin=\"0\"\n        nodesep=\"0.6\"\n        ranksep=\"1.2\"\n    ];\n\n    node [\n        shape=box\n        style=\"rounded,filled\"\n        fontname=\"Helvetica\"\n        fontsize=11\n        color=\"#555555\"\n    ];\n\n    edge [\n        color=\"#333333\"\n        penwidth=1.6\n        arrowsize=0.8\n    ];\n\n    /* =========================\n       Root\n       ========================= */\n\n    SentimentAnalysis [\n        label=\"Sentiment\\nAnalysis\"\n        fillcolor=\"#cde6a3\"\n        fontsize=12\n        color=\"#6b8e23\"\n    ];\n\n    /* =========================\n       TASKS CLUSTER\n       ========================= */\n\n    subgraph cluster_tasks {\n        label=\"Tasks\";\n        style=\"dashed,rounded\";\n        color=\"red\";\n        fontcolor=\"red\";\n        fontsize=14;\n\n        Subjectivity [\n            label=\"Subjectivity\\nClassification\"\n            fillcolor=\"#cfe1f7\"\n        ];\n\n        SentimentClass [\n            label=\"Sentiment\\nClassification\"\n            fillcolor=\"#ffd37f\"\n        ];\n\n        ReviewUse [\n            label=\"Review\\nUsefulness\\nMeasurement\"\n            fillcolor=\"#cfe1f7\"\n        ];\n\n        SpamDetect [\n            label=\"Opinion Spam\\nDetection\"\n            fillcolor=\"#cfe1f7\"\n        ];\n\n        LexiconCreate [\n            label=\"Lexicon\\nCreation\"\n            fillcolor=\"#bfe3ea\"\n        ];\n\n        AspectExtract [\n            label=\"Aspect\\nExtraction\"\n            fillcolor=\"#bfe3ea\"\n        ];\n\n        Application [\n            label=\"Application\"\n            fillcolor=\"#eeeeee\"\n            style=\"rounded,dashed,filled\"\n        ];\n    }\n\n    /* =========================\n       SUBTASKS (middle column)\n       ========================= */\n\n    subgraph cluster_subtasks {\n        label=\"\";\n        color=\"invis\";\n\n        Polarity [\n            label=\"Polarity\\nDetermination\"\n            fillcolor=\"#ffd37f\"\n        ];\n\n        Vagueness [\n            label=\"Vagueness\\nresolution in\\nopinionated text\"\n            fillcolor=\"#ffd37f\"\n        ];\n\n        MultiLingual [\n            label=\"Multi- & Cross-\\nLingual SC\"\n            fillcolor=\"#ffd37f\"\n        ];\n\n        CrossDomain [\n            label=\"Cross-domain\\nSC\"\n            fillcolor=\"#ffd37f\"\n        ];\n    }\n\n    /* =========================\n       APPROACHES CLUSTER\n       ========================= */\n\n    subgraph cluster_approaches {\n        label=\"Approaches\";\n        style=\"dashed,rounded\";\n        color=\"red\";\n        fontcolor=\"red\";\n        fontsize=14;\n\n        ML [\n            label=\"Machine Learning\\nbased\"\n            fillcolor=\"#ffb000\"\n        ];\n\n        LexiconBased [\n            label=\"Lexicon based\"\n            fillcolor=\"#ffb000\"\n        ];\n\n        Hybrid [\n            label=\"Hybrid\\napproaches\"\n            fillcolor=\"#ffb000\"\n        ];\n\n        Ontology [\n            label=\"Ontology based\"\n            fillcolor=\"#bfe3ea\"\n        ];\n\n        NonOntology [\n            label=\"Non-Ontology\\nbased\"\n            fillcolor=\"#bfe3ea\"\n        ];\n    }\n\n    /* =========================\n       EDGES — Main tree\n       ========================= */\n\n    SentimentAnalysis -> Subjectivity;\n    SentimentAnalysis -> SentimentClass;\n    SentimentAnalysis -> ReviewUse;\n    SentimentAnalysis -> SpamDetect;\n    SentimentAnalysis -> LexiconCreate;\n    SentimentAnalysis -> AspectExtract;\n    SentimentAnalysis -> Application;\n\n    /* =========================\n       Task → Subtask links\n       ========================= */\n\n    Subjectivity   -> Polarity;\n    SentimentClass -> Vagueness;\n    ReviewUse      -> MultiLingual;\n    SpamDetect     -> CrossDomain;\n\n    /* =========================\n       Subtask → Approaches\n       ========================= */\n\n    Polarity     -> ML;\n    Vagueness    -> LexiconBased;\n    MultiLingual -> Hybrid;\n    CrossDomain  -> Hybrid;\n\n    LexiconCreate -> Ontology;\n    AspectExtract -> NonOntology;\n\n    /* =========================\n       Rank alignment\n       ========================= */\n\n    { rank=same; Subjectivity; SentimentClass; ReviewUse; SpamDetect; LexiconCreate; AspectExtract; Application; }\n    { rank=same; Polarity; Vagueness; MultiLingual; CrossDomain; }\n    { rank=same; ML; LexiconBased; Hybrid; Ontology; NonOntology; }\n}\n\n```\n\n## Sentiment Classification Algorithms\n\n```{dot}\ndigraph Sentiment_Approaches {\n\n    /* =========================\n       Global settings\n       ========================= */\n\n    rankdir=TD;\n    newrank=true;\n    splines=curved;\n\n    graph [\n        fontname=\"Handlee\"\n        fontsize=11\n        bgcolor=\"white\"\n        pad=\"0.0\"\n        margin=\"0\"\n        nodesep=\"0.6\"\n        ranksep=\"1.2\"\n    ];\n\n    node [\n        shape=box\n        style=\"rounded,filled\"\n        fontname=\"Handlee\"\n        fontsize=11\n        color=\"#555555\"\n    ];\n\n    edge [\n        fontname = \"Handlee\"\n        color=\"#333333\"\n        penwidth=1.6\n        arrowsize=0.8\n    ];\n\n    /* =========================\n       ROOT NODES\n       ========================= */\n\n    ML_Approach [\n        label=\"Machine Learning\\nApproach\"\n        fillcolor=\"#ffd37f\"\n        fontsize=12\n    ];\n\n    Lexicon_Approach [\n        label=\"Lexicon-based\\nApproach\"\n        fillcolor=\"#cde6a3\"\n        fontsize=12\n    ];\n\n    /* =========================\n       MACHINE LEARNING BRANCH\n       ========================= */\n\n    Supervised [\n        label=\"Supervised\\nLearning\"\n        fillcolor=\"#ffd37f\"\n    ];\n\n    Unsupervised [\n        label=\"Unsupervised\\nLearning\"\n        fillcolor=\"#ffd37f\"\n    ];\n\n    ML_Approach -> Supervised;\n    ML_Approach -> Unsupervised;\n\n    /* ---- Classifier families ---- */\n\n    DecisionTree [\n        label=\"Decision Tree\\nClassifiers\"\n        fillcolor=\"#ffd37f\"\n    ];\n\n    LinearCls [\n        label=\"Linear\\nClassifiers\"\n        fillcolor=\"#ffd37f\"\n    ];\n\n    RuleBased [\n        label=\"Rule-based\\nClassifiers\"\n        fillcolor=\"#ffd37f\"\n    ];\n\n    Probabilistic [\n        label=\"Probabilistic\\nClassifiers\"\n        fillcolor=\"#ffd37f\"\n    ];\n\n    Supervised -> DecisionTree;\n    Supervised -> LinearCls;\n    Supervised -> RuleBased;\n    Unsupervised -> Probabilistic;\n\n    /* ---- Specific algorithms ---- */\n\n    SVM [\n        label=\"Support Vector\\nMachine (SVM)\"\n        fillcolor=\"#ffb000\"\n    ];\n\n    NN [\n        label=\"Neural Network\\n(NN)\"\n        fillcolor=\"#ffb000\"\n    ];\n\n    DL [\n        label=\"Deep Learning\\n(DL)\"\n        fillcolor=\"#ffb000\"\n    ];\n\n    NB [\n        label=\"Naïve Bayes\\n(NB)\"\n        fillcolor=\"#ffb000\"\n    ];\n\n    BN [\n        label=\"Bayesian Network\\n(BN)\"\n        fillcolor=\"#ffb000\"\n    ];\n\n    ME [\n        label=\"Maximum\\nEntropy (ME)\"\n        fillcolor=\"#ffb000\"\n    ];\n\n    DecisionTree -> SVM;\n    LinearCls   -> NN;\n    LinearCls   -> DL;\n\n    Probabilistic -> NB;\n    Probabilistic -> BN;\n    Probabilistic -> ME;\n\n    /* =========================\n       LEXICON-BASED BRANCH\n       ========================= */\n\n    Dictionary [\n        label=\"Dictionary-based\\nApproach\"\n        fillcolor=\"#cde6a3\"\n    ];\n\n    Corpus [\n        label=\"Corpus-based\\nApproach\"\n        fillcolor=\"#cde6a3\"\n    ];\n\n    Lexicon_Approach -> Dictionary;\n    Lexicon_Approach -> Corpus;\n\n    Statistical [\n        label=\"Statistical\"\n        fillcolor=\"#bfe3ea\"\n    ];\n\n    Semantic [\n        label=\"Semantic\"\n        fillcolor=\"#bfe3ea\"\n    ];\n\n    Corpus -> Statistical;\n    Corpus -> Semantic;\n\n    /* =========================\n       Rank alignment\n       ========================= */\n\n    { rank=same; ML_Approach; Lexicon_Approach; }\n    { rank=same; Supervised; Unsupervised; Dictionary; Corpus; }\n    { rank=same; DecisionTree; LinearCls; RuleBased; Probabilistic; Statistical; Semantic; }\n    { rank=same; SVM; NN; DL; NB; BN; ME; }\n}\n\n```\n\n## Types of Sentiment Analysis\n\n- Aspects-based\n- Emotion-based\n- Fine-grained\n- Intent-based\n\n:::{.r-stack}\n\n![](./M01_lecture02_figures/types-of-sentiment-analysis - 1.png){width=80% fig-align=\"center\" fig-alt=\"Types of Sentiment Analysis, describes Aspects-based, Emotion-based, Fine-grained, Intent-based sentiment analysis\" .fragment .fade-out}\n\n![](./M01_lecture02_figures/types-of-sentiment-analysis - 2.png){width=80% fig-align=\"center\"  fig-alt=\"Types of Sentiment Analysis, describes Aspects-based, Emotion-based, Fine-grained, Intent-based sentiment analysis\" .fragment .fade-in}\n\n![](./M01_lecture02_figures/types-of-sentiment-analysis - 3.png){width=80% fig-align=\"center\" fig-alt=\"Types of Sentiment Analysis, describes Aspects-based, Emotion-based, Fine-grained, Intent-based sentiment analysis\" .fragment .fade-in}\n\n![](./M01_lecture02_figures/types-of-sentiment-analysis - 4.png){width=80% fig-align=\"center\" fig-alt=\"Types of Sentiment Analysis, describes Aspects-based, Emotion-based, Fine-grained, Intent-based sentiment analysis\" .fragment .fade-in}\n:::\n\n# Web Mining, personalization, Social Analytics\n\n## Web Mining \n\n- Web mining is the application of data mining techniques to discover patterns from the World Wide Web.\n\n::: {.columns}\n::: {.column}\n\n:::\n::: {.column}\n:::\n:::\n\n```{dot}\ndigraph Weblog_Mining_Process {\n\n    /* =========================\n       Global settings\n       ========================= */\n\n    rankdir=LR;\n    newrank=true;\n    splines=ortho;\n\n    graph [\n        fontname=\"Helvetica\"\n        fontsize=14\n        bgcolor=\"white\"\n        pad=\"0\"\n        margin=\"0\"\n        nodesep=\"0.8\"\n        ranksep=\"1.1\"\n    ];\n\n    node [\n        shape=box\n        style=\"rounded\"\n        fontname=\"Helvetica\"\n        fontsize=14\n        margin=\"0.25,0.18\"\n        color=\"black\"\n    ];\n\n    edge [\n        color=\"black\"\n        penwidth=1.4\n        arrowsize=0.8\n    ];\n\n    /* =========================\n       Main vertical pipeline\n       ========================= */\n\n    RawData        [ label=\"Raw Data\" ];\n    Collection     [ label=\"Weblog Data Collection\" ];\n    Integration    [ label=\"Data Integration\" ];\n    Preprocessing  [ label=\"Data Pre-processing\" ];\n    Extraction     [ label=\"Pattern Extraction\" ];\n    Analysis       [ label=\"Pattern Analysis\" ];\n    Output         [ label=\"Pattern Formed\" ];\n\n    RawData -> Collection;\n    Collection -> Integration;\n    Integration -> Preprocessing;\n    Preprocessing -> Extraction;\n    Extraction -> Analysis;\n    Analysis -> Output;\n\n    /* =========================\n       Invisible anchors for braces\n       ========================= */\n\n    pre_top  [ shape=point width=0 label=\"\" ];\n    pre_bot  [ shape=point width=0 label=\"\" ];\n\n    disc_top [ shape=point width=0 label=\"\" ];\n    disc_bot [ shape=point width=0 label=\"\" ];\n\n    anal_top [ shape=point width=0 label=\"\" ];\n    anal_bot [ shape=point width=0 label=\"\" ];\n\n    /* =========================\n       Rank alignment\n       ========================= */\n\n    { rank=same; Integration; pre_top; }\n    { rank=same; Preprocessing; pre_bot; }\n\n    { rank=same; Extraction; disc_top; }\n    { rank=same; Analysis; disc_bot; }\n\n    { rank=same; Analysis; anal_top; }\n    { rank=same; Output; anal_bot; }\n\n}\n\n```\n\n## Web Content Mining (Web Scraping) {background-color=\"#eff8ff\"}\n\n![Web Scraping Process](./M01_lecture02_figures/web-scrapping.png){width=80% fig-align=\"center\" #fig-web-scrapping fig-alt=\"Web Scraping Process\"}\n\n\n## Web Usage Mining (Web Analytics)\n\n- Web usage mining (Web analytics) is the extraction of useful information from data generated through Web page visits and transactions.\n\n![](./M01_lecture02_figures/Web_Usage_Mining.png){width=80% fig-align=\"center\" fig-alt=\"Web Usage Mining Process\"}\n\n## Extraction of Knowledge from Web Usage Data\n\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-038.jpg?height=934&width=2382&top_left_y=629&top_left_x=64)\n\n## Social Analytics\n\n- Social analytics is defined as monitoring, analyzing, measuring and interpreting digital interactions and relationships of people, topics, ideas and content.\n\n\n## Branches of Social Analytics\n\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-040.jpg?height=1149&width=2299&top_left_y=496&top_left_x=80)\n\nText Mining Technologies\n\nText Mining (TM)\n\nNatural Language Processing (NLP)\n\n## Text Mining Concepts\n\n- 85-90 percent of all corporate data is in some kind of unstructured form (e.g., text)\n- Unstructured corporate data is doubling in size every 18 months\n- Tapping into these information sources is not an option, but a need to stay competitive\n- Answer: text mining\n- A semi-automated process of extracting knowledge from unstructured data sources\n- a.k.a. text data mining or knowledge discovery in textual databases\n\n\n## Text mining\n\nText Data Mining\n\n## Intelligent Text Analysis\n\nKnowledge-Discovery in Text (KDT)\n\nText Mining (text data mining)\nthe process of deriving high-quality information from text\nhttp:/len.wikipedia.orq/wiki/rext mining\n\n## Text Mining:\n\nthe process of extracting interesting and non-trivial information and knowledge from unstructured text.\n\n## Text Mining:\n\ndiscovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\n\n## An example of Text Mining\n\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-048.jpg?height=1468&width=2416&top_left_y=295&top_left_x=36)\n\n## Overview of Information Extraction based Text Mining Framework\n\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-049.jpg?height=781&width=2400&top_left_y=771&top_left_x=60)\n\n## Natural Language Processing (NLP)\n\n- Natural language processing (NLP) is an important component of text mining and is a subfield of artificial intelligence and computational linguistics.\n\n\n## Natural Language Processing (NLP) and Text Mining\n\n| Raw text | word's stem word's lemma am → am am $\\rightarrow$ be having → hav having → have |\n| :--- | :--- |\n| Sentence Segmentation |  |\n| Tokenization |  |\n| Part-of-Speech (POS) |  |\n| Stop word removal |  |\n| Stemming / Lemmatization |  |\n| Dependency Parser |  |\n| String Metrics \\& Matching |  |\n\n## Text Sunnnarization\n\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-052.jpg?height=1455&width=2177&top_left_y=315&top_left_x=73)\n\nTopics\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-053.jpg?height=298&width=341&top_left_y=474&top_left_x=7)\n\n| life | 0.02 |\n| :--- | :--- |\n| evolve | 0.01 |\n| organism | 0.01 |\n| $\\cdots$ |  |\n\n\n| brain | 0.04 |\n| :--- | :--- |\n| neuron | 0.02 |\n| nerve | 0.01 |\n| $\\cdots$ |  |\n\n\n| data | 0.02 |\n| :--- | :--- |\n| number | 0.02 |\n| computer | 0.01 |\n| . . ' |  |\n\n## Documents\n\n# Topic proportions and assignments \n\n## Seeking Life's Bare (Genetic) Necessities\n\nCold Spring Harbor, New York- \"are not all that far apart,\" especially in How many genes does an organism need to comparison to the 75,000 genes in the husurvive? Last week at the genome meeting here,* two genome researchers with radically different approaches presented complementary views of the basic genes needed for life. One research team, using computer analyses to compare known genomes, concluded that today's organisms can be sustained with just 250 genes, and that the earliest life forms required a mere 128 genes. The other researcher mapped genes in a simple parasite and estimated that for this organism, 800 genes are plenty to do the job-but that anything short of 100 wouldn't be enough.\n\nAlthough the numbers don't match precisely, those predictions\n\n* Genome Mapping and Sequencing, Cold Spring Harbor, New York, May 8 to 12.\n\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-053.jpg?height=469&width=1389&top_left_y=894&top_left_x=933)\nStripping down. Computer analysis yields an estimate of the minimum modern and ancient genomes.\n\n[^0]\n## Natural Language Processing (NLP)\n\n- Part-of-speech tagging\n- Text segmentation\n- Word sense disambiguation\n- Syntactic ambiguity\n- Imperfect or irregular input\n- Speech acts\n\n\n## NLP Tasks\n\n- Question answering\n- Automatic summarization\n- Natural language generation\n- Natural language understanding\n- Machine translation\n- Foreign language reading\n- Foreign language writing.\n- Speech recognition\n- Text-to-speech\n- Text proofing\n- Optical character recognition\n\n\n## Classical NLP\n\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-056.jpg?height=504&width=2020&top_left_y=409&top_left_x=243)\n\n## Deep Learning-based NLP\n\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-056.jpg?height=643&width=2090&top_left_y=1164&top_left_x=244)\n\n## Modern NLP Pipeline\n\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-057.jpg?height=575&width=2356&top_left_y=203&top_left_x=41)\nTask / Output\n\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-057.jpg?height=396&width=601&top_left_y=1130&top_left_x=48)\n\nTask / Output\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-057.jpg?height=691&width=1278&top_left_y=1128&top_left_x=626)\n\n## Modern NLP Pipeline\n\nTask / Output\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-058.jpg?height=675&width=2442&top_left_y=753&top_left_x=21)\n\n## Deep Learning NLP\n\nTask / Output\n\nClassification\n\nSentiment Analysis\n![](https://cdn.mathpix.com/cropped/c931bfb7-94ef-4062-8314-bc3ef9e9e62f-059.jpg?height=446&width=1976&top_left_y=847&top_left_x=23)\n\nEntity Extraction\n\nTopic Modeling\n\nDocument Similarity\n\n\n\n## Rule Based Sentiment\n\n:::{.callout-note}\n Given a review ($X$) on a movie ratings website, decide whether its label ($y$) is positive (1), negative (-1) or neutral (0).\n:::\n\n```{dot}\ndigraph SentimentAnalysis {\n  rankdir=LR\n  node [shape=plaintext, scale=0.60]\n\n  sentence1 [label=<\n    <TABLE BORDER=\"0\" CELLBORDER=\"0\" CELLSPACING=\"0\">\n      <TR><TD>\"I hate this movie\"</TD></TR>\n    </TABLE>\n  >]\n\n  sentence2 [label=<\n    <TABLE BORDER=\"0\" CELLBORDER=\"0\" CELLSPACING=\"0\">\n      <TR><TD>\"I love this movie\"</TD></TR>\n    </TABLE>\n  >]\n\n  sentence3 [label=<\n    <TABLE BORDER=\"0\" CELLBORDER=\"0\" CELLSPACING=\"0\">\n      <TR><TD>\"I saw this movie\"</TD></TR>\n    </TABLE>\n  >]\n\n  sentiment1 [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"0\" CELLSPACING=\"0\" CELLPADDING=\"4\">\n      <TR>\n        <TD><FONT COLOR=\"red\">negative</FONT></TD>\n        <TD><FONT COLOR=\"green\">positive</FONT></TD>\n        <TD><FONT COLOR=\"black\">neutral</FONT></TD>\n      </TR>\n    </TABLE>\n  >]\n\n  sentiment2 [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"0\" CELLSPACING=\"0\" CELLPADDING=\"4\">\n      <TR>\n        <TD><FONT COLOR=\"green\">positive</FONT></TD>\n        <TD><FONT COLOR=\"black\">neutral</FONT></TD>\n        <TD><FONT COLOR=\"red\">negative</FONT></TD>\n      </TR>\n    </TABLE>\n  >]\n\n  sentiment3 [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"0\" CELLSPACING=\"0\" CELLPADDING=\"4\">\n      <TR>\n        <TD><FONT COLOR=\"black\">neutral</FONT></TD>\n        <TD><FONT COLOR=\"green\">positive</FONT></TD>\n        <TD><FONT COLOR=\"red\">negative</FONT></TD>\n      </TR>\n    </TABLE>\n  >]\n\n  sentence1 -> sentiment1\n  sentence2 -> sentiment2\n  sentence3 -> sentiment3\n}\n\n```\n\n\n\n# Natural Language Processing Pipeline\n\n## Natural Language Processing Pipeline\n\n![NLP Pipeline](M01_lecture02_figures/NLP-Pipeline.jpeg){width=80% fig-align=\"center\" #fig-nlp-pipeline fig-alt=\"General NLP Pipeline\"}\n\n# Sentiment Classification \n\n## Text Information\n\n::: {#60b1cab5 .cell output-location='column' execution_count=1}\n``` {.python .cell-code}\ndef read_xy_data(filename: str) -> tuple[list[str], list[int]]:\n    x_data = []\n    y_data = []\n    with open(filename, 'r') as f:\n        for line in f:\n            label, text = line.strip().split(' ||| ')\n            x_data.append(text)\n            y_data.append(int(label))\n    return x_data, y_data\n\n\nx_train, y_train = read_xy_data('./data/sentiment-treebank/train.txt')\nx_test, y_test = read_xy_data('./data/sentiment-treebank/dev.txt')\n\n\nprint(\"Document:-\", x_train[0])\nprint(\"Label:-\", y_train[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument:- The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\nLabel:- 1\n```\n:::\n:::\n\n\n## Segmentation, Tokenization, and Cleaning\n\n::: {#151eb78e .cell output-location='column' execution_count=2}\n``` {.python .cell-code}\ndef extract_features(x: str) -> dict[str, float]:\n    features = {}\n    x_split = x.split(' ')\n\n    # Count the number of \"good words\" and \"bad words\" in the text\n    good_words = ['love', 'good', 'nice', 'great', 'enjoy', 'enjoyed']  # <1>\n    bad_words = ['hate', 'bad', 'terrible',\n                 'disappointing', 'sad', 'lost', 'angry']  # <1>\n    for x_word in x_split:  # <2>\n        if x_word in good_words:  # <2>\n            features['good_word_count'] = features.get(\n                'good_word_count', 0) + 1  # <2>\n        if x_word in bad_words:  # <2>\n            features['bad_word_count'] = features.get(\n                'bad_word_count', 0) + 1  # <2>\n\n    # The \"bias\" value is always one, to allow us to assign a \"default\" score to the text\n    features['bias'] = 1  # <3>\n\n    return features\n\n\nfeature_weights = {'good_word_count': 1.0, 'bad_word_count': -1.0, 'bias': 0.5}\n```\n:::\n\n\n1. We list the words that represent sentiment,\n2. We count the number of good words and bad words in the text,\n3. We add a bias term to allow us to assign a \"default\" score to the text.\n   1. Think of $\\beta_{0}$ in OLS calculation where we add an array of $[\\mathbb{1}]$\n\n## Decision ML Algorithm\n\n::: {#dfe769e5 .cell output-location='column' execution_count=3}\n``` {.python .cell-code}\ndef run_classifier(x: str) -> int:\n    score = 0\n    for feat_name, feat_value in extract_features(x).items():\n        score = score + feat_value * feature_weights.get(feat_name, 0)\n    if score > 0:\n        return 1\n    elif score < 0:\n        return -1\n    else:\n        return 0\n\ndef calculate_accuracy(x_data: list[str], y_data: list[int]) -> float:\n    total_number = 0\n    correct_number = 0\n    for x, y in zip(x_data, y_data):\n        y_pred = run_classifier(x)\n        total_number += 1\n        if y == y_pred:\n            correct_number += 1\n    return correct_number / float(total_number)\n\n```\n:::\n\n\n## Results \n\n::: {#1c8f1270 .cell output-location='column' execution_count=4}\n``` {.python .cell-code}\nlabel_count = {}\nfor y in y_test:\n    if y not in label_count:\n        label_count[y] = 0\n    label_count[y] += 1\nprint(label_count)\n\ntrain_accuracy = calculate_accuracy(x_train, y_train)\ntest_accuracy = calculate_accuracy(x_test, y_test)\n\nprint(f'Train accuracy: {train_accuracy}')\nprint(f'Dev/test accuracy: {test_accuracy}')\n\n# Display 4 decimal\nprint(f'Train accuracy: {train_accuracy:.4f}')\nprint(f'Dev/test accuracy: {test_accuracy:.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{1: 444, 0: 229, -1: 428}\nTrain accuracy: 0.4345739700374532\nDev/test accuracy: 0.4214350590372389\nTrain accuracy: 0.4346\nDev/test accuracy: 0.4214\n```\n:::\n:::\n\n\n## Model Evaluation\n\n::: {#b8fd86f6 .cell output-location='column' execution_count=5}\n``` {.python .cell-code}\nimport random\n\n\ndef find_errors(x_data, y_data):\n    error_ids = []\n    y_preds = []\n    for i, (x, y) in enumerate(zip(x_data, y_data)):\n        y_preds.append(run_classifier(x))\n        if y != y_preds[-1]:\n            error_ids.append(i)\n    for _ in range(5):\n        my_id = random.choice(error_ids)\n        x, y, y_pred = x_data[my_id], y_data[my_id], y_preds[my_id]\n        print(f'{x}\\ntrue label: {y}\\npredicted label: {y_pred}\\n')\n\n\nfind_errors(x_train, y_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLess-than-compelling documentary of a Yiddish theater clan .\ntrue label: 0\npredicted label: 1\n\nAs we have come to learn -- as many times as we have fingers to count on -- Jason is a killer who does n't know the meaning of the word ` quit . '\ntrue label: 0\npredicted label: 1\n\nThough there are entertaining and audacious moments , the movie 's wildly careening tone and an extremely flat lead performance do little to salvage this filmmaker 's flailing reputation .\ntrue label: -1\npredicted label: 1\n\nImpeccably filmed , sexually charged , but ultimately lacking in substance , not to mention dragged down by a leaden closing act .\ntrue label: 0\npredicted label: 1\n\nA big fat pain .\ntrue label: -1\npredicted label: 1\n\n```\n:::\n:::\n\n\n## Improving the Model\n\n1. What's going wrong with my system?\n2. Modify the system (featurization, scoring function, etc.)\n3. Measure accuracy improvements, accept/reject change\n4. Repeat from 1\n5. Finally, when satisfied with dev accuracy, evaluate on test\n\n# Extreme or Rare Cases\n\n## Linguistic Barriers\n\n- Low-frequency Words\n- Conjugation\n- Negation \n- Metaphor \n- Analogy\n- Symbolic Languages\n\n:::{.callout-tip}\nCan we think of solutions for these?\n:::\n\n\n# Probabilistic Topic Modeling \n\n## Probabilistic Topic Modeling {background-color=\"#f6e1d7\"}\n\n![Probabilistic Topic Modeling](M01_lecture02_figures/Probabilistic-Topic-Modeling.png){width=80% fig-align=\"center\" #fig-topic-modeling fig-alt=\"screenshot of papers from Science magazine about topic modeling\"}\n\n## Machine Learning\n-  We want to [estimate]{.uugreen-bold} a function that will [predict]{.uugreen-bold} the label of a given text relatively well.\n- The function $f(x)$ can be [linear]{.uugreen-bold} or [non-linear]{.uugreen-bold}.\n- It can be [defined by humans]{.uugreen-bold} or [learned from data]{.uugreen-bold}.\n\n![Machine Learning](M01_lecture02_figures/MLsteps.png){width=80% fig-align=\"center\" #fig-machine-learning fig-alt=\"Machine Learning end to end pipeline\"}\n\n# Bag of Words approach\n\n## What is Bag of Words?\n\n- Text is treated as a **collection (bag)** of words\n- Word order is discarded\n- Each document becomes a **vector of word counts**\n\n![Bag of Words](M01_lecture02_figures/bag-of-words.png){width=60% fig-align=\"center\" #fig-bag-of-words fig-alt=\"Bag of Words\"}\n\n## Why We Need Bag of Words\n\n- Machines do not understand text\n- Models require **fixed-length numeric vectors**\n- Bag of Words is the **first workable bridge** between language and math\n\n> \"Meaning is ignored; frequency is preserved.\"\n\n---\n\n## Text Cleaning\n\n::: {.columns}\n::: {.column width=\"50%\" .fragment}\n:::{.callout}\nDespite suffering a sense-of-humour failure , The Man Who Wrote Rocky does not deserve to go down with a ship as leaky as this .\n:::\n\n:::{.callout}\ndespite suffering a sense of humour failure the man who wrote rocky does not deserve to go down with a ship as leaky as this\n:::\n\n:::\n::: {.column width=\"50%\" .fragment}\n\n- Lowercasing\n- Removing punctuation\n- Removing numbers\n- Removing stopwords (optional)\n:::\n:::\n\n---\n\n## Stanford Text Corpus Import\n\n::: {#149523af .cell output-location='column' execution_count=6}\n``` {.python .cell-code}\nimport random\n\ndef sample_sentences(x, y, n=4, seed=42):\n    random.seed(seed)\n    idx = random.sample(range(len(x)), n)\n    return [(y[i], x[i]) for i in idx]\n\nsamples = sample_sentences(x_train, y_train, n=4)\n\nfor i, (label, text) in enumerate(samples, 1):\n    print(f\"S{i} [label={label}]: {text}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nS1 [label=1]: With Dickens ' words and writer-director Douglas McGrath 's even-toned direction , a ripping good yarn is told .\nS2 [label=0]: Maybe Thomas Wolfe was right : You ca n't go home again .\nS3 [label=-1]: Despite suffering a sense-of-humour failure , The Man Who Wrote Rocky does not deserve to go down with a ship as leaky as this .\nS4 [label=1]: It will guarantee to have you leaving the theater with a smile on your face .\n```\n:::\n:::\n\n\n- Import all documents in the corpus.\n- Make sure they are stored in the right format.\n\n## Tokenization\n\n- Breaking text into units\n  - Split text into tokens (usually words)\n  - Simple whitespace tokenization is often enough\n  - More advanced methods exist (e.g., subword tokenization)\n  - Tokens are the **building blocks** for Bag of Words\n  - Example:  \"I love NLP!\"  $\\rightarrow$  [\"I\", \"love\", \"NLP\"]\n\n---\n\n## Building `CountVectorizer`\n\n::: {#bafd11ca .cell output-location='column' execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n\ndocs = [text for _, text in samples]\n\nvectorizer = CountVectorizer(\n    lowercase=True,\n    stop_words=None   # keep everything for teaching clarity\n)\n\nX = vectorizer.fit_transform(docs)\n\nbow_df = pd.DataFrame(\n    X.toarray(),\n    columns=vectorizer.get_feature_names_out(),\n    index=[f\"S{i+1}\" for i in range(len(docs))]\n)\n\nbow_df.iloc[:, 0:8]\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>again</th>\n      <th>and</th>\n      <th>as</th>\n      <th>ca</th>\n      <th>deserve</th>\n      <th>despite</th>\n      <th>dickens</th>\n      <th>direction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>S1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>S2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>S3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>S4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Vocabulary Construction\n\n- Vocabulary = **unique words across all documents**\n- Vocabulary size grows quickly\n- Rare words may be removed\n- Vocabulary ≈ **feature space**\n- Too large → sparse, inefficient models\n\n## Counting Words - Document-Term Matrix (DTM)\n\n- Rows = documents\n- Columns = vocabulary terms\n- Values = word counts\n\nThis is the [actual model input]{.uublue-bold}.\n\n## Word Frequencies\n\n::: {#62786f5d .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](M01_P2_files/figure-revealjs/cell-9-output-1.png){width=944 height=512}\n:::\n:::\n\n\n---\n\n## What BoW Gets Right\n\n::: {.columns}\n::: {.column}\n### Strengths\n\n- Simple and interpretable\n- Fast to compute\n- Works surprisingly well for:\n  - sentiment analysis\n  - topic classification\n  - spam detection\n\n\n:::\n::: {.column}\n### Limitations\n\n- Ignores word order\n- Ignores meaning\n- Vocabulary explosion\n- Sparse matrices\n- Classic failure: \"not good\" ≈ \"good\"\n\n:::\n:::\n\n---\n\n\n\n## BoW in Practice (Sentiment Analysis)\n\n```{dot}\ndigraph SentimentPipeline {\n\n    rankdir=LR;\n    splines=ortho;\n    bgcolor=\"white\";\n    nodesep=0.6;\n    ranksep=0.9;\n    fontname=\"Helvetica\";\n\n    node [\n        shape=box,\n        style=\"rounded,filled\",\n        fontname=\"Helvetica\",\n        fontsize=10,\n        fillcolor=\"#F7F7F7\",\n        color=\"#555555\"\n    ];\n\n    edge [\n        fontname=\"Helvetica\",\n        fontsize=9,\n        color=\"#555555\"\n    ];\n\n    /* ===== Input Layer ===== */\n    Text [\n        label=\"Textual Data\\n(A statement)\",\n        fillcolor=\"#E8F1FA\"\n    ];\n\n    /* ===== Lexicons ===== */\n    Lex1 [\n        label=\"Corpus\",\n        shape=folder,\n        fillcolor=\"#FFF3CD\"\n    ];\n\n    Lex2 [\n        label=\"Lexicon\",\n        shape=cylinder,\n        fillcolor=\"#FFF3CD\"\n    ];\n\n    /* ===== Processing Steps ===== */\n    Step1 [\n        label=\"Step 1:\\nCalculate O–S Polarity\",\n        fillcolor=\"#E3F2FD\"\n    ];\n\n    Decision [\n        label=\"Is there a sentiment?\",\n        shape=diamond,\n        fillcolor=\"#FDEDEC\"\n    ];\n\n    Step2 [\n        label=\"Step 2:\\nCalculate N–P Polarity\\nof the sentiment\",\n        fillcolor=\"#E3F2FD\"\n    ];\n\n    Step3 [\n        label=\"Step 3:\\nIdentify the target\\nfor the sentiment\",\n        fillcolor=\"#E3F2FD\"\n    ];\n\n    /* ===== Output Layer ===== */\n    Record [\n        label=\"Record Polarity,\\nStrength,\\nand Target\",\n        fillcolor=\"#E8F8F5\"\n    ];\n\n    Step4 [\n        label=\"Step 4:\\nTabulate & aggregate\\nsentiment analysis results\",\n        fillcolor=\"#D5F5E3\"\n    ];\n\n    /* ===== Layout Control (Two Rows) ===== */\n    { rank=same; Text; Step1; Decision }\n    { rank=same; Step2; Step3; Record; Step4 }\n\n    /* ===== Edges ===== */\n    Text -> Step1;\n    Lex1 -> Step1;\n\n    Step1 -> Decision;\n\n    Decision -> Text   [label=\"No\"];\n    Decision -> Step2  [label=\"Yes\"];\n\n    Lex2 -> Step2;\n\n    Step2 -> Step3;\n\n    Step1 -> Record [label=\"O–S Polarity\"];\n    Step2 -> Record [label=\"N–P Polarity\"];\n    Step3 -> Record [label=\"Target\"];\n\n    Record -> Step4;\n}\n\n```\n\n\n---\n\n## When Should You Use BoW?\n\n::: {.columns}\n::: {.column}\n### When to Use Bag of Words\n-  Dataset is small to medium\n-  Interpretability matters\n-  You need a fast baseline\n-  Text is short and structured\n:::\n::: {.column}\n### When Not to Use Bag of Words\n- Long documents\n- Semantic nuance matters\n- Context is critical\n\n:::\n:::\n---\n\n## Conceptual Takeaway\n\n\n> Bag of Words is not about language—it is about [counting]{.uublue-bold}.\n\n- Bag of words is a stepping stone to more advanced techniques:\n  - TF-IDF (weighting counts)\n  - Embeddings (learning meaning)\n  - Transformers (learning context)\n\n\n\n# References\n\n:::{.refs}\n\n:::\n\n",
    "supporting": [
      "M01_P2_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}