{
  "hash": "c8102e3dfebd3076930bebb994d4aa10",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"AD698 - Applied Generative AI\"\nsubtitle: \"Language, Probability, and Generative Systems\"\nlogo: \"../theme/figures/met_logotype_black.png\"\ndate: 03/12/2024\ndate-modified: today\ndate-format: long\nauthor:\n  - name: Nakul R. Padalkar\n    affiliations:\n      - name: Boston University\n        city: Boston\n        state: MA\nformat: \n    revealjs:\n        theme: [../theme/presentation.scss]\n        html-math-method: katex\n        slide-number: c/t\n        toc: true\n        toc-depth: 1\n        auto-stretch: false\n        from: markdown+emoji\n        code-line-numbers: true\n\n    pptx:\n        reference-doc: ../theme/presentation_template.pptx\nself-contained-math: true\ncode-annotations: below\nfig-align: center\nmonofont: Roboto\ntitle-slide-attributes:\n    data-background-image: \"../theme/blank_red.png\"\n    data-background-size: 103% auto\n    data-background-opacity: \"0.95\"\nexecute: \n  echo: false\n  warning: false\n  message: false\n  freeze: auto\n  keep-ipynb: true\nbibliography: ../references.bib\ncsl: ../mis-quarterly.csl\n---\n\n# Text Analytics and Mining\n\n## Text Analytics\n\n![Text Analytics [@talib2016text]](./M01_lecture02_figures/text_mining_overview.jpg){width=80% fig-align=\"center\" #fig-text-mining-overview fig-alt=\"Text Mining Overview\"}\n\n## Text Mining Process\n\n![Text Mining Process](./M01_lecture02_figures/text_mining_operations_workflow.png){width=80% fig-align=\"center\" #fig-text-mining-process fig-alt=\"Text Mining Process\"}\n \n## What is Natural Language Processing (NLP)?\n\n- Technology that enables computers to process, generate, and interact with language (e.g., text). Some key aspects:\n- [Learn useful representations]{.uublue-bold}: capture meaning in a structured way that can be used for downstream tasks (e.g., embeddings used to classify a document)\n- [Generate language]{.uublue-bold}: create language (e.g., text, \n- code) for tasks like dialogue, translation, or question answering.\n- [Bridge language and action]{.uublue-bold}: Use language to perform tasks, solve problems, interact with environments (e.g., a code IDE)\n\n## General NLP Framework \n\n```{dot}\n\ndigraph NLP_Tasks {\n  node [shape=plaintext]\n\n  nlp_table [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"4\">\n      <TR>\n        <TD bgcolor=\"#137dcb\"><B>Input X</B></TD>\n        <TD bgcolor=\"lightcoral\"><B>Output Y</B></TD>\n        <TD bgcolor=\"#0ccc90\"><B>Task</B></TD>\n      </TR>\n      <TR>\n        <TD>Text</TD>\n        <TD>Continuing Text</TD>\n        <TD>Language Modeling</TD>\n      </TR>\n      <TR>\n        <TD>Text</TD>\n        <TD>Text in Other Language</TD>\n        <TD>Translation</TD>\n      </TR>\n      <TR>\n        <TD>Text</TD>\n        <TD>Label</TD>\n        <TD>Text Classification</TD>\n      </TR>\n      <TR>\n        <TD>Text</TD>\n        <TD>Linguistic Structure</TD>\n        <TD>Language Analysis</TD>\n      </TR>\n      <TR>\n        <TD>Image</TD>\n        <TD>Text</TD>\n        <TD>Image Captioning</TD>\n      </TR>\n    </TABLE>\n  >]\n\n  title [label=<\n    <TABLE BORDER=\"0\" CELLBORDER=\"0\">\n      <TR><TD><B>Create a function to map an input X into an output Y, where X and/or Y involve language.</B></TD></TR>\n    </TABLE>\n  >]\n\n  title -> nlp_table\n}\n\n```\n\n## Building NLP Systems\n\n- Rules: Manual creation of rules\n\n  ```\n  def classify(x: str) -> str:\n      sports_keywords = [\"baseball\", \"soccer\", \"football\", \"tennis\"]\n      if any(keyword in x for keyword in sports_keywords):\n          return \"sports\"\n      else:\n          return \"other\"\n  ```\n\n\n- Prompting: Prompting a language model w/o training\n\n```{dot}\ndigraph PromptLogic {\n  rankdir=LR\n  node [shape=plaintext]\n\n  decision_box [label=<\n    <TABLE BORDER=\"0.50\" CELLBORDER=\"0.5\" CELLSPACING=\"0\" CELLPADDING=\"2\" bgcolor=\"lightyellow\">\n      <TR><TD width=\"400\" fixedsize=\"false\"><B>If the following sentence is about 'sports', reply 'sports'. Otherwise reply 'other'.</B></TD></TR>\n    </TABLE>\n  >]\n\n  lm_node [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"2\" bgcolor=\"lightblue\">\n      <TR><TD width=\"75\" fixedsize=\"true\"><B>LM</B></TD></TR>\n    </TABLE>\n  >]\n\n  decision_box -> lm_node\n}\n\n```\n\n## Building NLP Systems\n\n- Fine-tuning: Machine learning from paired data $\\langle X, Y\\rangle$\n\n```{dot}\ndigraph TextClassificationTraining {\n    rankdir=LR\n\n  node [shape=plaintext]\n\n  samples [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"4\">\n      <TR><TD><B>Sentence</B></TD><TD><B>Label</B></TD></TR>\n      <TR><TD>\"I love to play baseball.\"</TD><TD>sports</TD></TR>\n      <TR><TD>\"The stock price is going up.\"</TD><TD>other</TD></TR>\n      <TR><TD>\"He got a hat-trick yesterday.\"</TD><TD>sports</TD></TR>\n      <TR><TD>\"He is wearing tennis shoes.\"</TD><TD>other</TD></TR>\n    </TABLE>\n  >]\n\n  training [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"6\" bgcolor=\"lightgray\">\n      <TR><TD><B>Training</B></TD></TR>\n    </TABLE>\n  >]\n\n  model [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"6\">\n      <TR>\n        <TD><IMG SRC=\"M01_lecture02_figures/reshot-icon-engineering-6XYGVMCJ59.png\"/></TD>\n        <TD><B>Model</B></TD>\n      </TR>\n    </TABLE>\n  >]\n\n  samples -> training -> model\n}\n\n```\n\n\n\n## Data Requirements for System Building\n\n-  [Rules/prompting based on intuition]{.uublue-bold}: No data needed, but also no performance guarantees  \n- [Rules/prompting based on spot-checks]{.uublue-bold}: A small amount of data with input $X$ only\n- [Rules/prompting with rigorous evaluation]{.uublue-bold}: Development set with input $X$ and output $Y$ (e.g. 200-2000 examples). Additional held-out test set also preferable.\n- [Fine-tuning]{.uublue-bold}: Additional train set. More is often better - constant accuracy increase when data size doubles.\n\n```{dot}\ndigraph DataSplit {\n  rankdir=LR\n  node [shape=plaintext]\n\n  split_table [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"1\" CELLSPACING=\"0\" CELLPADDING=\"6\">\n      <TR>\n        <TD bgcolor=\"lightgreen\" width=\"378\">\n          <B>Train Data</B><BR/><FONT COLOR=\"red\">X_train, y_train</FONT><BR/>60%\n        </TD>\n        <TD bgcolor=\"lightblue\" width=\"108\">\n          <B>Test Data</B><BR/><FONT COLOR=\"red\">X_test, y_test</FONT><BR/>20%\n        </TD>\n        <TD bgcolor=\"lightyellow\" width=\"54\">\n          <B>Validation (Dev)</B><BR/><FONT COLOR=\"red\">X_val, y_val</FONT><BR/>20%\n        </TD>\n      </TR>\n    </TABLE>\n  >]\n}\n\n```\n\n## Rule Based Sentiment\n\n:::{.callout-note}\n Given a review ($X$) on a movie ratings website, decide whether its label ($y$) is positive (1), negative (-1) or neutral (0).\n:::\n\n```{dot}\ndigraph SentimentAnalysis {\n  rankdir=LR\n  node [shape=plaintext, scale=0.60]\n\n  sentence1 [label=<\n    <TABLE BORDER=\"0\" CELLBORDER=\"0\" CELLSPACING=\"0\">\n      <TR><TD>\"I hate this movie\"</TD></TR>\n    </TABLE>\n  >]\n\n  sentence2 [label=<\n    <TABLE BORDER=\"0\" CELLBORDER=\"0\" CELLSPACING=\"0\">\n      <TR><TD>\"I love this movie\"</TD></TR>\n    </TABLE>\n  >]\n\n  sentence3 [label=<\n    <TABLE BORDER=\"0\" CELLBORDER=\"0\" CELLSPACING=\"0\">\n      <TR><TD>\"I saw this movie\"</TD></TR>\n    </TABLE>\n  >]\n\n  sentiment1 [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"0\" CELLSPACING=\"0\" CELLPADDING=\"4\">\n      <TR>\n        <TD><FONT COLOR=\"red\">negative</FONT></TD>\n        <TD><FONT COLOR=\"green\">positive</FONT></TD>\n        <TD><FONT COLOR=\"black\">neutral</FONT></TD>\n      </TR>\n    </TABLE>\n  >]\n\n  sentiment2 [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"0\" CELLSPACING=\"0\" CELLPADDING=\"4\">\n      <TR>\n        <TD><FONT COLOR=\"green\">positive</FONT></TD>\n        <TD><FONT COLOR=\"black\">neutral</FONT></TD>\n        <TD><FONT COLOR=\"red\">negative</FONT></TD>\n      </TR>\n    </TABLE>\n  >]\n\n  sentiment3 [label=<\n    <TABLE BORDER=\"1\" CELLBORDER=\"0\" CELLSPACING=\"0\" CELLPADDING=\"4\">\n      <TR>\n        <TD><FONT COLOR=\"black\">neutral</FONT></TD>\n        <TD><FONT COLOR=\"green\">positive</FONT></TD>\n        <TD><FONT COLOR=\"red\">negative</FONT></TD>\n      </TR>\n    </TABLE>\n  >]\n\n  sentence1 -> sentiment1\n  sentence2 -> sentiment2\n  sentence3 -> sentiment3\n}\n\n```\n\n\n\n# Natural Language Processing Pipeline\n\n## Natural Language Processing Pipeline\n\n![NLP Pipeline](M01_lecture02_figures/NLP-Pipeline.jpeg){width=80% fig-align=\"center\" #fig-nlp-pipeline fig-alt=\"General NLP Pipeline\"}\n\n# Sentiment Classification \n\n## Text Information\n\n::: {#684deddc .cell output-location='column' execution_count=1}\n``` {.python .cell-code}\ndef read_xy_data(filename: str) -> tuple[list[str], list[int]]:\n    x_data = []\n    y_data = []\n    with open(filename, 'r') as f:\n        for line in f:\n            label, text = line.strip().split(' ||| ')\n            x_data.append(text)\n            y_data.append(int(label))\n    return x_data, y_data\n\n\nx_train, y_train = read_xy_data('./data/sentiment-treebank/train.txt')\nx_test, y_test = read_xy_data('./data/sentiment-treebank/dev.txt')\n\n\nprint(\"Document:-\", x_train[0])\nprint(\"Label:-\", y_train[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument:- The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\nLabel:- 1\n```\n:::\n:::\n\n\n## Segmentation, Tokenization, and Cleaning\n\n::: {#9c29ba3e .cell output-location='column' execution_count=2}\n``` {.python .cell-code}\ndef extract_features(x: str) -> dict[str, float]:\n    features = {}\n    x_split = x.split(' ')\n\n    # Count the number of \"good words\" and \"bad words\" in the text\n    good_words = ['love', 'good', 'nice', 'great', 'enjoy', 'enjoyed']  # <1>\n    bad_words = ['hate', 'bad', 'terrible',\n                 'disappointing', 'sad', 'lost', 'angry']  # <1>\n    for x_word in x_split:  # <2>\n        if x_word in good_words:  # <2>\n            features['good_word_count'] = features.get(\n                'good_word_count', 0) + 1  # <2>\n        if x_word in bad_words:  # <2>\n            features['bad_word_count'] = features.get(\n                'bad_word_count', 0) + 1  # <2>\n\n    # The \"bias\" value is always one, to allow us to assign a \"default\" score to the text\n    features['bias'] = 1  # <3>\n\n    return features\n\n\nfeature_weights = {'good_word_count': 1.0, 'bad_word_count': -1.0, 'bias': 0.5}\n```\n:::\n\n\n1. We list the words that represent sentiment,\n2. We count the number of good words and bad words in the text,\n3. We add a bias term to allow us to assign a \"default\" score to the text.\n   1. Think of $\\beta_{0}$ in OLS calculation where we add an array of $[\\mathbb{1}]$\n\n## Decision ML Algorithm\n\n::: {#c8acdf6d .cell output-location='column' execution_count=3}\n``` {.python .cell-code}\ndef run_classifier(x: str) -> int:\n    score = 0\n    for feat_name, feat_value in extract_features(x).items():\n        score = score + feat_value * feature_weights.get(feat_name, 0)\n    if score > 0:\n        return 1\n    elif score < 0:\n        return -1\n    else:\n        return 0\n\ndef calculate_accuracy(x_data: list[str], y_data: list[int]) -> float:\n    total_number = 0\n    correct_number = 0\n    for x, y in zip(x_data, y_data):\n        y_pred = run_classifier(x)\n        total_number += 1\n        if y == y_pred:\n            correct_number += 1\n    return correct_number / float(total_number)\n\n```\n:::\n\n\n## Results \n\n::: {#98809220 .cell output-location='column' execution_count=4}\n``` {.python .cell-code}\nlabel_count = {}\nfor y in y_test:\n    if y not in label_count:\n        label_count[y] = 0\n    label_count[y] += 1\nprint(label_count)\n\ntrain_accuracy = calculate_accuracy(x_train, y_train)\ntest_accuracy = calculate_accuracy(x_test, y_test)\n\nprint(f'Train accuracy: {train_accuracy}')\nprint(f'Dev/test accuracy: {test_accuracy}')\n\n# Display 4 decimal\nprint(f'Train accuracy: {train_accuracy:.4f}')\nprint(f'Dev/test accuracy: {test_accuracy:.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{1: 444, 0: 229, -1: 428}\nTrain accuracy: 0.4345739700374532\nDev/test accuracy: 0.4214350590372389\nTrain accuracy: 0.4346\nDev/test accuracy: 0.4214\n```\n:::\n:::\n\n\n## Model Evaluation\n\n::: {#6b30df31 .cell output-location='column' execution_count=5}\n``` {.python .cell-code}\nimport random\n\n\ndef find_errors(x_data, y_data):\n    error_ids = []\n    y_preds = []\n    for i, (x, y) in enumerate(zip(x_data, y_data)):\n        y_preds.append(run_classifier(x))\n        if y != y_preds[-1]:\n            error_ids.append(i)\n    for _ in range(5):\n        my_id = random.choice(error_ids)\n        x, y, y_pred = x_data[my_id], y_data[my_id], y_preds[my_id]\n        print(f'{x}\\ntrue label: {y}\\npredicted label: {y_pred}\\n')\n\n\nfind_errors(x_train, y_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWhy did they deem it necessary to document all this emotional misery ?\ntrue label: -1\npredicted label: 1\n\nThe French director has turned out nearly 21\\/2 hours of unfocused , excruciatingly tedious cinema that , half an hour in , starts making water torture seem appealing .\ntrue label: -1\npredicted label: 1\n\nI would be shocked if there was actually one correct interpretation , but that should n't make the movie or the discussion any less enjoyable .\ntrue label: 0\npredicted label: 1\n\nMore successful at relating history than in creating an emotionally complex , dramatically satisfying heroine\ntrue label: -1\npredicted label: 1\n\nThanks to Ice Cube , Benjamins feels an awful lot like Friday in Miami .\ntrue label: 0\npredicted label: 1\n\n```\n:::\n:::\n\n\n## Improving the Model\n\n1. What's going wrong with my system?\n2. Modify the system (featurization, scoring function, etc.)\n3. Measure accuracy improvements, accept/reject change\n4. Repeat from 1\n5. Finally, when satisfied with dev accuracy, evaluate on test\n\n# Extreme or Rare Cases\n\n## Linguistic Barriers\n\n- Low-frequency Words\n- Conjugation\n- Negation \n- Metaphor \n- Analogy\n- Symbolic Languages\n\n:::{.callout-tip}\nCan we think of solutions for these?\n:::\n\n\n# Probabilistic Topic Modeling \n\n## Probabilistic Topic Modeling {background-color=\"#f6e1d7\"}\n\n![Probabilistic Topic Modeling](M01_lecture02_figures/Probabilistic-Topic-Modeling.png){width=80% fig-align=\"center\" #fig-topic-modeling fig-alt=\"screenshot of papers from Science magazine about topic modeling\"}\n\n## Machine Learning\n-  We want to [estimate]{.uugreen-bold} a function that will [predict]{.uugreen-bold} the label of a given text relatively well.\n- The function $f(x)$ can be [linear]{.uugreen-bold} or [non-linear]{.uugreen-bold}.\n- It can be [defined by humans]{.uugreen-bold} or [learned from data]{.uugreen-bold}.\n\n![Machine Learning](M01_lecture02_figures/MLsteps.png){width=80% fig-align=\"center\" #fig-machine-learning fig-alt=\"Machine Learning end to end pipeline\"}\n\n# Bag of Words approach\n\n## What is Bag of Words?\n\n- Text is treated as a **collection (bag)** of words\n- Word order is discarded\n- Each document becomes a **vector of word counts**\n\n![Bag of Words](M01_lecture02_figures/bag-of-words.png){width=80% fig-align=\"center\" #fig-bag-of-words fig-alt=\"Bag of Words\"}\n\n## Why We Need Bag of Words\n\n- Machines do not understand text\n- Models require **fixed-length numeric vectors**\n- Bag of Words is the **first workable bridge** between language and math\n\n> ‚ÄúMeaning is ignored; frequency is preserved.‚Äù\n\n---\n\n## Text Cleaning\n\n::: {.columns}\n::: {.column width=\"50%\" .fragment}\n:::{.callout-important}\nDespite suffering a sense-of-humour failure , The Man Who Wrote Rocky does not deserve to go down with a ship as leaky as this .\n:::\n\n:::\n::: {.column width=\"50%\" .fragment}\n\n- Lowercasing\n- Removing punctuation\n- Removing numbers\n- Removing stopwords (optional)\n:::\n:::\n\n---\n\n## Stanford Text Corpus Import\n\n::: {#1fb50c7f .cell output-location='column' execution_count=6}\n``` {.python .cell-code}\nimport random\n\n\ndef sample_sentences(x, y, n=4, seed=42):\n    random.seed(seed)\n    idx = random.sample(range(len(x)), n)\n    return [(y[i], x[i]) for i in idx]\n\n\nsamples = sample_sentences(x_train, y_train, n=4)\n\nfor i, (label, text) in enumerate(samples, 1):\n    print(f\"S{i} [label={label}]: {text}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nS1 [label=1]: With Dickens ' words and writer-director Douglas McGrath 's even-toned direction , a ripping good yarn is told .\nS2 [label=0]: Maybe Thomas Wolfe was right : You ca n't go home again .\nS3 [label=-1]: Despite suffering a sense-of-humour failure , The Man Who Wrote Rocky does not deserve to go down with a ship as leaky as this .\nS4 [label=1]: It will guarantee to have you leaving the theater with a smile on your face .\n```\n:::\n:::\n\n\n## Step 2: Tokenization\n\n**Breaking text into units**\n\n* Split text into tokens (usually words)\n* Simple whitespace tokenization is often enough\n\n---\n\n## Building `CountVectorizer`\n\n::: {#45df89f8 .cell output-location='column' execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n\ndocs = [text for _, text in samples]\n\nvectorizer = CountVectorizer(\n    lowercase=True,\n    stop_words=None   # keep everything for teaching clarity\n)\n\nX = vectorizer.fit_transform(docs)\n\nbow_df = pd.DataFrame(\n    X.toarray(),\n    columns=vectorizer.get_feature_names_out(),\n    index=[f\"S{i+1}\" for i in range(len(docs))]\n)\n\nbow_df.iloc[:, 0:8]\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>again</th>\n      <th>and</th>\n      <th>as</th>\n      <th>ca</th>\n      <th>deserve</th>\n      <th>despite</th>\n      <th>dickens</th>\n      <th>direction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>S1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>S2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>S3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>S4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Vocabulary Construction\n\n- Vocabulary = **unique words across all documents**\n- Vocabulary size grows quickly\n- Rare words may be removed\n- Vocabulary ‚âà **feature space**\n- Too large ‚Üí sparse, inefficient models\n\n## Counting Words - Document-Term Matrix (DTM)\n\n- Rows = documents\n- Columns = vocabulary terms\n- Values = word counts\n\nThis is the **actual model input**.\n\n## Word Frequencies\n\n::: {#00f0aa0f .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](M01_P2_files/figure-revealjs/cell-9-output-1.png){width=951 height=470}\n:::\n:::\n\n\n---\n\n## What BoW Gets Right\n\n**Strengths**\n\n* Simple and interpretable\n* Fast to compute\n* Works surprisingly well for:\n\n  * sentiment analysis\n  * topic classification\n  * spam detection\n\nBusiness angle:\n\n* Strong baseline\n* Easy to debug and explain\n\n---\n\n## What BoW Gets Wrong\n\n**Limitations**\n\n* Ignores word order\n* Ignores meaning\n* Vocabulary explosion\n* Sparse matrices\n\nClassic failure:\n\n```\n\"not good\" ‚âà \"good\"\n```\n\nüìå This slide sets up TF-IDF and embeddings.\n\n---\n\n## BoW in Practice (KEEP IMAGE)\n\nUse the **sentiment analysis workflow image** from the article.\n\nThis visually reinforces:\n\n* raw text ‚Üí vector ‚Üí model ‚Üí prediction\n\n---\n\n## When Should You Use BoW?\n\n**Decision guidance**\n\nUse BoW when:\n\n* Dataset is small to medium\n* Interpretability matters\n* You need a fast baseline\n* Text is short and structured\n\nAvoid BoW when:\n\n* Long documents\n* Semantic nuance matters\n* Context is critical\n\n---\n\n## Conceptual Takeaway\n\n**One sentence slide**\n\n> Bag of Words is not about language‚Äîit is about **counting**.\n\nThis prepares students mentally for:\n\n* TF-IDF (weighting counts)\n* Embeddings (learning meaning)\n* Transformers (learning context)\n\n\n\n# References\n\n:::{.refs}\n\n:::\n\n",
    "supporting": [
      "M01_P2_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}