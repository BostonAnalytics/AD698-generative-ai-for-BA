---
jupyter:
  kernelspec:
    display_name: Python [conda env:cs5246]
    language: python
    name: conda-env-cs5246-py
---

<img src='data/images/section-notebook-header.png' />

# Setup Check

Since Section 1 does not cover technical content yet, the purpose of this notebook is to provide some checks if the most important packages needed for this course are installed. If none of the code cells below throw an error, you should be ready to go for the course. In case you get an error stating that a package is missing, you may want to install it using `pip`, `conda`, or `mamba` -- depending on your Python installation and package manager.

Note taht individual notebooks might require the import of additional packages. Here, we cover only the most common and frequently used ones.

## Python Installation

As of **June 2023**, we recommend using Python 3.9 to ensure that all required packages are fully supported; although Python 3.10 should also cause no problems. You can check the in-built `sys` module to see which Python version you are using.

```{python}
import sys
print(sys.version)
```

## Basic Packages

The following packages should be part for any Python installation

### Re

The `re` module is a built-in Python package that provides regular expression matching operations. It allows you to work with regular expressions, which are powerful patterns used to match and manipulate text. The `re` module enables tasks such as searching for specific patterns, replacing substrings, and extracting relevant information from text data. With the `re` module, you can perform a wide range of text manipulation tasks using regular expressions. It is a versatile tool for handling text data and allows for powerful pattern matching and extraction operations.

```{python}
import re
```

## Core Data Science Packages

### NumPy

NumPy is a widely used Python package for numerical computing. It stands for "Numerical Python" and provides efficient and high-performance operations on large, multi-dimensional arrays and matrices. NumPy is a fundamental library in the Python scientific computing ecosystem and serves as a building block for many other data science and machine learning libraries.

```{python}
import numpy
```

### Pandas

Pandas is a powerful Python library for data manipulation and analysis. It provides high-performance, easy-to-use data structures and data analysis tools, making it a popular choice for working with structured and tabular data. Pandas builds on top of NumPy and extends its functionality with additional features tailored for data analysis tasks. Pandas is widely used in data analysis, data exploration, data cleaning, feature engineering, and other data-related tasks. It offers a comprehensive set of tools that make working with structured data in Python more convenient and efficient.

```{python}
import pandas
```

### Scikit-Learn

The "sklearn" abbreviation commonly refers to the popular Python library called scikit-learn. Scikit-learn is a powerful machine learning library that provides a wide range of algorithms and tools for data mining, data analysis, and machine learning tasks. It is built on top of other scientific Python libraries such as NumPy, SciPy, and matplotlib. Scikit-learn is widely used for various machine learning tasks, including classification, regression, clustering, and dimensionality reduction. It provides a user-friendly interface and a rich set of functionalities, making it a valuable tool for both beginners and experienced practitioners in the field of machine learning.

```{python}
import sklearn
```

### Matplotlib & Seaborn

Matplotlib is a popular Python library for creating static, animated, and interactive visualizations. It provides a comprehensive set of tools for generating plots, charts, histograms, scatter plots, and many other types of visualizations. Matplotlib is widely used in data analysis, scientific research, and data visualization tasks. Matplotlib is a powerful and versatile library for data visualization in Python. It offers a wide range of plotting options, customization features, and integration with other scientific libraries. Whether you need to create simple visualizations or complex, interactive plots, Matplotlib provides the tools and flexibility to meet your data visualization needs.

Seaborn is a Python data visualization library based on Matplotlib. It provides a high-level interface for creating visually appealing and informative statistical graphics. Seaborn is built on top of Matplotlib and enhances its functionalities, making it easier to generate complex visualizations with fewer lines of code. Seaborn is particularly useful for exploring and visualizing relationships in datasets, especially in the context of statistical analysis. Seaborn is a powerful library for creating visually appealing and informative statistical visualizations. It simplifies the process of generating complex plots and provides specialized functions for exploring relationships in data. Whether you are working on exploratory data analysis, statistical modeling, or data communication, Seaborn can enhance your visualization workflow and help you gain insights from your data.

```{python}
import matplotlib
import seaborn
```

### NetworkX

The `networkx` package is a Python library used for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. It provides a powerful toolset for working with network data, including graphs, nodes, edges, and various algorithms for analyzing and visualizing networks. `networkx` is widely used in diverse fields, including social network analysis, bioinformatics, recommendation systems, and transportation networks. `networkx` provides a comprehensive set of tools and algorithms for working with networks. It allows for the creation, manipulation, analysis, and visualization of complex networks, making it a valuable tool for studying network structures and properties. Whether you are exploring social networks, analyzing biological networks, or modeling interconnected systems, `networkx` provides a flexible and powerful framework for network analysis in Python.

```{python}
import networkx
```

## NLP Toolkits

### NLTK

NLTK, which stands for Natural Language Toolkit, is a comprehensive Python library for working with human language data. It provides a wide range of tools and resources for tasks related to natural language processing (NLP), such as tokenization, stemming, lemmatization, POS tagging, parsing, semantic reasoning, and more. NLTK is widely used in academia and industry for NLP research, development, and education. NLTK provides a wide range of modules and resources that can be explored and utilized according to specific NLP tasks and requirements. It is a versatile library that serves as a valuable resource for various aspects of natural language processing and text analysis.

```{python}
import nltk
```

### spaCy

Spacy is a popular Python library for natural language processing (NLP) that provides efficient and industrial-strength tools for various NLP tasks. It is designed to be fast, scalable, and easy to use, making it suitable for both research and production environments. Spacy offers pre-trained models for multiple languages and allows for easy customization and extension. Spacy provides a powerful and efficient set of tools for various NLP tasks. Its focus on speed, accuracy, and ease of use has made it a popular choice for NLP practitioners and researchers.

```{python}
import spacy
```

## Deep Learning Frameworks

Deep learning frameworks are software libraries that provide a high-level interface and a set of tools for building, training, and deploying deep neural networks. These frameworks simplify the implementation of complex deep learning models by providing pre-defined layers, optimization algorithms, and other components necessary for constructing neural networks. Deep learning frameworks enable researchers and developers to focus on designing and experimenting with models rather than writing low-level code for network operations. 

### PyTorch (and related packages)

PyTorch is a popular open-source deep learning framework developed by Facebook's AI Research lab. It provides a Pythonic interface for building and training deep neural networks. PyTorch is widely known for its flexibility, dynamic computation graph, and strong support for research and rapid prototyping. PyTorch has gained significant popularity in the deep learning community due to its intuitive syntax, dynamic graph, and strong support for research. It has a large and active community that contributes to its development, and it is widely adopted in both academia and industry. PyTorch's flexibility and ease of use make it a powerful framework for building and training deep neural networks for a wide range of applications.

The `torchtext` package is a Python library built on top of PyTorch that provides utilities and tools for working with text data in deep learning tasks. It aims to simplify the process of preprocessing, loading, and batching textual data for training deep learning models. `torchtext` offers functionalities for common natural language processing (NLP) tasks, such as tokenization, vocabulary management, and dataset handling. `torchtext` simplifies the process of handling text data in NLP tasks and integrates well with PyTorch, enabling seamless incorporation of text processing pipelines into deep learning workflows. It provides efficient and convenient tools for preprocessing, vocabulary management, dataset handling, and batch processing, saving time and effort in text data preparation for deep learning models.

```{python}
import torch
import torchtext
```

## GPU Support

A GPU (Graphics Processing Unit) is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images for display on a computer screen. However, GPUs are not limited to graphics processing and have found applications in various fields, including data science and deep learning.

In data science and deep learning, GPUs are incredibly useful due to their parallel processing capabilities. Unlike CPUs (Central Processing Units), which excel at performing tasks sequentially, GPUs are designed to handle multiple tasks simultaneously. This parallelism allows GPUs to perform computations on large datasets and complex models much faster than CPUs. Here's how GPUs can specifically help with data science and deep learning:

* **Faster computation:** GPUs can execute mathematical operations in parallel, which significantly speeds up computations for tasks such as matrix multiplications and convolutions. This acceleration is crucial for training deep neural networks, which involve numerous complex operations.

* **Model training:** Deep learning models, especially those with millions or billions of parameters, require extensive computational power for training. GPUs excel in this aspect by distributing the workload across thousands of cores, resulting in faster model convergence and reduced training times.

* **Model deployment and inference:** Once a deep learning model is trained, it needs to be deployed for real-world use. GPUs can continue to play a vital role in this phase by accelerating the inference process. Inference refers to the model's ability to make predictions or classify new data based on its learned knowledge. GPUs can perform inference computations in parallel, allowing for faster and more efficient real-time predictions.

* **Large dataset processing:** Data science often involves working with large datasets. GPUs can help accelerate data preprocessing tasks, such as feature extraction, data augmentation, and transformation. These operations can be parallelized across multiple GPU cores, reducing the time required for data preparation.

* **Experimentation and optimization:** Data scientists and deep learning researchers often iterate through numerous experiments to fine-tune their models and hyperparameters. GPUs facilitate faster experimentation cycles by providing quicker feedback on model performance, enabling researchers to iterate more efficiently.

To leverage GPUs for data science and deep learning, specialized libraries and frameworks have been developed, such as TensorFlow and **PyTorch**, which provide GPU support and take advantage of the parallel processing capabilities. These libraries allow users to write code that can run on GPUs with minimal modifications, making it easier to harness the power of GPUs in data science workflows.

Overall, GPUs offer significant benefits to data science and deep learning tasks, enabling faster computations, reduced training times, efficient model deployment, and improved experimentation cycles, ultimately advancing the capabilities of these fields.

### Check GPU Support with PyTorch

As mentioned above, GPU support is of particular benefit in the context of Deep Learning. Thus, all modern Deep Learning Frameworks support training and inferencing on GPUs. For example, PyTorch provides the method `torch.cuda.is_available()` to check it everything is set up correctly for PyTorch to utilize a GPU.

```{python}
import torch

# Setting device on GPU if available, else CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Using device:', device)
print()

# Additional Info when using cuda
if device.type == 'cuda':
    print(torch.cuda.get_device_name(0))
    print('Memory Usage:')
    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')
    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')
```

An output of the code cell above might look like as follows:
    
```
Using device: cuda

NVIDIA GeForce RTX 2080 Ti
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
```

Of course, the output may differ, particularly depending on which type of GPU is installed -- or how many. However, if the output is

```
Using device: cuda
```

your GPU is not correctly set up. There can be several reasons why a GPU may not be working with PyTorch. Here are some common issues to consider:

* **Incompatible hardware:** Ensure that your GPU is compatible with PyTorch and has the required specifications. PyTorch supports NVIDIA GPUs with CUDA architecture. Check the PyTorch documentation for specific GPU compatibility details.

* **CUDA installation:** PyTorch relies on the CUDA toolkit provided by NVIDIA for GPU acceleration. Make sure that CUDA is properly installed on your system and that you have the correct version compatible with your GPU. PyTorch requires a specific CUDA version to work seamlessly.

* **Driver issues:** Ensure that you have the latest GPU drivers installed. Outdated or incompatible drivers can prevent PyTorch from utilizing the GPU effectively. Visit the official website of your GPU manufacturer (e.g., NVIDIA) to download and install the latest drivers.

* **PyTorch installation:** Make sure that you have installed PyTorch with GPU support. PyTorch offers different distributions, including CPU-only versions and GPU-enabled versions. You need to install the appropriate version that includes CUDA support for your GPU.

If the code cell above successfully showed the availability of a GPU, it still might show `0.0 GB` for allocated and cached memory. This simply means that your GPU was mainly idling. However, we can easily test this. For example, in the code cell below, we create a random tensor (here: a 3-dimensional array). We can also calculate the size of the tensor in megabytes by multiplying the number of tensor elements with the size of an element.

```{python}
# Create tensors
a = torch.rand(1000, 1000, 1000, dtype=torch.float32)

# Calculate size in megabytes (#elements * #bytes_per_element)
size_in_megabytes = ((a.nelement() * a.element_size()) / 1000000)

print(size_in_megabytes)
```

If you see an output of 4000.0, this means that the tensor has a size of 4 GB. This means that an individual element in the tensor has a size of 4 bytes (typically a float with 32 bit precision, but this might differ depending on the platform; here we explicitly choose `float32` to ensure the same output). If the code cell above throws a memory error, try reducing the size of the tensor.

So far the tensor `a` "lives" in the main memory. However, we want to move to the GPU. In PyTorch, we can simply do this using the `to()` method. Of course, this assumes that a GPU was successfully detected previously.

```{python}
# First remove tensor from GPU (only relevant if you run this code cell multiple times)
a_on_gpu = None
torch.cuda.empty_cache()

# Move tensor to GPU
a_on_gpu = a.to(device) # If you comment this line and run the cell, it should clear the GPU memory
```

Let's again check the memory consumption using the code snippet from above.

```{python}
#Additional Info when using cuda
if device.type == 'cuda':
    print(torch.cuda.get_device_name(0))
    print('Memory Usage:')
    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')
    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')
```

Now the allocated and/or cached memory shows a memory consumption of around `4 GB`. It is unlikely to be exactly this value, but it should be in that ballpark.

### Check GPU Support with spaCy

spaCy can be used with a GPU for certain operations. However, it's important to note that not all aspects of spaCy are GPU-accelerated. The GPU support in spaCy primarily focuses on neural network models for certain NLP tasks, such as tokenization, part-of-speech tagging, named entity recognition, and dependency parsing.

To utilize a GPU with spaCy, you'll need to ensure that you have the necessary dependencies and configurations in place:

* **CUDA support:** Make sure that you have CUDA drivers and the CUDA toolkit installed on your system. GPU acceleration relies on CUDA to perform computations on the GPU.

* **GPU-enabled spaCy model:** spaCy provides certain pre-trained models that can be used with GPU acceleration. These models are usually prefixed with "lg" (large) or "md" (medium), indicating their size. The smaller models, typically prefixed with "sm" (small), do not have GPU support. Ensure that you download and load the appropriate GPU-enabled model.

* **Configuration:** To enable GPU usage in spaCy, you need to set up the proper configuration. This typically involves creating a spaCy language object with GPU support and specifying the GPU device to be used. Here's an example:

```{python}
import spacy

spacy.prefer_gpu() # or spacy.require_gpu()

# Load a GPU-enabled model
nlp = spacy.load('en_core_web_lg')  
```

If the code cell above does not throw any error, analyzing a document will now be done on the GPU.

```{python}
doc = nlp("This is an example sentence.")
```

Given this simple example sentence, there will be no noticeable difference compared to using the CPU. In fact, for very simple tasks, using the CPU is preferred since utilizing the GPU always includes some overhead such as moving models and data to the GPU. However, keep in mind that spaCy is also very flexible and allows you to train your own models. In these cases, using a GPU can have significant performance benefits.

It's important to note that not all spaCy operations are GPU-accelerated. Some functionality, such as rule-based matching, linguistic features, and custom components, may still run on the CPU. Therefore, it's recommended to benchmark and profile your specific use case to determine the performance benefits of using a GPU with spaCy.


