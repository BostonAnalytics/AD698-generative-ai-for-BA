---
jupyter:
  kernelspec:
    display_name: cs5246
    language: python
    name: cs5246
---

<img src='data/images/section-notebook-header.png' />

# N-Gram Language Models (basic)

A n-gram language model is a type of statistical language model that predicts the likelihood of a sequence of words based on the probability of occurrence of smaller word sequences called n-grams. An n-gram is a contiguous sequence of n items from a given sample of text, where the items can be words, characters, or even phonemes. In the context of language modeling, an n-gram typically refers to a sequence of n words. For example, in the sentence "I love to play soccer," the 2-grams (or bigrams) would be "I love," "love to," "to play," and "play soccer." The 3-grams (or trigrams) would be "I love to," "love to play," and "to play soccer."

The n-gram language model makes assumptions about the probability of a word occurring based on the previous n-1 words. It utilizes a training corpus of text to calculate the frequencies of n-grams and estimates the probability of encountering a particular word given the preceding n-1 words. This probability can be computed using the formula:

$$P(w_n|w_{1:n-1}) = \frac{Count(w_{1:n-1}w_n)}{\sum_{w}Count(w_{1:n-1}w)} = \frac{Count(w_{1:n})}{Count(w_{1:n-1})}$$

where $w_{1:n}$ are all the words in a sequence up the word $w_n$ at position $n$, and $w_{1:n-1}$ are all the words in a sequence before the word $w_n$ at position $n$.

The n-gram model is a simplified approach to language modeling and is based on the Markov assumption, which assumes that the probability of a word only depends on a fixed number of preceding words. For example, if we only consider the $k=2$ words before the word $w_n$, then our probability formula becomes

$$P(w_n|w_{1:n-1}) = P(w_n|w_{n-k:n-1})$$

where $w_{n-k:n-1}$ are the $k$ words ahead of word $w_n$ in the sequence. While it has limitations in capturing long-range dependencies and context, n-gram models are computationally efficient and widely used in various natural language processing tasks such as speech recognition, machine translation, and text generation.

This notebook covers the very basic idea behind n-gram language models. To this end, we implement the toy example from the lecture. The goal here is to show that training an n-Gram language model really just boils down to going through a text and counting the occurrences of n-grams for using Maximum Likelihood Estimation to compute the conditional probabilities. Of course, in practice, things can quickly become more complicated when (advanced) smoothing techniques are considered. But even then, the underlying requirement is counting the occurrences of n-gram. We use simple Add-k Smoothing to give an example for a smoothing technique -- compared to, e.g., Kneser-Ney Smoothing.

## Setting up the Notebook

### Import all Required Packages

We'll use spaCy to handle the basics such as tokenization. Given the toy example containing only 3 sentences without punctuation marks, hyphenated words, abbreviations, or anything else that would make tokenization in any sense challenging, using spaCy is kind of overkill. However, this makes it easy to extend the corpus with your own sentences without worrying about the correct tokenization.

```{python}
import spacy
nlp = spacy.load("en_core_web_sm")
```

NLTK comes with some convenient methods to generate bigrams and trigrams which can also handle the padding, i.e., the adding of start-of-sentence and end-of-sentence tokens.

```{python}
from nltk.util import trigrams, bigrams
from collections import defaultdict
```

---

## Training the Language Model

Training a n-gram language model is all about keeping counts of all n-grams found in a text. Here, we do this on a sentence-by-sentence basis. The method `process_sentence` below takes a sentence `s` and the current n-gram counts, goes through all n-grams in the sentence, and updates the n-gram counts accordingly. We also keep track of the vocabulary as we need the size of the vocabulary for smoothing.

Note that we use a `defaultdict` instead of a standard dictionary here. This is purely for convenience, since we don't need to check if a key (i.e., a n-gram) is already present in the dictionary before updating the count. This just saves a couple of lines of code. We also only consider bigram and trigram language models. Again, this is purely to make the code as compact and easy to read as possible -- this is not a principle limitation!

```{python}
def process_sentence(s, ngram_counts, vocabulary, ngram_size=2):
    # Let spaCy handle the tokenization
    doc = nlp(s)
    # Get the individual text tokens
    tokens = [ t.text for t in doc]
    
    # Update vocabulary
    vocabulary.update(set(tokens))
    
    # Since we want to keep the simple here, we let NLTK to the generation of all ngram incl. the padding
    # However, NLTK does this out of the box only for bigrams and trigrams, so let's limit ourselves here
    if ngram_size == 2:
        ngrams = bigrams
    elif ngram_size == 3:
        ngrams = trigrams
    else:
        raise Exception('Unsupported ngram size: either 2 or 3')
    
    # Now we can generate all ngrams and update our dictionary with all the counters
    # We need the counts for the whole ngram and the context (ngram without the last word)
    for ngram in ngrams(tokens, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):
        context = ngram[:-1]
        ngram_counts[context] += 1
        ngram_counts[ngram] += 1
```

Now we're ready to go to train the language model. We use the very small toy corpus from the lecture:

```
I am Sam
Sam I am
I do not like green eggs and ham
```

which is stored in the file `'data/toy-example-data.txt`. Feel free to modify and/or extend this corpus with sentences of your own. The only requirement is that each sentence must be in its own line in the file. Otherwise the following code would need a little bit of tweaking

So let's just read the file and go through all the sentences and use method `process_sentence` to update our n-gram counts which are stored in `ngram_counts`.

```{python}
# Initialize n-gram counts; using a defaultdict, the default count for missing keys/ngrams is 0
ngram_counts = defaultdict(int)
vocabulary = set()

ngram_size = 2
#ngram_size = 3

with open('data/corpora/toy-example-data.txt', 'r') as file:
    # Loop over each line in the file (1 line = 1 sentence)
    for sentence in file.read().split('\n'):
        # Update n-gram counts using our auxiliary method
        process_sentence(sentence, ngram_counts, vocabulary, ngram_size=ngram_size)

# Display the final n-gram counts
ngram_counts
```

Now we have the counts for all n-grams of size `ngram_size` and `ngram_size-1`, which is all we need to calculate the probabilities. In the case of a bigram language, a basic sanity check for the output above is that we see the unigram `(<s>)` 3 times since we have 3 sentences in out toy dataset. For a trigram language model, we should of course see the bigram `(<s>, <s>)` 3 times.

For the smoothing, we also need the size of the vocabulary. Since we have the vocabulary already, this is a trivial step:

```{python}
V = len(vocabulary)

print("Numner of n-grams: {}".format(len(ngram_counts)))
print("Size of vocabulary: {}".format(V))
```

---

## Calculating Probabilities

With all the n-gram counts, we can now compute the conditional probabilities using Maximum Likelihood Estimation as defined in the lecture. We also saw that Add-k Smoothing was a very simple extension. The method `calc_prob` below computes the probability of an n-gram; the Add-k Smoothing is optional.

```{python}
def calc_prob(ngram, ngram_counts, k=0, V=0):
    # Convert the n-gram from a list to a tuples so we can use it as a key for our n-gram counts dictionary
    ngram = tuple(ngram)
    # Get the context, i.e., the n-gram without the last word
    context = ngram[:-1]
    # Calculate and return the probability using Maximum Likelihood Estimation
    # We wrap the calculation in a TRY-CATCH block to handle divisions by zero if the context count is 0
    try:
        return (ngram_counts[ngram] + k) / (ngram_counts[context] + k*V)
    except Exception as e:
        return 0.0
```

### Bigram Examples

Let's first calculate the bigram probabilities we used in the lecture. Note that these examples only work if you trained the language model with bigrams. 

```{python}
print(calc_prob(['<s>', 'I'], ngram_counts))    
print(calc_prob(['I', 'am'], ngram_counts))
print(calc_prob(['am', 'Sam'], ngram_counts))
print(calc_prob(['Sam', '</s>'], ngram_counts))
```

We can also compute the probabilities using Add-k Smoothing (e.g., Laplace Smoothing with $k=1$). The method `calc_prob` is ready for that.

```{python}
k = 1

print(calc_prob(['<s>', 'I'], ngram_counts, k=k, V=V))    
print(calc_prob(['I', 'am'], ngram_counts, k=k, V=V))
print(calc_prob(['am', 'Sam'], ngram_counts, k=k, V=V))
print(calc_prob(['Sam', '</s>'], ngram_counts, k=k, V=V))
```

You can see how the probabilities drop when using smoothing as we move probability mass from n-grams with non-zero counts to all n-grams with zero counts. The smaller you make k, the less probability mass is moved; try $k=0.1$ to see the difference.

### Trigram Examples

If you train the language model using trigrams, you can check out the example below. The code cell still works after training a bigram model, but the probabilities will of course be 0 (at least without smoothing).

```{python}
print(calc_prob(['<s>', 'I', 'am'], ngram_counts))    
print(calc_prob(['I', 'am', 'Sam'], ngram_counts))
print(calc_prob(['<s>', 'Sam', 'I'], ngram_counts))
print(calc_prob(['am', 'Sam', '</s>'], ngram_counts))
```

And again, the same with smoothing

```{python}
k = 0.1

print(calc_prob(['<s>', 'I', 'am'], ngram_counts, k=k, V=V))    
print(calc_prob(['I', 'am', 'Sam'], ngram_counts, k=k, V=V))
print(calc_prob(['<s>', 'Sam', 'I'], ngram_counts, k=k, V=V))
print(calc_prob(['am', 'Sam', '</s>'], ngram_counts, k=k, V=V))
```

---

## Summary

The basic idea behind training a n-gram language model is indeed quite straightforward: just count the occurrences of all n-grams. Of course, here, we purposefully ignored all challenges that come when training a language model over a very large corpus. Most importantly, the number of unique n-grams quickly increases, even exponentially w.r.t. the size of the n-grams.

Still, basically all changes needed to the code above would address this challenge of handling a very large text corpus. The basic algorithm of going through the corpus and keeping track of all n-gram counts would remain exactly the same. Please keep in mind, however, that the code used in this notebook focuses on simplicity and understanding, and not on performance!


