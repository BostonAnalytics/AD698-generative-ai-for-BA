---
title: "M02 Presentation 02"
logo: "../theme/figures/met_signature_toptier_rgb.png"
date: 03/12/2024
date-modified: today
date-format: long
author:
  - name: Nakul R. Padalkar
    affiliations:
      - id: BU
        name: Boston University
        city: Boston
        state: MA
  - name: Author Two
    affiliations:
      - ref: BU
format: 
    revealjs:
        theme: [../theme/presentation.scss]
        html-math-method: katex
        slide-number: c/t
        toc: true
        toc-depth: 1
    pptx:
        reference-doc: ../theme/presentation_template.pptx
self-contained-math: true
fig-align: center
monofont: Roboto
title-slide-attributes:
    data-background-image: "../theme/blank_red.png"
    data-background-size: 103% auto
    data-background-opacity: "0.95"
execute: 
  echo: false
  warning: false
  message: false
bibliography: ../references.bib
csl: ../mis-quarterly.csl
---

# Defining the Word

## Types, tokens, vocabulary                             

## Tokenization                                          

## Out-of-vocabulary problem                             

# Representing Meaning

## Static embeddings                                     
## Contextual embeddings                                 
## Distance metrics in embedding space                   
## Mahalanobis distance for covariance-aware similarity  

# Structured Prediction

## Sequence labeling                                     
## Hidden Markov Models                                  
## Conditional Random Fields                             
## Neural sequence labeling                              

# Classification Diagnostics & Evaluation

## Confusion matrix                                      
## Precision, recall, F1                                 
## ROC curves                                            
## AUC                                                   
## Threshold-free metrics                                
## Statistical significance testing                      

# Attention & Long-Range Dependencies

## RNN limitations                                       
## Attention mechanism                                   
## Multi-head self-attention                             

# Transformer Architectures
## Encoder                                               
## Decoder                                               
## Encoderâ€“decoder                                       
## Cross-attention                                       

# Pretraining Objectives

## Masked language modeling                              
## Causal language modeling                              

# Modern Extensions

## Vision transformers                                   
## Architectural innovations (MoE, RoPE, Flash Attention)
## Multimodal LLMs                                       

