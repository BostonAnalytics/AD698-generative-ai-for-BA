---
title: "M02 Presentation 02"
logo: "../theme/figures/met_signature_toptier_rgb.png"
date: 03/12/2024
date-modified: today
date-format: long
author:
  - name: Nakul R. Padalkar
    affiliations:
      - id: BU
        name: Boston University
        city: Boston
        state: MA
  - name: Author Two
    affiliations:
      - ref: BU
format: 
    revealjs:
        theme: [../theme/presentation.scss]
        html-math-method: katex
        slide-number: c/t
        toc: true
        toc-depth: 1
    pptx:
        reference-doc: ../theme/presentation_template.pptx
self-contained-math: true
fig-align: center
monofont: Roboto
title-slide-attributes:
    data-background-image: "../theme/blank_red.png"
    data-background-size: 103% auto
    data-background-opacity: "0.95"
execute: 
  echo: false
  warning: false
  message: false
bibliography: ../references.bib
csl: ../mis-quarterly.csl
---

| **1. Defining the Word**                                  |                           |                        |
| └─ Types, tokens, vocabulary                              | 2.2.1                     | —                      |
| └─ Tokenization                                           | 8.4                       | 4.3.2                  |
| └─ Out-of-vocabulary problem                              | 6.5                       | —                      |
| **2. Representing Meaning**                               |                           |                        |
| └─ Static embeddings                                      | 3.1                       | 2.3                    |
| └─ Contextual embeddings                                  | 6.3                       | 4.3                    |
| └─ Distance metrics in embedding space                    | —                         | 2.6                    |
| └─ Mahalanobis distance for covariance-aware similarity   | —                         | —                      |
| **3. Structured Prediction**                              |                           |                        |
| └─ Sequence labeling                                      | 7.1                       | —                      |
| └─ Hidden Markov Models                                   | 7.4                       | —                      |
| └─ Conditional Random Fields                              | 7.5                       | —                      |
| └─ Neural sequence labeling                               | 7.6                       | —                      |
| **4. Classification Diagnostics & Evaluation**            |                           |                        |
| └─ Confusion matrix                                       | 4.4.1                     | —                      |
| └─ Precision, recall, F1                                  | 4.4.1                     | —                      |
| └─ ROC curves                                             | 4.4.2                     | —                      |
| └─ AUC                                                    | 4.4.2                     | —                      |
| └─ Threshold-free metrics                                 | 4.4.2                     | —                      |
| └─ Statistical significance testing                       | 4.4.3                     | —                      |
| **5. Attention & Long-Range Dependencies**                |                           |                        |
| └─ RNN limitations                                        | 6.3                       | —                      |
| └─ Attention mechanism                                    | —                         | 4.4                    |
| └─ Multi-head self-attention                              | —                         | 4.4.2                  |
| **6. Transformer Architectures**                          |                           |                        |
| └─ Encoder                                                | —                         | 4.3                    |
| └─ Decoder                                                | —                         | 4.8                    |
| └─ Encoder–decoder                                        | —                         | 4.10                   |
| └─ Cross-attention                                        | —                         | 4.10.2                 |
| **7. Pretraining Objectives**                             |                           |                        |
| └─ Masked language modeling                               | —                         | 4.7                    |
| └─ Causal language modeling                               | 6                         | 4.9                    |
| **8. Modern Extensions**                                  |                           |                        |
| └─ Vision transformers                                    | —                         | 4.6                    |
| └─ Architectural innovations (MoE, RoPE, Flash Attention) | —                         | 4.12                   |
| └─ Multimodal LLMs                                        | —                         | 3.6                    |

