---
title: "Module 1: Participation 2"
subtitle: "Group Paper Presentation"
number-sections: true
date: "2024-11-21"
date-modified: today
date-format: long
bibliography: ../references.bib
csl: ../mis-quarterly.csl
---

# Overview {.unnumbered}


In this participation assignment, you will work in **groups** to study, synthesize, and present a **foundational or state-of-the-art research paper** in Generative AI, Large Language Models (LLMs), or reproducible computational practice.

The goal is **not** to reproduce the paper technically, but to:

* Understand *why* the paper mattered
* Explain *what problem* it solved
* Situate it within the **modern GenAI stack**
* Critically assess its **assumptions, limitations, and downstream implications**

Each group will deliver a **short academic-style presentation** aimed at a technically literate but non-specialist audience (e.g., analytics managers, graduate students, applied researchers).

This assignment emphasizes:

* Conceptual clarity
* Systems thinking
* Research literacy
* Responsible AI awareness


# Goals {.unnumbered}

By completing this assignment, you will:

* Develop the ability to **read and interpret AI research papers**
* Learn how modern GenAI systems evolved from earlier computational ideas
* Practice explaining complex ideas clearly and precisely
* Engage critically with **reproducibility, scale, alignment, and responsibility**
* Strengthen academic and professional presentation skills


# Paper Selection (One Paper per Group)

- Everyone must read through the curated list below.
- Papers are organized by **theme**. 
- Each group will need to present a presentation on one of the papers from the week's theme listed under numbered list.
- Names of the papers are listed in the table and are also available at the end of this document.
- You can also search these papers on [scholar.google.com](https://scholar.google.com). 


## Open science & reproducibility (classic but still essential)

[Reading instruction]{.uured-bold}: Read these, no presentation. 

- Wilson et al. (2014) *Best Practices for Scientific Computing*
- Peng (2011) *Reproducible Research in Computational Science*
- Stodden et al. (2014) *The Practice of Reproducible Research*
- Stodden et al. (2016) *Computational Reproducibility*
- Knuth (1984) *Literate Programming

## Foundations Readings in Language Representation

[Reading instruction]{.uured-bold}: Read these, no presentation. 

| **Neural Foundations**             | **Distributional Semantics**  | **Embeddings in Practice**        |
| ----------------------------------------- | ------------------------------------ | ---------------------------------------- |
| A Neural Probabilistic Language Model [@Bengio2003NeuralLM] | Vector Space Models of Semantics [@TurneyPantel2010VSM] | Distributed Representations of Words [@Mikolov2013Word2Vec] |
|Sequence to sequence learning [@Sutskever2014Sequence]|Are LLMs Models of Distributional Semantics? A Case Study on Quantifiers  [@Enyan2024Distributional]|Efficient Estimation of Word Representations in Vector Space [@Mikolov2013Efficient]|


## Group 1 – Transformers: Architecture and Attention

| **Transformer Core**                                      | **Interpretability & Attention**                   | **Representational Power**                                              |
| --------------------------------------------------------- | -------------------------------------------------- | ----------------------------------------------------------------------- |
| Attention Is All You Need [@Vaswani2017Attention]         | What Does BERT Look At? [@Clark2019Bert]       | On the Turing Completeness of Modern Neural Network Architectures [@Perez2019Turing] |
| Sequence to Sequence Learning [@Sutskever2014Sequence]    | Quantifying attention flow in transformers [@Abnar2020Quantifying] | Are Transformers Universal Approximators? [@Yun2019Transformers]        |
| Self-Attention with Relative Position [@Shaw2018Selfattention] | Attention Is Not Explanation [@Jain2019Attention]  | Efficient streaming language models[@Xiao2023Efficient]          |


## Group 2 – Scaling & Emergence

* Scaling Laws for Neural LMs [@Kaplan2020Scaling]
* Sparks of AGI [@Bubeck2023SparksAGI]


## Group 3 – Text-as-Data & Business NLP

* Text as Data [@Gentzkow2019TextAsDataPaper]
* Information Extraction from Business Text [@Kogan2019BusinessIE]


## Group 4 – Prompting & Reasoning

* Chain-of-Thought Prompting [@Wei2022CoT]
* ReAct [@Yao2023ReAct]


## Group 5 – Alignment & Instruction

* Training Language Models with Human Feedback [@Ouyang2022RLHF]
* Constitutional AI [@Bai2022Constitutional]


## Group 6 – Retrieval-Augmented Generation

* Retrieval-Augmented Generation [@Lewis2020RAG]
* Self-RAG [@Asai2023SelfRAG]


## Group 7 – Efficient Fine-Tuning

* LoRA [@Hu2021LoRA]
* QLoRA [@Dettmers2023QLoRA]


## Group 8 – Long Context & Memory

* Lost in the Middle [@Liu2023ContextWindow]
* Transformer-XL [@Dai2019TransformerXL]


## Group 9 – Evaluation & Benchmarks

* HELM Benchmark [@Liang2022HELM]
* Evaluating LLMs [@Chang2024EvaluatingLLMs]


## Group 10 – Hallucination & Trust

* Survey on Hallucination in LLMs [@Huang2023Hallucinations]
* Stochastic Parrots [@Bender2021StochasticParrots]


## Group 11 – Governance & Deployment

* Foundation Models Paper [@bommasani2022opportunities]
* Responsible AI for LLMs [@Raji2022ResponsibleAI]


# Presentation Expectations

## Presentation Length

* **10–12 minutes total**
* **5–7 slides**
* All group members must participate

## Suggested Slide Structure

Your presentation **must include**:

1. **Problem Framing**

   * What problem did this paper address?
   * Why was it important *at the time*?

2. **Core Contribution**

   * Key idea, model, framework, or insight
   * What changed because of this paper?

3. **Technical Intuition (Not Math-Heavy)**

   * Diagrams encouraged
   * Focus on system logic, not equations

4. **Impact and Legacy**

   * How does this paper influence modern GenAI systems?
   * Where do we see it today?

5. **Limitations and Critique**

   * What does the paper *not* address?
   * What assumptions may no longer hold?

6. **Relevance to This Course**

   * How does this connect to:

     * Prompting
     * RAG
     * Fine-tuning
     * Reproducibility
     * Responsible AI


# Submission Requirements

1. **Presentation Slides (PDF)**
2. **One-page Paper Brief (PDF)** including:
   - Paper citation
   - Key contribution (≤150 words)
   - One critique
   - One open research question


## Hints and Best Practices

* Focus on **ideas**, not implementation details
* Assume your audience understands Python and ML basics
* Use diagrams over equations
* Avoid reading slides verbatim
* Practice explaining the paper *without jargon*


This assignment is designed to help you **think like a researcher and systems designer**, not just a tool user.
Choose wisely, read deeply, and present with clarity.

:::{.ref}

:::